[{"path":"https://ctesta01.github.io/nadir/CONVENTIONS.html","id":null,"dir":"","previous_headings":"","what":"Developer Conventions","title":"Developer Conventions","text":"order facilitate consistency across codebase, effort made notate conventions intend follow throughout.","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/CONVENTIONS.html","id":"referring-to-column-names","dir":"","previous_headings":"On Function Arguments","what":"Referring to Column Names","title":"Developer Conventions","text":"functions take argument column name, expect string/character values passed. order make clear users, arguments refer column name (hence string value) end _col, _cols, _var _variable depending context. _variable used refer variables contextual meaning, like y_variable yy implied meaning outcome variable. standardized names try use throughout, non-standard variants following discouraged: id_col y_variable covariate_cols","code":""},{"path":"https://ctesta01.github.io/nadir/CONVENTIONS.html","id":"optional-singleton-or-list-arguments","dir":"","previous_headings":"On Function Arguments","what":"Optional Singleton or List Arguments","title":"Developer Conventions","text":"now, one acceptable place readily often use partial argument matching (https://stackoverflow.com//14155259/3161979), formulas argument nadir::super_learner(). reason partial argument matching used throughout much documentation formulas argument super_learner() case one formula used across learners, user may (either preference without thinking) pass formula = <...> super_learner(). make concerted effort use partial argument matching examples documentation except regard formulas argument super_learner(). part, usage partial argument match formula super_learner() acceptable control-flow/logic detects single formula passed (rather named list formulas).","code":""},{"path":"https://ctesta01.github.io/nadir/CONVENTIONS.html","id":"protected-arguments","dir":"","previous_headings":"On Function Arguments","what":"Protected Arguments","title":"Developer Conventions","text":"Certain arguments always follow standard convention, privileged others. prefer use data function arguments refer data.frames. explicitly recognize weights argument learners possible. Moreover, weights passed, explicit code handle appropriately given underlying model’s syntax included learner comes nadir nadir::super_learner() can pass observation weights included candidate learners rely handled properly.","code":""},{"path":"https://ctesta01.github.io/nadir/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 nadir authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ctesta01.github.io/nadir/articles/Basic-Examples.html","id":"getting-more-information-out","dir":"Articles","previous_headings":"","what":"Getting More Information Out","title":"Basic Examples","text":"Suppose want know lot super_learner() process, weighted candidate learners, candidate learners predicted held-data, etc., use verbose_output = TRUE option. put description ’s contained verbose_output = TRUE output super_learner(): prediction function, $sl_predictor() takes newdata character fields like $y_variable $outcome_type provide context learning task performed. $learner_weights indicate weight different candidate learners given $holdout_predictions: data.frame predictions candidate learners, along actual outcome held-data. can call compare_learners() verbose output super_learner() want assess different learners performed. can also call cv_super_learner() arguments super_learner() wrap super_learner() call another layer cross-validation assess super_learner() performs held-data. can, course, anything super learned model conventional prediction model, like calculating performance statistics like R2R^2.","code":"sl_model_iris <- super_learner(   data = iris,   formula = petal_formula,   learners = learners,   verbose = TRUE)  str(sl_model_iris, max.level = 2) #> List of 5 #>  $ sl_predictor       :function (newdata)   #>   ..- attr(*, \"srcref\")= 'srcref' int [1:8] 466 39 472 3 39 3 2830 2836 #>   .. ..- attr(*, \"srcfile\")=Classes 'srcfilealias', 'srcfile' <environment: 0x123ba6e48>  #>  $ y_variable         : chr \"Petal.Width\" #>  $ outcome_type       : chr \"continuous\" #>  $ learner_weights    : Named num [1:4] 0.665 0.335 0 0 #>   ..- attr(*, \"names\")= chr [1:4] \"lm\" \"rf\" \"earth\" \"mean\" #>  $ holdout_predictions: tibble [150 × 6] (S3: tbl_df/tbl/data.frame) #>  - attr(*, \"class\")= chr [1:2] \"list\" \"nadir_sl_verbose_output\" compare_learners(sl_model_iris) #> Inferring the loss metric for learner comparison based on the outcome type: #> outcome_type=continuous -> using mean squared error #> # A tibble: 1 × 4 #>       lm     rf earth  mean #>    <dbl>  <dbl> <dbl> <dbl> #> 1 0.0381 0.0493  2.03 0.579  cv_super_learner(   data = iris,    formula = petal_formula,   learners = learners)$cv_loss #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> [1] 0.03294804 var_residuals <- var(iris$Sepal.Length - sl_model_iris$sl_predictor(iris)) total_variance <- var(iris$Sepal.Length) variance_explained <- total_variance - var_residuals   rsquared <- variance_explained / total_variance print(rsquared) #> [1] 0.7202032"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"iris-data","dir":"Articles","previous_headings":"","what":"iris data","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"","code":"library(pacman) p_load('nadir', 'sl3', 'SuperLearner', 'microbenchmark', 'tidytuesdayR', 'future', 'ggplot2', 'forcats', 'dplyr')  # setup multicore use  future::plan(future::multicore)  petal_formula <- Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length + Species"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"nadirsuper_learner-on-iris-7-3-kb-data","dir":"Articles","previous_headings":"iris data","what":"nadir::super_learner() on iris (7.3 KB) data","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"","code":"iris_nadir_sl_benchmark <- microbenchmark::microbenchmark(   times = 10,   list(     nadir = {       super_learner(         data = iris,         formula = petal_formula,         learners = list(lnr_mean, lnr_lm, lnr_rf, lnr_earth, lnr_glmnet, lnr_xgboost)       )     }   ) ) #> Warning in microbenchmark::microbenchmark(times = 10, list(nadir = {: less #> accurate nanosecond times to avoid potential integer overflows iris_nadir_sl_benchmark #> Unit: milliseconds #>                                                                                                                                                               expr #>  list(nadir = {     super_learner(data = iris, formula = petal_formula, learners = list(lnr_mean,          lnr_lm, lnr_rf, lnr_earth, lnr_glmnet, lnr_xgboost)) }) #>       min       lq     mean   median      uq      max neval #>  889.8272 902.7249 971.1413 945.5787 1007.02 1192.484    10  iris_nadir_cv_sl_benchmark <- system.time({   cv_super_learner(     data = iris,     formula = petal_formula,     learners = list(lnr_mean, lnr_lm, lnr_rf, lnr_earth, lnr_glmnet, lnr_xgboost)   ) }) #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE iris_nadir_cv_sl_benchmark #>    user  system elapsed  #>   3.891   0.975   1.419"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"sl3-on-iris-7-3-kb-data","dir":"Articles","previous_headings":"iris data","what":"sl3 on iris (7.3 KB) data","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"","code":"task <- make_sl3_Task(   data = iris,   outcome = \"Petal.Width\",   covariates = c(\"Sepal.Length\", 'Sepal.Width', 'Petal.Length', 'Species') )  lrn_mean <- Lrnr_mean$new() lrn_lm <- Lrnr_glm$new() lrn_rf <- Lrnr_randomForest$new() lrn_earth <- Lrnr_earth$new() lrn_glmnet <- Lrnr_glmnet$new() lrn_xgboost <- Lrnr_xgboost$new()  stack <- Stack$new(lrn_mean, lrn_lm, lrn_rf, lrn_earth, lrn_glmnet, lrn_xgboost)  sl <- Lrnr_sl$new(learners = stack, metalearner = Lrnr_nnls$new(),                   cv_control = list(V = 5))  iris_sl3_sl_benchmark <- microbenchmark::microbenchmark(   times = 10,   list(     sl3 = { sl_fit <- sl$train(task = task) }   ) ) iris_sl3_sl_benchmark #> Unit: seconds #>                                                 expr      min       lq     mean #>  list(sl3 = {     sl_fit <- sl$train(task = task) }) 1.080533 1.268148 1.462321 #>    median       uq      max neval #>  1.433153 1.487204 1.954279    10  sl_fit <- sl$train(task = task) iris_sl3_cv_sl_benchmark <- system.time({   sl3_cv = { cv_sl(lrnr_sl = sl_fit, eval_fun = loss_squared_error) } }) #> [1] \"Cross-validated risk:\" #> Key: <learner> #>                                    learner        MSE          se     fold_sd #>                                     <fctr>      <num>       <num>       <num> #> 1:                               Lrnr_mean 0.58845981 0.039084452 0.095707715 #> 2:                           Lrnr_glm_TRUE 0.02902919 0.004196715 0.010396127 #> 3:            Lrnr_randomForest_500_TRUE_5 0.03148265 0.004415253 0.009794200 #> 4:         Lrnr_earth_2_3_backward_0_1_0_0 0.04513001 0.010649813 0.038136107 #> 5: Lrnr_glmnet_NULL_deviance_10_1_100_TRUE 0.02900513 0.004208611 0.010167544 #> 6:                       Lrnr_xgboost_20_1 0.03800387 0.005239035 0.013841992 #> 7:                            SuperLearner 0.01707933 0.002604897 0.005322035 #>    fold_min_MSE fold_max_MSE #>           <num>        <num> #> 1:  0.430667215   0.73268861 #> 2:  0.013528873   0.04413943 #> 3:  0.021548138   0.04761416 #> 4:  0.016095248   0.14942276 #> 5:  0.013102653   0.04301907 #> 6:  0.017879247   0.05820837 #> 7:  0.007382378   0.02344127 iris_sl3_cv_sl_benchmark #>    user  system elapsed  #>  26.072  36.402  41.021"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"superlearner-on-iris-7-3-kb-data","dir":"Articles","previous_headings":"iris data","what":"SuperLearner on iris (7.3 KB) data","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"Visual comparison","code":"sl_lib = c(    \"SL.mean\", \"SL.lm\", \"SL.randomForest\", \"SL.earth\", \"SL.glmnet\", \"SL.xgboost\")  iris_SuperLearner_sl_benchmark <- microbenchmark::microbenchmark(   times = 10,    list(SuperLearner = {     mcSuperLearner(Y = iris$Petal.Width,                  X = iris[, -4],                  SL.library = sl_lib,                  cvControl = list(V = 5))   }   ) ) iris_SuperLearner_sl_benchmark #> Unit: seconds #>                                                                                                                                      expr #>  list(SuperLearner = {     mcSuperLearner(Y = iris$Petal.Width, X = iris[, -4], SL.library = sl_lib,          cvControl = list(V = 5)) }) #>       min       lq     mean   median       uq      max neval #>  1.203083 1.221367 1.231049 1.230699 1.238238 1.265271    10  iris_SuperLearner_cv_sl_benchmark <- system.time({   CV.SuperLearner(     Y = iris$Petal.Width,      X = iris[, -14],      SL.library = sl_lib,     parallel = 'multicore',     V = 5) }) iris_SuperLearner_cv_sl_benchmark #>    user  system elapsed  #>   3.776   0.241   5.852 sl_timing <- data.frame(   package = c('nadir', 'sl3', 'SuperLearner'),   time = c(mean(iris_nadir_sl_benchmark$time) / 1e9, # convert nanoseconds to seconds            mean(iris_sl3_sl_benchmark$time) / 1e9,            mean(iris_SuperLearner_sl_benchmark$time) / 1e9) )  sl_timing |>    mutate(package = forcats::fct_reorder(package, time)) |>    ggplot(aes(x = time, y = package, fill = package)) +    geom_col() +    geom_point() +    scale_fill_brewer(palette = 'Set2') +    theme_bw() +    ggtitle(\"Time taken to super learn on the iris dataset across 6 candidate learners\") +    labs(caption = \"Task: Petal.Width ~ . on the iris dataset, 150 rows, 5 features\") +     theme(plot.caption.position = 'plot') cv_timing <- data.frame(   package = c('nadir', 'sl3', 'SuperLearner'),   time = c(     iris_nadir_cv_sl_benchmark[[3]], # user time     iris_sl3_cv_sl_benchmark[[3]],     iris_SuperLearner_cv_sl_benchmark[[3]]) ) cv_timing |>   dplyr::mutate(package = forcats::fct_reorder(package, time)) |>    ggplot(aes(x = time, y = package, fill = package)) +    geom_col() +    geom_point() +    scale_fill_brewer(palette = 'Set2') +    theme_bw() +    ggtitle(\"Elapsed time taken to run cross-validated super learner on the iris dataset across 6 candidate learners\",           \"Elapsed time (as opposed to user or system time) shows the amount of time experienced by the user.\") +    labs(caption = \"Task: Petal.Width ~ . on the iris dataset, 150 rows, 5 features\") +     theme(plot.caption.position = 'plot')"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"penguins-data-16-8-kb","dir":"Articles","previous_headings":"","what":"penguins data (16.8 KB)","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"","code":"penguins <- palmerpenguins::penguins penguins <- penguins[complete.cases(penguins),]  flipper_length_formula <-   flipper_length_mm ~ species + island + bill_length_mm +     bill_depth_mm + body_mass_g + sex"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"nadir-on-penguins-data-16-8-kb","dir":"Articles","previous_headings":"penguins data (16.8 KB)","what":"nadir on penguins data (16.8 KB)","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"","code":"penguins_nadir_sl_benchmark <- microbenchmark(   times = 10,   nadir = {     nadir::super_learner(       data = penguins,       formula = flipper_length_formula,       learners = list(lnr_mean, lnr_lm, lnr_rf, lnr_earth, lnr_glmnet, lnr_xgboost)     )   } ) penguins_nadir_sl_benchmark #> Unit: seconds #>   expr      min       lq     mean   median       uq      max neval #>  nadir 1.157808 1.192269 1.290731 1.218286 1.309779 1.728092    10  penguins_nadir_cv_sl_benchmark <- system.time({   cv_super_learner(     data = penguins,     formula = flipper_length_formula,     learners = list(lnr_mean, lnr_lm, lnr_rf, lnr_earth, lnr_glmnet, lnr_xgboost)   ) }) #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE penguins_nadir_cv_sl_benchmark #>    user  system elapsed  #>   7.037   1.595   2.599"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"sl3-on-penguins-data-16-8-kb","dir":"Articles","previous_headings":"penguins data (16.8 KB)","what":"sl3 on penguins data (16.8 KB)","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"","code":"task <- make_sl3_Task(   data = penguins,   outcome = \"flipper_length_mm\",   covariates = c(\"species\",                  \"island\",                  \"bill_length_mm\",                  \"bill_depth_mm\",                  \"body_mass_g\",                  \"sex\") )  lrn_mean <- Lrnr_mean$new() lrn_lm <- Lrnr_glm$new() lrn_rf <- Lrnr_randomForest$new() lrn_earth <- Lrnr_earth$new() lrn_glmnet <- Lrnr_glmnet$new() lrn_xgboost <- Lrnr_xgboost$new()  stack <- Stack$new(lrn_mean, lrn_lm, lrn_rf, lrn_earth, lrn_glmnet, lrn_xgboost)  sl <- Lrnr_sl$new(learners = stack, metalearner = Lrnr_nnls$new(),                   cv_control = list(V = 5))  penguins_sl3_sl_benchmark <- microbenchmark::microbenchmark(   times = 10,   list(     sl3 = { sl_fit <- sl$train(task = task) }   ) ) penguins_sl3_sl_benchmark #> Unit: seconds #>                                                 expr      min       lq     mean #>  list(sl3 = {     sl_fit <- sl$train(task = task) }) 1.314009 1.465803 2.642716 #>    median       uq      max neval #>  1.640787 1.831675 9.482718    10   sl_fit <- sl$train(task = task) penguins_sl3_cv_sl_benchmark <- system.time({   sl3_cv = { cv_sl(lrnr_sl = sl_fit, eval_fun = loss_squared_error) } }) #> [1] \"Cross-validated risk:\" #> Key: <learner> #>                                    learner       MSE        se   fold_sd #>                                     <fctr>     <num>     <num>     <num> #> 1:                               Lrnr_mean 196.47868 10.979336 33.704733 #> 2:                           Lrnr_glm_TRUE  28.08063  2.496742  6.675033 #> 3:            Lrnr_randomForest_500_TRUE_5  28.82479  2.349832  5.264787 #> 4:         Lrnr_earth_2_3_backward_0_1_0_0  29.00131  2.650027  5.636709 #> 5: Lrnr_glmnet_NULL_deviance_10_1_100_TRUE  28.39991  2.504920  6.139148 #> 6:                       Lrnr_xgboost_20_1  32.54006  2.627296  6.469929 #> 7:                            SuperLearner  19.35342  1.825590  6.853104 #>    fold_min_MSE fold_max_MSE #>           <num>        <num> #> 1:    139.88804    238.52211 #> 2:     16.23283     36.93534 #> 3:     17.60123     36.40382 #> 4:     21.20477     37.72809 #> 5:     19.34574     37.51000 #> 6:     21.36498     42.66572 #> 7:     11.12754     35.96351 penguins_sl3_cv_sl_benchmark #>    user  system elapsed  #>  38.712  48.137  44.837"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"superlearner-on-penguins-16-8-kb-data","dir":"Articles","previous_headings":"penguins data (16.8 KB)","what":"SuperLearner on penguins (16.8 KB) data","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"Visual comparison","code":"sl_lib = c(    \"SL.mean\", \"SL.lm\", \"SL.randomForest\", \"SL.earth\", \"SL.glmnet\", \"SL.xgboost\")  penguins_SuperLearner_sl_benchmark <- microbenchmark::microbenchmark(   times = 10,    list(SuperLearner = {     mcSuperLearner(Y = penguins$flipper_length_mm,                    X = penguins[, c(\"species\",                                     \"island\",                                     \"bill_length_mm\",                                     \"bill_depth_mm\",                                     \"body_mass_g\",                                     \"sex\")],                   SL.library = sl_lib,                  cvControl = list(V = 5))   }   ) ) penguins_SuperLearner_sl_benchmark #> Unit: seconds #>                                                                                                                                                                                                                                              expr #>  list(SuperLearner = {     mcSuperLearner(Y = penguins$flipper_length_mm, X = penguins[,          c(\"species\", \"island\", \"bill_length_mm\", \"bill_depth_mm\",              \"body_mass_g\", \"sex\")], SL.library = sl_lib, cvControl = list(V = 5)) }) #>       min       lq     mean   median       uq      max neval #>  2.112144 2.154183 2.174983 2.175506 2.202207 2.224516    10  num_cores = RhpcBLASctl::get_num_cores()  penguins_SuperLearner_cv_sl_benchmark <- system.time({   CV.SuperLearner(     Y = penguins$flipper_length_mm,     X = penguins[, c(\"species\",                      \"island\",                      \"bill_length_mm\",                      \"bill_depth_mm\",                      \"body_mass_g\",                      \"sex\")],      SL.library = sl_lib,     parallel = 'multicore',     V = 5) }) penguins_SuperLearner_cv_sl_benchmark #>    user  system elapsed  #>   8.211   0.295  12.642 sl_timing <- data.frame(   package = c('nadir', 'sl3', 'SuperLearner'),   time = c(mean(penguins_nadir_sl_benchmark$time) / 1e9, # convert nanoseconds to seconds            mean(penguins_sl3_sl_benchmark$time) / 1e9,            mean(penguins_SuperLearner_sl_benchmark$time) / 1e9) )  ggplot(sl_timing, aes(x = time, y = package, fill = package)) +    geom_col() +    geom_point() +    scale_fill_brewer(palette = 'Set2') +    theme_bw() +    ggtitle(\"Time taken to super learn on the penguins dataset across 6 candidate learners\") +    labs(caption = \"Task: flipper_length_mm ~ . on the penguins dataset, 333 rows, 8 features\") +    theme(plot.caption.position = 'plot') cv_timing <- data.frame(   package = c('nadir', 'sl3', 'SuperLearner'),   time = c(     penguins_nadir_cv_sl_benchmark[[3]], # user time     penguins_sl3_cv_sl_benchmark[[3]],     penguins_SuperLearner_cv_sl_benchmark[[3]]) ) cv_timing |>   dplyr::mutate(package = forcats::fct_reorder(package, time)) |>    ggplot(aes(x = time, y = package, fill = package)) +    geom_col() +    geom_point() +    scale_fill_brewer(palette = 'Set2') +    theme_bw() +    ggtitle(\"Elapsed time taken to run cross-validated super learner on the penguins dataset across 6 candidate learners\",           \"Elapsed time (as opposed to user or system time) shows the amount of time experienced by the user.\") +    labs(caption = \"Task: flipper_length_mm ~ . on the penguins dataset, 333 rows, 8 features\") +    theme(plot.caption.position = 'plot')"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"tornados-data-2-8-mb","dir":"Articles","previous_headings":"","what":"tornados data (2.8 MB)","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"","code":"# recommendations from here for dealing with large data  # in future multicore setup https://github.com/satijalab/seurat/issues/1845  options(future.globals.maxSize = 8000 * 1024^2)   tuesdata <- tidytuesdayR::tt_load('2023-05-16') tornados <- tuesdata$tornados tornados <- tornados[,c('yr', 'mo', 'dy', 'mag', 'st', 'inj', 'fat', 'loss')] tornados <- tornados[complete.cases(tornados),]  # these states appear only very infrequently, like 2 and 1 times respectively — DC and Alaska  tornados <- tornados |> dplyr::filter(!st %in% c('DC', 'AK'))  tornado_formula <- inj ~ yr + mo + mag + fat + st + loss"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"nadir-on-tornados-data-2-8-mb","dir":"Articles","previous_headings":"tornados data (2.8 MB)","what":"nadir on tornados data (2.8 MB)","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"","code":"tornados_nadir_sl_benchmark <- system.time({   super_learner(     data = tornados,     formula = tornado_formula,     learners = list(lnr_mean, lnr_lm, lnr_rf, lnr_earth, lnr_glmnet, lnr_xgboost),     cv_schema = cv_character_and_factors_schema   ) }) tornados_nadir_sl_benchmark"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"sl3-on-tornados-data-2-8-mb","dir":"Articles","previous_headings":"tornados data (2.8 MB)","what":"sl3 on tornados data (2.8 MB)","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"","code":"task <- make_sl3_Task(   data = tornados,   outcome = \"inj\",   covariates = c(\"yr\", \"mo\", \"dy\", \"mag\",                   \"st\", \"fat\", \"loss\") )  lrn_mean <- Lrnr_mean$new() lrn_lm <- Lrnr_glm$new() lrn_rf <- Lrnr_randomForest$new() lrn_earth <- Lrnr_earth$new() lrn_glmnet <- Lrnr_glmnet$new() lrn_xgboost <- Lrnr_xgboost$new()  stack <- Stack$new(lrn_mean, lrn_lm, lrn_rf, lrn_earth, lrn_glmnet, lrn_xgboost)  sl <- Lrnr_sl$new(learners = stack, metalearner = Lrnr_nnls$new(),                   cv_control = list(V = 5))  tornados_sl3_sl_benchmark <- system.time({   sl_fit <- sl$train(task = task) }) tornados_sl3_sl_benchmark"},{"path":"https://ctesta01.github.io/nadir/articles/Benchmarking.html","id":"superlearner-on-tornados-data-2-8-mb","dir":"Articles","previous_headings":"tornados data (2.8 MB)","what":"SuperLearner on tornados data (2.8 MB)","title":"Benchmarking Against `{SuperLearner}` and `{sl3}`","text":"Unfortunately unable get results SuperLearner package tornados dataset, produced obscure error little guidance debug error presently available. Visualizing Results","code":"sl_lib = c(    \"SL.mean\", \"SL.lm\", \"SL.randomForest\", \"SL.earth\", \"SL.glmnet\", \"SL.xgboost\")  tornados_SuperLearner_sl_benchmark <- system.time({     mcSuperLearner(Y = tornados$inj,                    X = tornados[, c(\"yr\", \"mo\", \"dy\", \"mag\",                   \"st\", \"fat\", \"loss\")],                   SL.library = sl_lib,                  cvControl = list(V = 5))   }) sl_timing <- data.frame(   package = c('nadir', 'sl3', 'SuperLearner'),   time = c(tornados_nadir_sl_benchmark[[3]],     tornados_sl3_sl_benchmark[[3]],     NA))  sl_timing |>    ggplot(aes(y = package, x = time, fill = package)) +    geom_col() +    geom_point() +    xlab(\"Seconds\") +    geom_text(     data = data.frame(package = 'SuperLearner', label = 'failed to run', time = 200),     mapping = aes(label = label)) +    theme_bw() +    scale_fill_brewer(palette = 'Set2') +    ggtitle(\"Time taken to super learn on the tornados dataset across 6 candidate learners\") +    labs(caption = \"Task: inj ~ . on the tornados dataset, 41,514 rows, 8 features, 2.8 MB\") +    theme(plot.caption.position = 'plot', panel.grid.major.y = element_blank())"},{"path":"https://ctesta01.github.io/nadir/articles/Binary-and-Multiclass-Outcomes.html","id":"binary-outcomes-","dir":"Articles","previous_headings":"","what":"Binary Outcomes.","title":"Binary and Multiclass Outcomes","text":"example, show using Boston dataset creating binary outcome regression problem, train super_learner() predict binary outcome. handle binary outcomes, need adjust method determining weights. don’t want use default mse() loss function, instead rely using negative log likelihood loss held-data. appropriately context binary data, want make sure loss function used determine_weights_for_binary_outcomes() function provided nadir. can either setting outcome_type = 'binary' passing function directly super_learner() argument determine_super_learner_weights = determine_weights_for_binary_outcomes.   important thing know constructing learners binary outcome context determine_weights_for_binary_outcomes() requires outputs learners newdata predictions outcome equal 1.","code":"data('Boston', package = 'MASS')  # create a binary outcome to predict Boston$high_crime <- as.integer(Boston$crim > mean(Boston$crim)) data <- Boston |> dplyr::select(-crim)  # train a super learner on a binary outcome trained_binary_super_learner <- super_learner(   data = data,   formula = high_crime ~ nox + rm + age + tax + ptratio,   learners = list(     logistic = lnr_logistic, # the same as a lnr_glm with extra_learner_args                               # set to list(family = binomial(link = 'logit'))                              # for that learner     rf = lnr_rf_binary,  # random forest     lm = lnr_lm), # linear probability model   outcome_type = 'binary',   verbose = TRUE )  # let's take a look at the learned weights trained_binary_super_learner$learner_weights #>     logistic           rf           lm  #> 1.000000e+00 2.691182e-16 4.748858e-20  # what are the predictions? you can think of them as \\hat{P}(Y = 1 | X). # i.e., predictions of P(Y = 1) given X where Y and X are the left & right hand # side of your regression formula(s) head(trained_binary_super_learner$sl_predict(data)) #>            1            2            3            4            5            6  #> 2.772297e-05 1.034418e-05 5.749397e-06 2.472188e-06 3.237557e-06 3.784687e-06  # classification table data.frame(   truth = data$high_crim,    prediction = round(trained_binary_super_learner$sl_predict(data))) |>    dplyr::group_by(truth, prediction) |>    dplyr::count() |>    ggplot2::ggplot(mapping = ggplot2::aes(y = truth, x = prediction, fill = n, label = n)) +    ggplot2::geom_tile() +    ggplot2::geom_label(fill = 'white', alpha = .7) +    ggplot2::scale_fill_distiller(palette = 'Oranges', direction = 1) +    ggplot2::scale_x_continuous(breaks = c(0, 1)) +    ggplot2::scale_y_continuous(breaks = c(0, 1)) +    ggplot2::xlab(\"super_learner() prediction\") +    ggplot2::theme_minimal() +    ggplot2::theme(legend.position = 'none') +    ggplot2::ggtitle(\"Classification Table\") data.frame(   truth = data$high_crim,    predicted_pr_of_1 = trained_binary_super_learner$sl_predict(data)) |>    ggplot2::ggplot(mapping = ggplot2::aes(x = predicted_pr_of_1)) +    ggplot2::geom_histogram() +    ggplot2::facet_grid(truth ~ ., labeller = ggplot2::labeller(truth = ~ paste0('truth: ', .))) +    ggplot2::theme_bw() +    ggplot2::theme(panel.grid.minor = ggplot2::element_blank()) +    ggplot2::xlab(bquote(paste(\"super_learner() predictions, \", hat(bold(P)), '(Y = 1)'))) +    ggplot2::ggtitle(\"Classification Task\")  #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"https://ctesta01.github.io/nadir/articles/Binary-and-Multiclass-Outcomes.html","id":"multiclass-regression-i-e--multinomial-regression","dir":"Articles","previous_headings":"","what":"Multiclass Regression, i.e., Multinomial Regression","title":"Binary and Multiclass Outcomes","text":"covered binary classification — now turn classification problems dependent variable one discrete number unique levels. nadir includes two learners designed multiclass regression problems: lnr_multinomial_vglm lnr_multinomial_nnet. can perform super learning classification problem like classifying penguins’ species palmerpenguins dataset.","code":"library(palmerpenguins)  df <- penguins[complete.cases(penguins),]  sl_learned_model <- super_learner(   data = df,   formulas = list(     .default = species ~ flipper_length_mm + bill_depth_mm,     nnet2 = species ~ poly(flipper_length_mm, 2) + poly(bill_depth_mm, 2) + body_mass_g,     nnet3 = species ~ flipper_length_mm * bill_depth_mm + island     ),   learners = list(     nnet1 = lnr_multinomial_nnet,     nnet2 = lnr_multinomial_nnet,     nnet3 = lnr_multinomial_nnet,     vglm = lnr_multinomial_vglm     ),   outcome_type = 'multiclass',   verbose = TRUE)  compare_learners(sl_learned_model, loss_metric = negative_log_loss) #> # A tibble: 1 × 4 #>   nnet1 nnet2 nnet3  vglm #>   <dbl> <dbl> <dbl> <dbl> #> 1  120.  119.  156.  120.  round(sl_learned_model$learner_weights, 3) #> nnet1 nnet2 nnet3  vglm  #> 0.000 0.000 0.956 0.044"},{"path":"https://ctesta01.github.io/nadir/articles/Clustered-Data.html","id":"high-level-summary","dir":"Articles","previous_headings":"","what":"High Level Summary","title":"Clustered and Dependent Data","text":"important data completely independent, don’t want ‘leakage’ information training test splits across cross-validation splits. particularly useful categorical features, model may error prediction model never seen particular level categorical variable training. assumptions present using cross validation estimate risk unseen data done super learner algorithm include training validation splits independent samples underlying data distribution. data come clusters otherwise dependent samples, need make sure cross-validation split clusters putting observations training splits others validation split cluster. Read Practical considerations specifying super learner Rachel Phillips et al (2023, Int J Epidemiology) https://academic.oup.com/ije/article/52/4/1276/7076266. data ..d., clustered observations must assigned group validation training sets. ensures validation data completely independent training data, loss function evaluated cluster level. V-fold CV clustered data specified existing SL software supplying cluster identifier ‘id’ argument. quote describing syntax SuperLearner package, try achieve something similar. just need specify schema using origami follows. Behind scenes, cluster_ids strata_ids passed super_learner(), cv_schema argument set Let’s check see explicitly cv_origami_schema working: can see cv_origami_schema assigns cluster either training validation data split. fact, can get equivalent results run following code, shows explicitly pass user-chosen origami folds_* function:","code":"library(nadir) ## Registered S3 method overwritten by 'future': ##   method               from       ##   all.equal.connection parallelly library(origami) ## origami v1.0.7: Generalized Framework for Cross-Validation # generate synthetic clustered data  n_clusters <- 25 n_participants <- 100 cluster_ids <- sample.int(25, 100, replace = TRUE)  age <- sample.int(100, n_participants, replace = TRUE) drug <- sample(x = c(0, 1), n_participants, replace = TRUE)  cluster_mean_outcomes <- rnorm(mean = 25, sd = 5, n = n_clusters)  participant_outcomes <-    cluster_mean_outcomes[cluster_ids] +    drug * 15 +    age   df <- data.frame(   cluster_id = cluster_ids,   age = age,    drug = drug,    outcome = participant_outcomes)  set.seed(1234) sl_model <- super_learner(   data = df,   formulas = outcome ~ age + drug,   learners = list(lnr_rf, lnr_lm, lnr_earth, lnr_glmnet),   cluster_ids = df$cluster_id,   verbose = TRUE ) cv_schema <- function(data, n_folds) {   cv_origami_schema(data, n_folds = n_folds,      fold_fun = origami::folds_vfun,      cluster_ids = cluster_ids,     strata_ids = strata_ids   ) } df_splits <- cv_origami_schema(   data = df,n_folds = 5, fold_fun = origami::folds_vfold, cluster_ids = df$cluster_id)  unisort <- \\(x) sort(unique(x))  unisort(df_splits$training_data[[1]]$cluster_id) ##  [1]  2  3  4  5  7  8  9 12 13 14 15 16 17 18 19 20 21 23 24 25 unisort(df_splits$validation_data[[1]]$cluster_id) ## [1]  1  6 10 11 22 unisort(df_splits$training_data[[2]]$cluster_id) ##  [1]  1  2  4  5  6  7  8  9 10 11 14 15 16 17 20 21 22 23 24 25 unisort(df_splits$validation_data[[2]]$cluster_id) ## [1]  3 12 13 18 19 set.seed(1234) sl_model2 <- super_learner(   data = df,   formulas = outcome ~ age + drug,   learners = list(lnr_rf, lnr_lm, lnr_earth, lnr_glmnet),   cv_schema = function(data, n_folds) {     cv_origami_schema(data = data, n_folds = n_folds, fold_fun = origami::folds_vfold,                       cluster_ids = data$cluster_id)   },   verbose = TRUE )  sl_model$learner_weights ##        rf        lm     earth    glmnet  ## 0.0000000 0.6891014 0.3108986 0.0000000 sl_model2$learner_weights ##        rf        lm     earth    glmnet  ## 0.0000000 0.6891014 0.3108986 0.0000000"},{"path":"https://ctesta01.github.io/nadir/articles/Clustered-Data.html","id":"strata","dir":"Articles","previous_headings":"High Level Summary","what":"Strata","title":"Clustered and Dependent Data","text":", hand, want ensure strata always represented training sets every cross-validation split data, can use strata_ids argument super_learner(). see works, can look ","code":"df$strata_id <- rep(1:20, each = 5)  df_splits <- cv_origami_schema(   data = df,n_folds = 5, fold_fun = origami::folds_vfold, strata_ids = df$strata_id)  unisort <- \\(x) sort(unique(x))  unisort(df_splits$training_data[[1]]$strata_id) ##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 unisort(df_splits$validation_data[[1]]$strata_id) ##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20"},{"path":"https://ctesta01.github.io/nadir/articles/Creating-Learners.html","id":"simple-learner-examples","dir":"Articles","previous_headings":"","what":"Simple Learner Examples","title":"Creating Learners","text":"","code":"lnr_lm <- function(data, formula, ...) {   model <- stats::lm(formula = formula, data = data, ...)    predict_from_trained_lm <- function(newdata) {     predict(model, newdata = newdata, type = 'response')   }   return(predict_from_trained_lm) }"},{"path":"https://ctesta01.github.io/nadir/articles/Creating-Learners.html","id":"weights","dir":"Articles","previous_headings":"","what":"Weights","title":"Creating Learners","text":"recommend explicitly handling weights argument learners protected argument. internals different algorithms may vary, using names weights instead, recommend standardize weights argument across different learner algorithms. concrete example, ranger::ranger() function takes case.weights argument rather weights. typical learner supports weights might look like: However, model fitting procedures like passing weights = NULL may necessary careful pass default weights = NULL model fitting procedure. example , refer curious reader source lnr_glm https://github.com/ctesta01/nadir/blob/main/R/learners.R.","code":"lnr_supportsWeights <- function(data, formula, weights = NULL, ...) {    # train the model    model <- model_fit(data = data, formula = formula, weights = weights, ...)      return(function(newdata) {     predict(model, newdata = newdata)   }) } model_training_arguments <- list(data = data, formula = formula)      # add weights they aren't missing   if (! is.null(weights) & length(weights) == nrow(data)) {      model_training_arguments$weights <- weights   }"},{"path":"https://ctesta01.github.io/nadir/articles/Creating-Learners.html","id":"attributes","dir":"Articles","previous_headings":"","what":"Attributes","title":"Creating Learners","text":"’s recommended create learners, also give couple attributes couple reasons: learner sl_lnr_name attribute, can automatically used outputs name learner left unspecified. learner sl_lnr_type attribute, checked output_type argument super_learner(). set attributes, making new learner, one run something along lines ","code":"lnr_myNewLearner <- function(data, formula, ...) {   model <- # fit your learner given data, formula, ...        predictor_fn <- function(newdata) {     predict(model, newdata = newdata)   }   return(predictor) } attr(lnr_myNewLearner, 'sl_lnr_name') <- 'newLearnerName' attr(lnr_myNewLearner, 'sl_lnr_type') <- 'continuous' # or c('continuous', 'binary') and similar # see ?nadir_supported_types"},{"path":"https://ctesta01.github.io/nadir/articles/Creating-Learners.html","id":"modifying-predictions","dir":"Articles","previous_headings":"","what":"Modifying Predictions","title":"Creating Learners","text":"Occasionally settings like enforce constraint model behavior. One example clipping predicted values pre-specified range. concept often relevant logistic regression (.e., binary outcomes) setting, may broadly applicable. show truncate predictions learner specified range illustrative example. many learners, possible specify family = binomial(link = 'logit') similar argument. However, nonetheless settings fitting truncated learner benefits, including sometimes truncated linear probability model lower risk logistic regression model, sometimes truncated models useful manually expanding variety candidate learners used super_learner() algorithm, thereby making likely true data generating mechanism closely estimated fit super_learner(). wondering whether truncated linear probability model binary outcomes misspecified, see following section FAQ: FAQ: think “misspecified” learners?","code":"truncate_lnr <- function(lnr, min, max) {   truncate <- function(x, min, max) {     pmax(pmin(x, max), min)   }      # we want to return a modified learner: so in other words, we want to return a   # function that takes in a learner's inputs and returns a prediction function,   # just with the added truncation.   return(     function(...) {       predictor_fn <- lnr(...)        truncated_predictor_fn <- function(...) {         truncate(predictor_fn(...), min, max)       }       return(truncated_predictor_fn)     }   ) }  # create the new, truncated learner lnr_truncated_hal <- truncate_lnr(lnr = lnr_hal, min = 0, max = 1)  # fit the learner learned_hal_model <- lnr_truncated_hal(data = mtcars, am ~ .)  # produce predictions, for example learned_hal_model(mtcars) #>  [1] 0.699742339 0.664010374 0.695709519 0.000000000 0.000000000 0.060095349 #>  [7] 0.000000000 0.511040606 0.283893910 0.390855426 0.361627216 0.194663708 #> [13] 0.057058232 0.009506522 0.000000000 0.015485391 0.047048910 0.807818383 #> [19] 0.837018020 0.857143078 0.590793239 0.225453528 0.141459463 0.233407327 #> [25] 0.000000000 0.820616609 0.760721447 0.750925569 0.192624472 0.777093601 #> [31] 0.354637109 0.630647486   # in contrast, if we had not truncated this learner, we could have gotten # predictions outside of [0, 1]: learned_hal_not_truncated <- lnr_hal(data = mtcars, am ~ .) range(learned_hal_not_truncated(mtcars)) #> [1] -0.4658581  1.0268739"},{"path":"https://ctesta01.github.io/nadir/articles/Density-Estimation.html","id":"basic-syntax-for-fitting-density-models","dir":"Articles","previous_headings":"","what":"Basic Syntax for Fitting Density Models","title":"Density Estimation","text":"fit conditional density model, need things: dataset select outcome, predictor variables (together define formula) collection candidate learners use negative log loss function super learner algorithm choosing among learners.","code":"# specify our data and regression problem  # ---------------------------------------  # in order to build a weighting based estimator, we might fit a conditional # density model of our continuous exposure  data(\"Boston\", package = \"MASS\")  # we're going to regress the crime measurement variable in the Boston dataset # on all the other variables.  #  # and we want to control for the following potential confounders (all other variables in the Boston dataset)  #  # so we want to regress crime on all the other variables in the Boston dataset  reg_formula <- crim ~ .    # specify a collection of candidate learners # ------------------------------------------  # we'll use a normal error model with a linear conditional mean model,  # and random forest, ranger, lm, and earth with kernel bandwidth density() models for the  # error distribution  # nadir provides the helper function lnr_homoskedastic_density that can be used # to build a homoskedastic density learner out of a given conditional mean learner.  lnr_rf_homoskedastic_density <- function(data, formula, ...) {    lnr_homoskedastic_density(data, formula, mean_lnr = lnr_rf, ...) }  lnr_ranger_homoskedastic_density <- function(data, formula, ...) {    lnr_homoskedastic_density(data, formula, mean_lnr = lnr_ranger, ...) }  lnr_lm_homoskedastic_density <- function(data, formula, ...) {    lnr_homoskedastic_density(data, formula, mean_lnr = lnr_lm, ...) }  lnr_earth_homoskedastic_density <- function(data, formula, ...) {    lnr_homoskedastic_density(data, formula, mean_lnr = lnr_earth, ...) }  # we recommend setting the sl_lnr_type attribute to avoid warnings from super_learner() #  # this tells the super_learner() function that these learners are designed # to work with density estimation problems. #  # without this, users will see a friendly warning asking them if they're sure  # they want to use the given learners with the outcome_type = 'density'. #  attr(lnr_rf_homoskedastic_density, 'sl_lnr_type') <- 'density' attr(lnr_ranger_homoskedastic_density, 'sl_lnr_type') <- 'density' attr(lnr_lm_homoskedastic_density, 'sl_lnr_type') <- 'density' attr(lnr_earth_homoskedastic_density, 'sl_lnr_type') <- 'density'   # super learning:  # choosing weights for candidate learners and ensembling # ======================================================  learned_sl_density_model <- super_learner(   data = Boston,   formula = reg_formula,   learners = list(     normal = lnr_lm_density,     ranger = lnr_ranger_homoskedastic_density,     earth = lnr_earth_homoskedastic_density,     lm = lnr_lm_homoskedastic_density     ),   outcome_type = 'density',   verbose = TRUE )  # if you prefer not to code up the new learners as above, another way to perform # the same super learning as above is to use the `extra_learner_args` option # with named learners as follows: # # learned_sl_density_model <- super_learner( #   data = Boston, #   formula = reg_formula, #   learners = list( #     normal = lnr_lm, #     ranger = lnr_homoskedastic_density, #     earth = lnr_homoskedastic_density, #     lm = lnr_lm_homoskedastic_density #     ), #   extra_learner_args = list( #     ranger = list(mean_lnr = lnr_ranger), #     earth = list(mean_lnr = lnr_earth), #     lm = list(mean_lnr = lnr_lm)), #   outcome_type = 'density', #   verbose = TRUE # ) #  # one reason it can be useful to create the learners explicitly is to be able to # visualize and debug their behaviors individually.   # fit diagnostics # ===============  # two things we might want to do with a fit super learner model are: # 1) see how each candidate learner performed with regard to the specified loss #    function # 2) see the weights assigned to each learner (how favored they are in the final #    ensemble model). # # to be able to see these diagnostics, it's important that super_learner() is # run with vebose = TRUE otherwise these extra details are not stored in the  # output object.   # 1) compare the learners using negative log likelihood loss compare_learners(learned_sl_density_model, loss_metric = negative_log_loss) #> # A tibble: 1 × 4 #>   normal ranger earth    lm #>    <dbl>  <dbl> <dbl> <dbl> #> 1  1716.  1055. 1403. 1473.  # 2) see the weights given to each candidate learner learned_sl_density_model$learner_weights #>     normal     ranger      earth         lm  #> 0.07190413 0.87099445 0.02994368 0.02715774"},{"path":"https://ctesta01.github.io/nadir/articles/Density-Estimation.html","id":"lets-validate-that-conditional-density-works-the-way-it-should","dir":"Articles","previous_headings":"","what":"Let’s validate that conditional density works the way it should","title":"Density Estimation","text":"One test can check conditional density estimation working correctly condition set predictors, produced probability density function integrate 1 – , area curve equal 1. show example test :","code":"lm_density_predict <- lnr_lm_homoskedastic_density(Boston, reg_formula)  f_lm <- function(ys) {   x <- Boston[1,]   sapply(ys, function(y) {     x[['crim']] <- y     lm_density_predict(x)   }) }  integrate(f_lm, min(Boston$crim) - sd(Boston$crim), max(Boston$crim) + sd(Boston$crim), subdivisions = 10000) #> 0.981094 with absolute error < 6.9e-05   earth_density_predict <- lnr_earth_homoskedastic_density(Boston, reg_formula, density_args = list(bw = 30))  f_earth <- function(ys) {   x <- Boston[1,]   sapply(ys, function(y) {     x[['crim']] <- y     earth_density_predict(x)   }) } earth_density_predict(Boston[1,]) #> [1] 0.01315833  # we're looking for this number to be very close to 1: integrate(f_earth, min(Boston$crim) - 10*sd(Boston$crim), max(Boston$crim) + 10*sd(Boston$crim)) #> 0.9987836 with absolute error < 5.4e-05   # sometimes if integrate() has trouble, it's useful to perform the integration by hand: y_seq <- seq(min(Boston$crim) - 10*sd(Boston$crim), max(Boston$crim) + 10*sd(Boston$crim), length.out = 10000) delta_y <- y_seq[2]-y_seq[1] sum(f_earth(y_seq)*delta_y) # we're looking for this to be close to 1 #> [1] 0.9987911"},{"path":"https://ctesta01.github.io/nadir/articles/Density-Estimation.html","id":"heteroskedastic-learners","dir":"Articles","previous_headings":"","what":"Heteroskedastic Learners","title":"Density Estimation","text":"example 2D (e.g., one predictor one outcome variable), practical settings may estimating density one variable conditional many variables. syntax easily extends follows, recalling reg_formula instantiated ~, crim, .. ’ve omitted visualizing predictions look like model. general, can useful spot check whether integration working correctly, certain heuristics hold, can hard reason 2D slice higher dimensional conditional density model appear. example, intuition begins fail higher dimensional conditional density models density higher data points clustered: one conditions set 2 parameter values visualizes grid like example remaining predictor outcome variables, set variables conditioned may consistent higher density region outside bulk data distribution conditioning set consistent outliers. general, higher dimensional models, prefer rely checks like “density integrate 1?” conditioning full set predictors integrating possible values outcome.","code":"lnr_earth_mean_glm_var_heteroskedastic_density <- function(data, formula, ...) {    lnr_heteroskedastic_density(     data,     formula,     mean_lnr = lnr_earth,     var_lnr = lnr_lm,     density_args = list(bw=5),     ...   ) }  Boston$log_crim <- log(Boston$crim)  earth_mean_glm_var_heteroskedastic_predict <-   lnr_earth_mean_glm_var_heteroskedastic_density(Boston[,-1], log_crim ~ medv)  earth_mean_glm_var_heteroskedastic_predict(Boston[1,]) #> [1] 0.07019618  f_earth_glm <- function(ys) {   x <- Boston[2,]   sapply(ys, function(y) {     x[['log_crim']] <- y     earth_mean_glm_var_heteroskedastic_predict(x)   }) }  # check that the estimated density has area-under-the-curve = 1 integrate(   f_earth_glm,   min(Boston$log_crim) - 10 * sd(Boston$log_crim),   max(Boston$log_crim) + 10 * sd(Boston$log_crim),   subdivisions = 1000 ) #> 1.590631 with absolute error < 0.00015  # create a grid to predict densities over prediction_grid <- expand.grid(   log_crim = log(seq(min(Boston$crim), max(Boston$crim), length.out = 100)),   medv = seq(min(Boston$medv), max(Boston$medv), length.out = 100))  # calculate densities for every grid-point prediction_grid$density_prediction <- earth_mean_glm_var_heteroskedastic_predict(   prediction_grid)  # visualize the density model across a grid of outcomes and predictors ggplot2::ggplot(Boston, aes(x = medv, y = exp(log_crim))) +    geom_tile(     data = prediction_grid,      mapping = aes(fill = density_prediction)) +    geom_point() +    theme_minimal() +    MetBrewer::scale_fill_met_c('Hiroshige', direction = -1) +    ggtitle(\"Heteroskedastic Density Estimation Model\",            \"(Mean model: earth, Variance model: GLM)\") lnr_earth_glm_heteroskedastic_density <- function(data, formula, ...) {    lnr_heteroskedastic_density(data, formula, mean_lnr = lnr_earth,                              var_lnr = lnr_mean,                              density_args = list(bw = 3),                             ...) }  lnr_earth_glm_heteroskedastic_predict <-   lnr_earth_glm_heteroskedastic_density(Boston, reg_formula)  lnr_earth_glm_heteroskedastic_predict(Boston[1,]) #> [1] 0.1280906  f_earth_mean <- function(ys) {   x <- Boston[1,]   sapply(ys, function(y) {     x[['crim']] <- y     lnr_earth_glm_heteroskedastic_predict(x)   }) }  integrate(f_earth, min(Boston$crim) - sd(Boston$crim), max(Boston$crim) + sd(Boston$crim)) #> 0.6147477 with absolute error < 5.8e-05"},{"path":"https://ctesta01.github.io/nadir/articles/Density-Estimation.html","id":"d-density-estimation","dir":"Articles","previous_headings":"","what":"2D Density Estimation","title":"Density Estimation","text":"Perhaps visualizing density estimation 2--3 dimensions help people understand ’s going . Suppose regression problem regress flipper length bill depth palmer penguins dataset. ’ll fit conditional density models visualize predictions.","code":"library(palmerpenguins) library(MetBrewer)  data <- penguins[,c('flipper_length_mm', 'bill_depth_mm')] data <- data[complete.cases(data),]  # conditional normal example:  learned_conditional_normal <- lnr_lm_density(data, flipper_length_mm ~ bill_depth_mm)  prediction_grid <- expand.grid(   flipper_length_mm = seq(172, 231, length.out = 100),   bill_depth_mm = seq(13.1, 21.5, length.out = 100))  prediction_grid$density_prediction <- learned_conditional_normal(prediction_grid)  ggplot(data, aes(x = bill_depth_mm, y = flipper_length_mm)) +    geom_tile(     data = prediction_grid,      mapping = aes(fill = density_prediction)) +    geom_point() +    theme_minimal() +    MetBrewer::scale_fill_met_c('Hiroshige', direction = -1) +    ggtitle(\"Conditionally Normal Density Model\") # earth and stats::density  learned_earth_homoskedastic_density <-   lnr_earth_homoskedastic_density(data, flipper_length_mm ~ bill_depth_mm)  prediction_grid$density_prediction <- learned_earth_homoskedastic_density(prediction_grid)  ggplot(data, aes(x = bill_depth_mm, y = flipper_length_mm)) +    geom_tile(     data = prediction_grid,      mapping = aes(fill = density_prediction)) +    geom_point() +    theme_minimal() +    MetBrewer::scale_fill_met_c('Hiroshige', direction = -1) +    ggtitle(\"Using earth::earth for conditional mean and stats::density for error\") # randomForest and stats::density  learned_rf_homoskedastic_density <-   lnr_rf_homoskedastic_density(data, flipper_length_mm ~ bill_depth_mm)  prediction_grid$density_prediction <- learned_rf_homoskedastic_density(prediction_grid)  ggplot(data, aes(x = bill_depth_mm, y = flipper_length_mm)) +    geom_tile(     data = prediction_grid,      mapping = aes(fill = density_prediction)) +    geom_point() +    theme_minimal() +    MetBrewer::scale_fill_met_c('Hiroshige', direction = -1) +    ggtitle(\"Using randomForest::randomForest for conditional mean and stats::density for error\") # ranger and stats::density  learned_ranger_homoskedastic_density <-   lnr_ranger_homoskedastic_density(data, flipper_length_mm ~ bill_depth_mm)  prediction_grid$density_prediction <- learned_ranger_homoskedastic_density(prediction_grid)  ggplot(data, aes(x = bill_depth_mm, y = flipper_length_mm)) +    geom_tile(     data = prediction_grid,      mapping = aes(fill = density_prediction)) +    geom_point() +   theme_minimal() +    MetBrewer::scale_fill_met_c('Hiroshige', direction = -1) +    ggtitle(\"Using ranger::ranger for conditional mean and stats::density for error\") # glm and stats::density  learned_glm_homoskedastic_density <-   lnr_homoskedastic_density(data, flipper_length_mm ~ bill_depth_mm, mean_lnr = lnr_glm)  prediction_grid$density_prediction <- learned_glm_homoskedastic_density(prediction_grid)  ggplot(data, aes(x = bill_depth_mm, y = flipper_length_mm)) +    geom_tile(     data = prediction_grid,      mapping = aes(fill = density_prediction)) +    geom_point() +    theme_minimal() +    MetBrewer::scale_fill_met_c('Hiroshige', direction = -1) +    ggtitle(\"Using stats::glm for conditional mean and stats::density for error\") #  heteroskedastic method   learned_earth_heteroskedastic_density <-   lnr_heteroskedastic_density(data,                               flipper_length_mm ~ bill_depth_mm,                               mean_lnr = lnr_earth,                               var_lnr = lnr_glm)  prediction_grid$density_prediction <- learned_earth_heteroskedastic_density(prediction_grid)  ggplot(data, aes(x = bill_depth_mm, y = flipper_length_mm)) +    geom_tile(     data = prediction_grid,      mapping = aes(fill = density_prediction)) +    geom_point() +    theme_minimal() +    MetBrewer::scale_fill_met_c('Hiroshige', direction = -1) +    ggtitle(\"Using earth::earth for conditional mean, stats::density for error distribution, \\nand a stats::glm for predicting heteroskedasticity\") # super learner  learned_sl_density <-   super_learner(data,     flipper_length_mm ~ bill_depth_mm,     learners = list(       lm = lnr_lm_density,       earth = lnr_earth_homoskedastic_density,       rf = lnr_rf_homoskedastic_density,       ranger = lnr_ranger_homoskedastic_density,       heteroskedastic = lnr_heteroskedastic_density       ),     extra_learner_args = list(       heteroskedastic = list(mean_lnr = lnr_earth, var_lnr = lnr_glm)       ),     determine_super_learner_weights = determine_weights_using_neg_log_loss) #> Warning in validate_learner_types(learners, outcome_type): Learners 1, 2, 3, 4, 5 with names [lm, earth, rf, ranger, heteroskedastic] do not have attr(., 'sl_lnr_type') == 'continuous'. #> See the Creating Learners article on the {nadir} website. #>   prediction_grid$density_prediction <- learned_sl_density(prediction_grid)  ggplot(data, aes(x = bill_depth_mm, y = flipper_length_mm)) +    geom_tile(     data = prediction_grid,      mapping = aes(fill = density_prediction)) +    geom_point() +    theme_minimal() +    MetBrewer::scale_fill_met_c('Hiroshige', direction = -1) +    ggtitle(\"Using super_learner on all of the above with negative log loss\")"},{"path":"https://ctesta01.github.io/nadir/articles/Doubly-Robust-Estimation.html","id":"background","dir":"Articles","previous_headings":"","what":"Background","title":"Doubly Robust Estimation","text":"useful references doubly robust estimation, refer curious reader : Kang, J. D. Y., & Schafer, J. L. (2007). Demystifying Double Robustness: Comparison Alternative Strategies Estimating Population Mean Incomplete Data. Statistical Science, 22(4). https://doi.org/10.1214/07-sts227 Fine Point 13.2 Technical Point 13.2 Hernán MA, Robins JM (2020). Causal Inference: . Boca Raton: Chapman & Hall/CRC https://miguelhernan.org/whatifbook Funk, M. J., Westreich, D., Wiesen, C., Stürmer, T., Brookhart, M. ., & Davidian, M. (2011). Doubly Robust Estimation Causal Effects. American Journal Epidemiology, 173(7), 761–767. https://doi.org/10.1093/aje/kwq439 Bang, H., & Robins, J. M. (2005). Doubly Robust Estimation Missing Data Causal Inference Models. Biometrics, 61(4), 962–973. https://doi.org/10.1111/j.1541-0420.2005.00377.x Zivich, P. N., & Breskin, . (2021). Machine Learning Causal Inference: Use Cross-fit Estimators. Epidemiology, 32(3), 393–401. https://doi.org/10.1097/ede.0000000000001332 informal, introductory explanation, StitchFix tech blog Multithreaded nice article : Nettiksimmons, J. & Davies, M. (2021). Gimme robust estimator - make double! https://multithreaded.stitchfix.com/blog/2021/07/23/double-robust-estimator/ particular, following paragraph Zivich Breskin nicely summarizes important use doubly robust estimator using machine learning algorithms /cross-fit estimators like nadir::super_learner(): need doubly-robust estimators cross-fitting using data-adaptive machine learning nuisance function estimation arises two terms Von Mises expansion estimator. first term, described empirical process term expansion, can controlled either restricting complexity nuisance models (e.g., requiring Donsker class) cross-fitting. can difficult impossible verify given machine learning method Donsker class, cross-fitting provides simple attractive alternative. second term second-order remainder, converges zero sample size increases. valid inference, desirable remainder term converge function n−1/2, referred root-n convergence. Convergence rates computational issue, rather feature estimator . Unfortunately, data-adaptive algorithms often slower convergence rates result flexibility. However, second-order remainder term doubly-robust estimators product approximation errors treatment outcome nuisance models, doubly-robust estimators require product convergence rates nuisance models n−1/2. summarize, cross-fitting permits use highly complex nuisance models, doubly-robust estimators permit use slowly converging nuisance models. Used together, approaches allow one use wide class data-adaptive machine learning methods estimate causal effects.","code":""},{"path":"https://ctesta01.github.io/nadir/articles/Doubly-Robust-Estimation.html","id":"constructing-an-example-dataset","dir":"Articles","previous_headings":"","what":"Constructing an Example Dataset","title":"Doubly Robust Estimation","text":"create synthetic dataset model effect active drug (treatment) vs. placebo (control) generic health outcome. Additionally, simulated data, treatment assignment mechanism drug arm completely random, rather dependent observed covariates (example, age income). reflect either observational setting participants self-selected treatment arms, randomized setting, assignment arms designed investigators (stochastically) rely age income.","code":"library(dplyr) library(tibble) library(ggplot2) library(patchwork) library(nadir)  sample_size <- 500 # sample size for the simulation dataset  # the simulation dataset has the following properties:  #   the A->Y is confounded by the L->A and L->Y pathways;  #  #   Y_observed is the \"coarsened\" or observed one of the two  #   potential outcomes Y(0) and Y(1).  #  #   Y(0) and Y(1) are constructed as the output of a baseline  #   mean model given L1 and L2 (mu) and a heterogeneous treatment #   effect (tau), again given L1 and L2  df <-    tibble::tibble(     # ages are taken to be between 30 and 50      L1_age = sample(x = seq(30, 50), size = sample_size, replace = TRUE),     # household income is centered around $35,000, with a floor of $5,000      L2_income = rlnorm(n=sample_size, meanlog = log(30000), sdlog = log(3)) + 5000,          # Treatment mechanism: logit depends on age and (log) income     A_treatment = rbinom(       n = sample_size,       size = 1,       prob = plogis(-6 + 0.6 * (L1_age - 40) + 4 * log(L2_income / 40000))     ),          # Outcome model components: baseline mu(L) and heterogeneous effect tau(L)     mu  = 50 + 0.15 * (L1_age - 40) - 2.5 * log(L2_income / 40000),     tau =  4 + 0.05 * (50 - L1_age) + 1.2 * log(L2_income / 40000),        # Potential outcomes and observed outcome     `Y(0)` = rnorm(sample_size, mean = mu,          sd = 1),     `Y(1)` = rnorm(sample_size, mean = mu + tau,    sd = 1),     Y_observed = dplyr::if_else(A_treatment == 1L, `Y(1)`, `Y(0)`)) |>    dplyr::select(-mu, -tau)"},{"path":"https://ctesta01.github.io/nadir/articles/Doubly-Robust-Estimation.html","id":"visualizing-confounding-in-the-dataset","dir":"Articles","previous_headings":"","what":"Visualizing Confounding in the Dataset","title":"Doubly Robust Estimation","text":"visualize treatment-assignment treatment effect outcome confounded age income covariates. Intuitively, way understand problem confounding poses estimating average treatment effect follows: completely randomized regime, ’d like able simply compare treated placebo arm outcomes. However, people treated arm systematically different (distributional sense) people placebo arm. higher income age, average. result, good estimator average treatment effect take systematic difference account.","code":"default_theme <- theme_bw()  p1 <- ggplot(df, aes(x = L1_age, y = A_treatment)) +    geom_jitter(height = 0.025, width = 0.25, alpha = 0.1) +    stat_smooth(formula = \"y ~ x\", method = \"glm\",                  method.args = list(family=\"binomial\"), se = FALSE) +    default_theme +    ggtitle(expression(Age %->% Treatment))   p2 <- ggplot(df, aes(x = L2_income, y = A_treatment)) +    geom_jitter(height = 0.025, width = 0.25, alpha = 0.1) +    stat_smooth(formula = \"y ~ x\", method = \"glm\",                  method.args = list(family=\"binomial\"), se = FALSE) +    default_theme +    scale_x_log10(labels = scales::dollar_format()) +    ggtitle(expression(Income %->% Treatment))  p3 <- ggplot(df, aes(x = L1_age, y = Y_observed)) +    geom_jitter(width = 0.25, height = 0.025, alpha = 0.1) +    geom_smooth(se = FALSE) +    default_theme +    ggtitle(expression(Age %->% Outcome))  p4 <- ggplot(df, aes(x = L2_income, y = Y_observed)) +    geom_point(alpha = 0.1) +    geom_smooth(se = FALSE) +    default_theme +    scale_x_log10(labels = scales::dollar_format()) +    ggtitle(expression(Income %->% Outcome))  patchwork <- { p1 | p2 } / { p3 | p4 }   patchwork + plot_annotation(   title = \"Evidence of Confounding\",   subtitle =     expression(paste(       \"The following four relationships are visualized: \",       list(L[1], L[2] %->% A, L[1], L[2] %->% Y)     )),    theme = theme(plot.title = element_text(size = 18)) ) &    theme(text = element_text(size = 10)) #> `geom_smooth()` using method = 'loess' and formula = 'y ~ x' #> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'"},{"path":"https://ctesta01.github.io/nadir/articles/Doubly-Robust-Estimation.html","id":"determine-the-true-average-treatment-effect","dir":"Articles","previous_headings":"","what":"Determine the True Average Treatment Effect","title":"Doubly Robust Estimation","text":"Since data simulated including poptential outcomes Y(1)Y(1) Y(0)Y(0), able obtain average treatment effect population simple average individual treatment effects. Normally, possible real world, assign treatment placebo individuals order observe potential outcomes.  number, 4.43, ’re looking estimate doubly robust code , without knowledge Y(1) Y(0) instead Y_observed.","code":"# the true average treatment effect true_ate <- mean(df$`Y(1)` - df$`Y(0)`) print(paste0(\"True ATE: \", round(true_ate, 2))) #> [1] \"True ATE: 4.43\"  # visualize the individual treatment effects ggplot(df, aes(x = `Y(1)` - `Y(0)`)) +   geom_histogram(fill = 'cadetblue', alpha = 0.4) +   geom_vline(xintercept = true_ate, color = 'firebrick') +   default_theme +   xlab(\"Individual Treatment Effect\") +   ggtitle(     \"Histogram of the Individual Treatment Effects\",     subtitle = paste0(       \"We can only know these because this is a simulation.  The ATE is shown by the red vertical line.\")) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"https://ctesta01.github.io/nadir/articles/Doubly-Robust-Estimation.html","id":"fitting-the-outcome-and-propensity-score-models","dir":"Articles","previous_headings":"","what":"Fitting the Outcome and Propensity Score Models","title":"Doubly Robust Estimation","text":"reminder, outcome model refers model outcome given treatment received covariates, propensity score refers probability receiving treatment given covariates. mathematical notation, denote mA(L)≡𝔼[Y∣,L], m_A(L) \\equiv \\mathbb{E}\\left[ Y \\mid , L\\right],   π(L)≡ℙ(=1∣L). \\pi(L) \\equiv \\mathbb{P}(= 1 \\mid L). refer true outcome model propensity score models, estimate data using algorithm, refer estimated models m̂(L)\\hat{m}_A(L) π̂(L)\\hat{\\pi}(L). Note real world, don’t get see Y(1)Y(1) Y(0)Y(0) – get see observed outcome, Y=AY(1)+(1−)Y(0)Y = Y(1) + (1-) Y(0). , ’ll drop values Y(1) Y(0) dataset don’t risk accidentally making use estimation procedure. Now ’re ready fit outcome propensity score models data. flexible possible, use nadir::super_learner() variety different learners. details fit available using verbose = TRUE option allow us see learners favored, performed:","code":"df <- df |> dplyr::select(-c(`Y(1)`, `Y(0)`)) outcome_model <- super_learner(   data = df,    formulas = list(     .default = Y_observed ~ L1_age + L2_income + A_treatment,     gam = Y_observed ~ s(L1_age, L2_income) + A_treatment),   learners = list(     mean = lnr_mean,     lm = lnr_lm,     earth = lnr_earth,     rf = lnr_rf,     gam = lnr_gam,     hal = lnr_hal     ),   verbose = TRUE) propensity_model <- super_learner(   data = df,    formulas = list(     .default = A_treatment ~ L1_age + L2_income,     gam = A_treatment ~ s(L1_age, L2_income)),   learners = list(     mean = lnr_mean,     logistic = lnr_logistic,     rf = lnr_rf_binary,     earth = lnr_earth,     xgboost = lnr_xgboost,     gam = lnr_gam,     hal = lnr_hal),   extra_learner_args = list(     earth = list(glm = list(family='binomial')),     xgboost = list(param = list(objective = 'binary:logistic')),     gam = list(family = binomial),     hal = list(family = 'binomial')   ),   outcome_type = 'binary',   verbose = TRUE ) # learner weights  round(outcome_model$learner_weights, 2) #>  mean    lm earth    rf   gam   hal  #>  0.00  0.00  0.11  0.05  0.66  0.18 round(propensity_model$learner_weights, 2) #>     mean logistic       rf    earth  xgboost      gam      hal  #>     0.00     0.00     0.22     0.00     0.00     0.58     0.21  # compare learners by their loss metric compare_learners(outcome_model) #> Inferring the loss metric for learner comparison based on the outcome type: #> outcome_type=continuous -> using mean squared error #> # A tibble: 1 × 6 #>    mean    lm earth    rf   gam   hal #>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1  8.01  3.67  1.20  1.83  1.15  1.44 compare_learners(propensity_model) #> Inferring the loss metric for learner comparison based on the outcome type: #> outcome_type=binary -> using negative log loss #> # A tibble: 1 × 7 #>    mean logistic    rf earth xgboost   gam   hal #>   <dbl>    <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> #> 1  194.     84.2  85.6  95.0    148.  75.3  81.4"},{"path":"https://ctesta01.github.io/nadir/articles/Doubly-Robust-Estimation.html","id":"doubly-robust-estimation-and-inference","dir":"Articles","previous_headings":"","what":"Doubly Robust Estimation and Inference","title":"Doubly Robust Estimation","text":"Now ’ve fit outcome model m̂(L)\\hat{m}_A(L) propensity score model π̂(L)\\hat{\\pi}(L), can proceed constructing doubly robust estimate average treatment effect. several different doubly robust estimators ATE, use one-step estimator. one-step estimator ATE following form:  θ̂=1n∑=1n{m̂1(Li)−m̂0(Li)+[1(Ai=1)π̂(Li)−1(=0)1−π̂(Li)](Yi−m̂Ai(Li))}  \\hat \\theta = \\frac{1}{n} \\sum_{=1}^n \\left\\{  \\hat m_1(L_i) - \\hat m_0(L_i) + \\left[ \\frac{1(A_i = 1)}{\\hat \\pi(L_i)} - \\frac{1(=0)}{1 - \\hat \\pi(L_i)} \\right] \\left( Y_i - \\hat m_{A_i}(L_i) \\right) \\right\\} θ̂\\hat{\\theta} asymptotically linear estimator, meaning admits following asymptotic representation linear sum plus term decaying 0 probability: n(θ̂−θ0)=1n∑=1nD(Oi)+oℙ(1). \\sqrt{n}(\\hat \\theta - \\theta_0) = \\frac{1}{\\sqrt{n}} \\sum_{=1}^n D(O_i) + o_{\\mathbb P}(1).  equation, θ0\\theta_0 true ATE, D(Oi)D(O_i) efficient influence function, D(Oi)={m1(Li)−m0(Li)+[1(Ai=1)π(Li)−1(=0)1−π(Li)](Yi−mAi(Li))}−θ0. D(O_i) = \\left\\{   m_1(L_i) -  m_0(L_i) + \\left[ \\frac{1(A_i = 1)}{\\pi(L_i)} - \\frac{1(=0)}{1 - \\pi(L_i)} \\right] \\left( Y_i - m_{A_i}(L_i) \\right) \\right\\} - \\theta_0. θ̂\\hat \\theta asymptotically linear, can use efficient influence function D(Oi)D(O_i) construct asymptotically valid confidence intervals. specifically, following asymptotic result allows us estimate valid asymptotic confidence intervals. n(θ̂−θ0)→d𝒩(0,Var(D)n). \\sqrt{n}(\\hat \\theta - \\theta_0) \\xrightarrow{d} \\mathcal N\\left(0, \\frac{\\text{Var}(D)}{n}\\right). useful references efficient influence function based inference Hines, O., Dukes, O., Diaz-Ordaz, K., & Vansteelandt, S. (2022). Demystifying Statistical Learning Based Efficient Influence Functions. American Statistician, 76(3), 292–304. https://doi.org/10.1080/00031305.2021.2021984 Kennedy, E. H. (2022). Semiparametric doubly robust targeted double machine learning: review (Version 2). arXiv. https://doi.org/10.48550/ARXIV.2203.06469 Fisher, ., & Kennedy, E. H. (2020). Visually Communicating Teaching Intuition Influence Functions. American Statistician, 75(2), 162–172. https://doi.org/10.1080/00031305.2020.1717620","code":"A <- df$A_treatment Y <- df$Y_observed   # get components of propensity scores pi_hat_of_L <- propensity_model$sl_predictor(df) one_minus_pi_hat_of_L <- 1-pi_hat_of_L  # get outcome predictions m_of_0_exposure_and_L <- # counterfactual model, everyone untreated   outcome_model$sl_predictor({ df |> mutate(A_treatment = 0) })  m_of_1_exposure_and_L <- # counterfactual model, everyone treated   outcome_model$sl_predictor({ df |> mutate(A_treatment = 1) })  m_of_A_and_L <- outcome_model$sl_predictor(df) # outcome model for treatment received  # these are the components of the uncentered EIF uncentered_EIF <- m_of_1_exposure_and_L - m_of_0_exposure_and_L +    (A/pi_hat_of_L - (1-A)/(1- pi_hat_of_L)) * (Y - m_of_A_and_L)  # we use the uncentered EIF to estimate theta_hat # this can be thought of as solving the estimating equation  #    uncentered_EIF - theta_0 = 0 theta_hat <- mean(uncentered_EIF)  # center the efficient influence function EIF <- uncentered_EIF - theta_hat  # in order to conduct inference, we can use the influence curve of the  # estimator and take its variance divided by n theta_hat_variance_estimate <- var(EIF)/nrow(df)  # calculate confidence intervals ATE_ConfidenceIntervals <- theta_hat + qnorm(c(0.025, 0.975)) * sqrt(theta_hat_variance_estimate)  # format the results formatted_CI <-   # sprintf(fmt = \"%.2f\", ...) formats a numeric to a string that has two digits   # past the decimal, even if there are trailing zeroes. round(0.4, 2) evaluates   # to \"0.4\", while sprintf(fmt = \"%.2f\", 0.4) evaluates to \"0.40\".   paste0(\"(\", paste0(sprintf(fmt = \"%.2f\", round(     ATE_ConfidenceIntervals, 2   )), collapse = ', '),   \")\")  # print the results cat(paste0(   \"ATE Estimate: \", sprintf(fmt = \"%.2f\", round(theta_hat, 2)), \"\\n\",   \"Confidence Intervals: \", formatted_CI, \"\\n\")) #> ATE Estimate: 4.48 #> Confidence Intervals: (4.33, 4.63)"},{"path":"https://ctesta01.github.io/nadir/articles/Doubly-Robust-Estimation.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Doubly Robust Estimation","text":"looks like estimate 4.48 quite close true value, 4.43, great! Moreover, truth easily lies within confidence interval produced (4.33, 4.63). truly remarkable approach, can read references listed throughout article, cross-fitting nuisance models (, via nadir::super_learner()) using doubly robust estimator, able avoid problems related curse dimensionality machine learning algorithms (.e., often called regularization bias overfitting bias) able produce consistent estimates causal parameters asymptotically valid confidence intervals relatively mild assumptions. want keep going nadir::super_learner() causal inference, natural subsequent directions use causal parameters besides ATE, like average treatment effect among treated, instrumental variables estimands, mediational quantities like direct/indirect effects, heterogeneous treatment effects, continuous treatment effects like modified treatment policy effects, .","code":""},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"how-should-we-think-about-misspecified-learners","dir":"Articles","previous_headings":"","what":"How should we think about “misspecified” learners?","title":"FAQs","text":"’s important clear fact super learner purely predicitive algorithm statistically useful properties. fit super learner explanatory model. Therefore, pure prediction standpoint, matter whether aspects models fit “incorrect” structural standpoint. make clear, example: example, “linear probability model” (.e., linear model fit binary outcome data) may perform quite well instances outperform logistic regression model. main downside linear probability model can make predictions outside [0,1] interval, user may want modify linear probability model learner produce predictions truncated [0,1] interval. However, certain kinds misspecification genuinely problematic. example, supervised learning situation, predictors included based outcomes available situation predictions needed outcomes yet observed, incorrect usage super learner algorithm.","code":""},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"i-want-to-use-super_learner-for-count-or-nonnegative-outcomes-","dir":"Articles","previous_headings":"","what":"I want to use super_learner() for count or nonnegative outcomes.","title":"FAQs","text":"principle, can use super_learner() whatever type outcomes want long things hold: learners pass super_learner() predict type outcome. loss function used inside determine_super_learner_weights() function argument consistent loss function used type data. loss function “” used depends context, using mean-squared-error loss continuous outcomes negative log loss binary outcomes conditional density models written far. Refer source R/determine_weights.R. using nadir::super_learner() applications context Targeted Learning, may useful understand better arguments ofUnified Cross-Validation Methodology Selection Among Estimators General Cross-Validated Adaptive Epsilon-Net Estimator: Finite Sample Oracle Inequalities Examples Mark van der Laan Sandrine Dudoit, 2003 first understand appropriate loss function chosen depending outcome type. understanding people used Poisson distribution motivated loss functions (https://discuss.pytorch.org/t/poisson-loss-function/44301/6, https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html) need think right thing general count outcome data.","code":""},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"what-are-the-limitations-of-nadirsuper_learner","dir":"Articles","previous_headings":"","what":"What are the limitations of nadir::super_learner()?","title":"FAQs","text":"key limitations design. say, want peek beta coefficients fit statistics learner, supported nadir::super_learner() design. reasoning explicit goal nadir keep learner objects lightweight building super_learner() can fast. explicit subpoint call attention , means far, work put supporting survival type outcomes. far, everything nadir assumes completeness (missingness) data. nadir::super_learner() pure prediction algorithm, provide confidence intervals. obtain confidence intervals, nadir::super_learner() needs embedded inferential paradigm influence function based estimation inference Targeted Learning, similar.","code":""},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"what-if-the-learner-that-i-want-to-write-really-isnt-formula-based","dir":"Articles","previous_headings":"","what":"What if the learner that I want to write really isn’t formula based?","title":"FAQs","text":"solution case --less ditch formula piece learner entirely, just treating unused argument, custom needs can always build learners encode details structure data. can see immediately prior code snippet, niche application like avoid using formulas argument nadir::super_learner() , can taking advantage know ’re going structure data argument.","code":"data <- matrix(data = rnorm(n = 200), nrow = 20) colnames(data) <- paste0(\"X\", 1:10) data <- cbind(data, data %*% rnorm(10)) colnames(data)[ncol(data)] <- 'Y'  lnr_nonformula1 <- function(data, formula, ...) {      # notice by way of knowing things about our data structure, we never reference   # the formula;  so if you truly don't want to use it, you don't have to.      # as an example, here we do OLS assuming inputs are numeric matrices —    # this might even be computationally more performant given how much extra   # stuff is inside an lm or glm fit.   X <- as.matrix(data[,grepl(pattern = \"^X\", colnames(data))])   Y <- as.matrix(data[,'Y'])   model_betas <- solve(t(X) %*% X) %*% t(X) %*% Y      learned_predictor <- function(newdata) {     if ('Y' %in% colnames(newdata)) {       index_of_y <- which(colnames(newdata) == 'Y')[[1]]     } else {       index_of_y <- NULL     }     if (is.data.frame(newdata)) {       newdata <- as.matrix(newdata)     }     as.vector(t(model_betas) %*% t(newdata[,-index_of_y, drop=FALSE]))   }   return(learned_predictor) } attr(lnr_nonformula1, 'sl_lnr_type') <- 'continuous'  # this is essentially a re-implementation of lnr_mean with no reference to the formula  lnr_nonformula2 <- function(data, formula, ...) {      Y <- data[,'Y']   Y_mean <- mean(Y)      learned_predictor <- function(newdata) {     rep(Y_mean, nrow(newdata))   }   return(learned_predictor) } attr(lnr_nonformula2, 'sl_lnr_type') <- 'continuous'  learned_super_learner <- super_learner(   data = data,   learners = list(     nonformula1 = lnr_nonformula1,     nonformula2 = lnr_nonformula2),   formulas = . ~ ., # it doesn't matter what we put here, because neither    # learner uses their formula inputs.    y_variable = 'Y',   verbose = TRUE   )  # observe that the OLS model gets all the weight because it's the correct model: round(learned_super_learner$learner_weights, 10)  #> nonformula1 nonformula2  #>           1           0   rm(data) # cleanup"},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"can-i-use-origami-with-nadir","dir":"Articles","previous_headings":"","what":"Can I use {origami} with {nadir}?","title":"FAQs","text":"Yes, can. ’s wrapper provided working folds_* functions origami. first example bit boring, internally use origami::folds_vfold. second example demonstrates pass another fold_* function origami package, extra arguments passed cv_origami_schema get passed origami::folds_* function.","code":"sl_model <- super_learner(   data = mtcars,   formula = mpg ~ cyl + hp,   learners = list(rf = lnr_rf, lm = lnr_lm, mean = lnr_mean),   cv_schema = cv_origami_schema,   verbose = TRUE )  # if you want to use a different origami::folds_* function, pass it into cv_origami_schema sl_model <- super_learner(   data = mtcars,   formula = mpg ~ cyl + hp,   learners = list(rf = lnr_rf, lm = lnr_lm, mean = lnr_mean),   cv_schema = \\(data, n_folds) {     cv_origami_schema(data, n_folds, fold_fun = origami::folds_loo)   },   verbose = TRUE )"},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"are-there-potentially-sharp-edges-to-nadir-worth-knowing-about","dir":"Articles","previous_headings":"","what":"Are there potentially ‘sharp edges’ to {nadir} worth knowing about?","title":"FAQs","text":"Yes! Though nadir tries make process user-friendly, may unexpected behaviors use outside design-scope tested functionality.","code":""},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"outcome-transformations","dir":"Articles","previous_headings":"Are there potentially ‘sharp edges’ to {nadir} worth knowing about?","what":"Outcome Transformations","title":"FAQs","text":"example, nadir learners far built handle regression formulas left-hand-side appears column name data passed . means transformations outcome variable implied formula supported. advice want transform outcome variable, store column data run super_learner() using column name formula(s). recommended way handle outcome transformations store transformed outcome new column data frame refer left-hand-side formula(e).","code":""},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"what-does-outcome_type-----do-and-what-doesnt-it-do","dir":"Articles","previous_headings":"Are there potentially ‘sharp edges’ to {nadir} worth knowing about?","what":"What does outcome_type = ... do and what doesn’t it do?","title":"FAQs","text":"Another sharp-edge around meta-learning step. example, predicting continuous outcomes, one specify super_learner() outcome_type = 'continuous' (default) non-negative least squares used minimize linear combination candidate learners based held-mean squared error loss function. Additionally, setting outcome_type = 'binary' outcome_type = 'density' negative log likelihood / negative log predicted density used respectively loss functions. cases, defaults translate setting determine_weights_for_super_learner function argument appropriately one determine_super_learner_weights_nnls(), determine_weights_for_binary_outcomes, determine_weights_using_neg_log_loss. loss functions selected based work loss based estimation literature, especially1 2 3. outcome_type argument doesn’t modify behavior candidate learners, want use say lnr_glm family = binomial(link = 'logit'), need pass extra_learner_args argument super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/articles/Guidance-for-Developers.html","id":"on-the-topic-of-pkgdown","dir":"Articles","previous_headings":"","what":"On the topic of {pkgdown}","title":"Guidance for Developers","text":"experience, started running issues compiling pkgdown site, (errors saying .Rd files parsed), needed update dependencies pkgdown. ran install.packages(\"pkgdown\", dependencies = TRUE) fixed issues running towards start pkgdown setup.. think debug rendering pkgdown website, also ended using advice : https://stackoverflow.com/questions/66806694/pkgdown-fails-parsing-rd-files--examples--added, namely make sure development version downlit installed. articles pkgdown website take time compile, find useful know can run sub-components pkgdown::build_site() individually. See: https://pkgdown.r-lib.org/reference/build_site.html order get pkgdown website render math properly documentation nadir::lnr_homoskedastic_density() follow advice https://github.com/r-lib/pkgdown/issues/2704 user @louisaslett said needed include _pkgdown.yml manually: now, images README.Rmd just embedded manually generated (Feb 27 2025) based lack support Rmd generated images getting copied site docs pkgdown discussed thread https://github.com/r-lib/pkgdown/issues/133 advice put images man/figures/ images can also rendered CRAN. tried following advice https://github.com/r-lib/pkgdown/issues/995 get articles better ordering, didn’t seem work . Note parallelization demo vignettes/articles/Running-super_learner--Parallel.Rmd uses multicore setting, necessary run pkgdown::build_article(\"articles/Running-super_learner--Parallel\") terminal RStudio.","code":"library(devtools) install_github('r-lib/downlit') template:   bootstrap: 5   includes:     in_header: |       <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css\" integrity=\"sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+\" crossorigin=\"anonymous\">       <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js\" integrity=\"sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg\" crossorigin=\"anonymous\"><\/script>       <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js\" integrity=\"sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk\" crossorigin=\"anonymous\" onload=\"renderMathInElement(document.body);\"><\/script> # library(nadir)"},{"path":"https://ctesta01.github.io/nadir/articles/Running-super_learner-in-Parallel.html","id":"speed-gains-are-most-obvious-in-cv_super_learner","dir":"Articles","previous_headings":"","what":"Speed gains are most obvious in cv_super_learner()","title":"Running `super_learner()` in Parallel","text":"Let’s run timing test see can tell ’s improvement performance using multicore vs. sequential plan:","code":"# sequential version:  plan(sequential)  microbenchmark({   cv_super_learner(     data,     formula = medv ~ .,     learners = list(rf = lnr_rf, lm = lnr_lm, mean = lnr_mean))   }, times = 3) #> Warning in microbenchmark({: less accurate nanosecond times to avoid potential #> integer overflows #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> Unit: seconds #>                                                                                                                     expr #>  {     cv_super_learner(data, formula = medv ~ ., learners = list(rf = lnr_rf,          lm = lnr_lm, mean = lnr_mean)) } #>       min      lq     mean   median       uq      max neval #>  7.574719 7.63037 7.816375 7.686021 7.937203 8.188385     3 # multicore version:  plan(multicore, workers = 10)  microbenchmark({   cv_super_learner(     data,      formula = medv ~ .,     learners = list(rf = lnr_rf, lm = lnr_lm, mean = lnr_mean)) }, times = 3) #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> Unit: seconds #>                                                                                                                     expr #>  {     cv_super_learner(data, formula = medv ~ ., learners = list(rf = lnr_rf,          lm = lnr_lm, mean = lnr_mean)) } #>       min      lq     mean   median       uq      max neval #>  2.150917 2.16683 2.238391 2.182743 2.282128 2.381514     3 learners <- list(   mean = lnr_mean,   lm = lnr_lm,   rf = lnr_rf,   earth = lnr_earth,   xgboost = lnr_xgboost,   glmnet0 = lnr_glmnet,   glmnet1 = lnr_glmnet,   glmnet2 = lnr_glmnet,   glmnet3 = lnr_glmnet )  extra_args <- list(   glmnet0 = list(lambda = 0.01),   glmnet1 = list(lambda = 0.2),   glmnet2 = list(lambda = 0.4),   glmnet3 = list(lambda = 0.6) ) plan(sequential)  microbenchmark({    cv_out <- cv_super_learner(     data = mtcars,      formulas = mpg ~ .,     learners = learners,     extra_learner_args = extra_args) }, times = 3) #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> Unit: seconds #>                                                                                                                                  expr #>  {     cv_out <- cv_super_learner(data = mtcars, formulas = mpg ~          ., learners = learners, extra_learner_args = extra_args) } #>       min       lq     mean   median       uq      max neval #>  2.235282 2.252174 2.267698 2.269065 2.283905 2.298745     3 plan(multicore)  microbenchmark({    cv_out <- cv_super_learner(     data = mtcars,      formulas = mpg ~ .,     learners = learners,     extra_learner_args = extra_args) }, times = 3) #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> The loss_metric is being inferred based on the outcome_type=continuous -> using CV-MSE #> Unit: milliseconds #>                                                                                                                                  expr #>  {     cv_out <- cv_super_learner(data = mtcars, formulas = mpg ~          ., learners = learners, extra_learner_args = extra_args) } #>       min       lq     mean   median       uq      max neval #>  862.5016 891.1052 906.6127 919.7088 928.6682 937.6277     3"},{"path":"https://ctesta01.github.io/nadir/articles/Running-super_learner-in-Parallel.html","id":"but-why-is-it-not-so-obvious-for-just-super_learner","dir":"Articles","previous_headings":"","what":"But why is it not so obvious for just super_learner()?","title":"Running `super_learner()` in Parallel","text":"cv_super_learner() involves additional layer cross-validation, effect parallelization obvious cv_super_learner() compared super_learner(). However, make obvious parallelization working super_learner() well, number cv folds want run higher, increases relative payoff using parallel option.","code":"plan(sequential)  microbenchmark({    sl_out <- nadir::super_learner(     data = Boston,     formulas = medv ~ .,     learners = learners,     n_folds = 20,     extra_learner_args = extra_args,     verbose = TRUE) }, times = 3) #> Unit: seconds #>                                                                                                                                                                              expr #>  {     sl_out <- nadir::super_learner(data = Boston, formulas = medv ~          ., learners = learners, n_folds = 20, extra_learner_args = extra_args,          verbose = TRUE) } #>       min       lq     mean   median       uq      max neval #>  13.20251 13.25625 13.52498 13.30998 13.68621 14.06243     3 plan(multicore)  microbenchmark({    sl_out <- nadir::super_learner(     data = Boston,     formulas = medv ~ .,     learners = learners,     n_folds = 20,     extra_learner_args = extra_args,     verbose = TRUE) }, times = 3) #> Unit: seconds #>                                                                                                                                                                              expr #>  {     sl_out <- nadir::super_learner(data = Boston, formulas = medv ~          ., learners = learners, n_folds = 20, extra_learner_args = extra_args,          verbose = TRUE) } #>       min       lq     mean   median       uq      max neval #>  9.268101 9.327153 9.407291 9.386205 9.476886 9.567568     3"},{"path":"https://ctesta01.github.io/nadir/articles/Screeners.html","id":"screening-variables-out-of-the-regression-problem","dir":"Articles","previous_headings":"","what":"Screening Variables out of the Regression Problem","title":"Screeners","text":"screeners currently available nadir : screener_cor — thresholds based correlation coefficient. screener_cor_top_n — keeps top n predictors correlated outcome. screener_t_test — uses t statistic p.value linear model outcome, intercept, one predictor time. order use set regression problem, can follow following template: noted functionality used fairly extreme discretion. point offering settings huge number variables may computationally challenging, likely screening variables may induce issues around post-selection inference.","code":"library(nadir) ## Registered S3 method overwritten by 'future': ##   method               from       ##   all.equal.connection parallelly # step 1: # specify the original regression problem, before any screening has been done #  # data:    will be the mtcars dataset  # formula: will be mpg ~ .  #   as in, mpg regressed on every other column. #   # step 2:  # use a screener to modify the problem, dropping some predictors  screened_regression_problem <- screener_cor(   data = mtcars,    formula = mpg ~ .,   threshold = 0.5) # we will require predictors have correlation coefficient of at least 0.5 to keep them #  # if you want to look, you can see what was kept: screened_regression_problem$formula ## mpg ~ cyl + disp + hp + drat + wt + vs + am + carb ## <environment: 0x1196722b8> screened_regression_problem$failed_to_correlate_names ## [1] \"qsec\" \"gear\" # step 3:  # now we can use nadir::super_learner() with the modified problem super_learner(   data = screened_regression_problem$data,   formula = screened_regression_problem$formula,   learners = list(lnr_lm, lnr_earth, lnr_rf),   verbose = TRUE) ## $sl_predictor ## function(newdata) { ##     # for each model, predict on the newdata and apply the model weights ##     parallel_lapply(1:length(fit_learners), function(i) { ##       fit_learners[[i]](newdata) * learner_weights[[i]] ##     }) |> ##       Reduce(`+`, x = _) # aggregate across the weighted model predictions ##   } ## <bytecode: 0x119f06228> ## <environment: 0x119f3ebc0> ##  ## $y_variable ## [1] \"mpg\" ##  ## $outcome_type ## [1] \"continuous\" ##  ## $learner_weights ##         lm      earth         rf  ## 0.07154578 0.00000000 0.92845422  ##  ## $holdout_predictions ## # A tibble: 32 × 5 ##    .sl_fold    lm earth    rf   mpg ##       <int> <dbl> <dbl> <dbl> <dbl> ##  1        1 22.6   17.0  19.7  21.4 ##  2        1 18.0   15.0  16.4  18.7 ##  3        1 21.5   17.6  19.0  18.1 ##  4        1 12.7   12.9  13.4  10.4 ##  5        1 17.4   21.6  20.0  19.7 ##  6        1  9.47  16.2  15.3  15   ##  7        2 21.9   18.6  20.5  21   ##  8        2 21.1   20.4  23.2  24.4 ##  9        2 12.3   15.1  17.1  16.4 ## 10        2 13.8   16.2  17.1  15.2 ## # ℹ 22 more rows ##  ## attr(,\"class\") ## [1] \"list\"                    \"nadir_sl_verbose_output\" # use super_learner() as you would, just with the updated formula and data"},{"path":"https://ctesta01.github.io/nadir/articles/Screeners.html","id":"adding-a-screening-layer-into-a-learner","dir":"Articles","previous_headings":"","what":"Adding a Screening Layer into a Learner","title":"Screeners","text":"contrast approach, another approach screening variables screen within given learner. involves constructing learners screening “baked ” speak. construct new learners, one use add_learner(learner, screener, screener_extra_args) function, returns new learner. approach shows construct screeners manually, may settings constructing screened learners programmatically beneficial. show produce array learners built-screeners programmatically:","code":"# construct some new learners with varying levels and types of screening   # here we just show a small sample of examples manually constructed: lnr_glm_screened_pearson_cor_50 <- add_screener(lnr_glm, screener_cor, list(threshold = 0.5)) lnr_glm_screened_spearman_cor_50 <- add_screener(lnr_glm, screener_cor, list(threshold = 0.5, cor... = list(method = 'spearman'))) lnr_rf_screened_cor_top_5 <- add_screener(lnr_rf, screener_cor_top_n, list(keep_n_terms = 5)) lnr_earth_screened_t_test_p_lt_05 <- add_screener(lnr_earth, screener_t_test, list(p_value_threshold = 0.05))  # use the learners with built-in screeners in a super_learner(): super_learner(   data = MASS::Boston,    formula = medv ~ .,   learners = list(     lnr_glm_screened_pearson_cor_50, lnr_glm_screened_spearman_cor_50,     lnr_rf_screened_cor_top_5, lnr_earth_screened_t_test_p_lt_05),   verbose = TRUE) ## $sl_predictor ## function(newdata) { ##     # for each model, predict on the newdata and apply the model weights ##     parallel_lapply(1:length(fit_learners), function(i) { ##       fit_learners[[i]](newdata) * learner_weights[[i]] ##     }) |> ##       Reduce(`+`, x = _) # aggregate across the weighted model predictions ##   } ## <bytecode: 0x119f06228> ## <environment: 0x1193b2e88> ##  ## $y_variable ## [1] \"medv\" ##  ## $outcome_type ## [1] \"continuous\" ##  ## $learner_weights ## cor_threshold_screened_glm_1 cor_threshold_screened_glm_2  ##                    0.0000000                    0.0000000  ##        cor_top_n_screened_rf        t_test_screened_earth  ##                    0.4010399                    0.5989601  ##  ## $holdout_predictions ## # A tibble: 506 × 6 ##    .sl_fold cor_threshold_screene…¹ cor_threshold_screen…² cor_top_n_screened_rf ##       <int>                   <dbl>                  <dbl>                 <dbl> ##  1        1                    25.8                   26.6                  24.4 ##  2        1                    27.0                   27.2                  28.8 ##  3        1                    22.0                   21.4                  19.6 ##  4        1                    21.1                   20.7                  18.9 ##  5        1                    16.9                   15.8                  17.4 ##  6        1                    16.3                   16.7                  16.8 ##  7        1                    14.4                   15.1                  15.6 ##  8        1                    20.5                   20.5                  20.6 ##  9        1                    28.6                   27.8                  30.2 ## 10        1                    25.1                   25.7                  23.1 ## # ℹ 496 more rows ## # ℹ abbreviated names: ¹​cor_threshold_screened_glm_1, ## #   ²​cor_threshold_screened_glm_2 ## # ℹ 2 more variables: t_test_screened_earth <dbl>, medv <dbl> ##  ## attr(,\"class\") ## [1] \"list\"                    \"nadir_sl_verbose_output\" # construct new learners with builtin screeners # =============================================  # learners base_learners <- list(lnr_glm, lnr_hal, lnr_earth, lnr_rf, lnr_glmnet)  # screeners screeners <- list(screener_cor, screener_cor, screener_cor, screener_cor_top_n) screener_extra_args <- list(list(threshold = 0.3),                              list(threshold = 0.4),                              list(threshold = 0.5),                              list(keep_n_terms = 10)) # ensure that the screeners and screener_extra_args are the same length  # set up a grid of combinations of learners and screeners # we'll refer to them by indices to avoid duplicating objects unnecessarily  learner_screener_grid <- expand.grid(learner = 1:length(base_learners), screener = 1:length(screeners))  new_learners <- lapply(1:nrow(learner_screener_grid), \\(i) {   learner_i <- learner_screener_grid[['learner']][i]   screener_i <- learner_screener_grid[['screener']][i]      new_learner <- add_screener(learner = base_learners[[learner_i]],                               screener = screeners[[screener_i]],                               screener_extra_args = screener_extra_args[[screener_i]])   new_learner })   # run super_learner() with the new screeners # ==========================================  nadir::super_learner(   data = MASS::Boston,    formula = medv ~ .,   learners = new_learners,   verbose = TRUE) ## $sl_predictor ## function(newdata) { ##     # for each model, predict on the newdata and apply the model weights ##     parallel_lapply(1:length(fit_learners), function(i) { ##       fit_learners[[i]](newdata) * learner_weights[[i]] ##     }) |> ##       Reduce(`+`, x = _) # aggregate across the weighted model predictions ##   } ## <bytecode: 0x119f06228> ## <environment: 0x11a7c35a8> ##  ## $y_variable ## [1] \"medv\" ##  ## $outcome_type ## [1] \"continuous\" ##  ## $learner_weights ##    cor_threshold_screened_glm_1    cor_threshold_screened_hal_1  ##                      0.00000000                      0.12424255  ##  cor_threshold_screened_earth_1     cor_threshold_screened_rf_1  ##                      0.02788718                      0.08182205  ## cor_threshold_screened_glmnet_1    cor_threshold_screened_glm_2  ##                      0.00000000                      0.00000000  ##    cor_threshold_screened_hal_2  cor_threshold_screened_earth_2  ##                      0.00000000                      0.00000000  ##     cor_threshold_screened_rf_2 cor_threshold_screened_glmnet_2  ##                      0.00000000                      0.00000000  ##    cor_threshold_screened_glm_3    cor_threshold_screened_hal_3  ##                      0.00000000                      0.00000000  ##  cor_threshold_screened_earth_3     cor_threshold_screened_rf_3  ##                      0.00000000                      0.00000000  ## cor_threshold_screened_glmnet_3          cor_top_n_screened_glm  ##                      0.00000000                      0.00000000  ##          cor_top_n_screened_hal        cor_top_n_screened_earth  ##                      0.00000000                      0.00000000  ##           cor_top_n_screened_rf       cor_top_n_screened_glmnet  ##                      0.76604822                      0.00000000  ##  ## $holdout_predictions ## # A tibble: 506 × 22 ##    .sl_fold cor_threshold_screen…¹ cor_threshold_screen…² cor_threshold_screen…³ ##       <int>                  <dbl>                  <dbl>                  <dbl> ##  1        1                  31.1                    28.2                  37.2  ##  2        1                  27.6                    27.0                  27.0  ##  3        1                  22.9                    20.7                  20.4  ##  4        1                  12.2                    11.7                  15.0  ##  5        1                  11.6                    12.7                  14.1  ##  6        1                  19.3                    18.1                  21.5  ##  7        1                   7.66                   16.3                   9.64 ##  8        1                  13.4                    17.6                  13.3  ##  9        1                  20.7                    21.8                  22.0  ## 10        1                  20.0                    22.7                  21.1  ## # ℹ 496 more rows ## # ℹ abbreviated names: ¹​cor_threshold_screened_glm_1, ## #   ²​cor_threshold_screened_hal_1, ³​cor_threshold_screened_earth_1 ## # ℹ 18 more variables: cor_threshold_screened_rf_1 <dbl>, ## #   cor_threshold_screened_glmnet_1 <dbl>, cor_threshold_screened_glm_2 <dbl>, ## #   cor_threshold_screened_hal_2 <dbl>, cor_threshold_screened_earth_2 <dbl>, ## #   cor_threshold_screened_rf_2 <dbl>, cor_threshold_screened_glmnet_2 <dbl>, … ##  ## attr(,\"class\") ## [1] \"list\"                    \"nadir_sl_verbose_output\""},{"path":"https://ctesta01.github.io/nadir/articles/Survival.html","id":"review-of-pooled-logistic-regression","dir":"Articles","previous_headings":"","what":"Review of Pooled Logistic Regression","title":"Survival Analysis via Pooled Logistic Regression","text":"Pooled logistic regression statistical method performing logistic regression yields parameter estimates asymptotically equivalent Cox proportional hazards regression. procedure works repeating observations conventional survival (time--event) dataset many observation periods row observed either event censoring time. , logistic regression can treat rows associated given observation period (e.g., often day) active risk set observations, logistic regression indicator event yields approximation instantaneous hazard. example (Introduction section Craig et al. 2025), pooled logistic regression approach works converting following original survival dataset: X=(covariate 1covariate 2x11x12x21x22x31x32),y=(survival timeeventt11t20t31). X =  \\begin{pmatrix} \\text{covariate 1} & \\text{covariate 2} \\\\ x_{11} & x_{12} \\\\ x_{21} & x_{22} \\\\ x_{31} & x_{32} \\end{pmatrix}, \\quad y =  \\begin{pmatrix} \\text{survival time} & \\text{event} \\\\  t_1 & 1 \\\\ t_2 & 0 \\\\ t_3 & 1 \\end{pmatrix}.  following: X̃=(covariate 1covariate 2risk set 1risk set 2x11x1210x21x2210x31x3210x31x3201),ỹ=(event1001). \\tilde{X} =  \\begin{pmatrix} \\text{covariate 1} & \\text{covariate 2} & \\text{risk set 1} & \\text{risk set 2} \\\\ x_{11} & x_{12} & 1 & 0 \\\\ x_{21} & x_{22} & 1 & 0 \\\\ x_{31} & x_{32} & 1 & 0 \\\\ x_{31} & x_{32} & 0 & 1 \\end{pmatrix}, \\quad \\tilde{y} =  \\begin{pmatrix} \\text{event} \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}. One key utilities pooled logistic regression approach allows straightforward incorporation time-varying-covariates /interactions time. References method: Craig, Zhong, Tibshirani. review survival stacking: method cast survival regression analysis classification problem. Int. J. Biostatistics, 2025 (also arXiv ) Polley van der Laan. Super Learning Right-Censored Data, Chapter 16 Targeted Learning: Causal Inference Observational Experimental Data, 2011 Zivich, Cole, Shook-Sa, DeMonte, Edwards. Estimating equations survival analysis pooled logistic regression. arXiv, 2025 D’Agostino, Lee, Belanger, Cupples, Anderson, Kannel. Relation pooled logistic regression time dependent cox regression analysis: framingham heart study. Stat Med 1990 Cupples, D’Agostino, Anderson, Kannel. Comparison baseline repeated measure covariate techniques Framingham heart study. Stat Med 1988","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/articles/Survival.html","id":"replicating-the-table-1-from-craig-et-al--2025","dir":"Articles","previous_headings":"Review of Pooled Logistic Regression > Examples","what":"Replicating the Table 1 from Craig et al. 2025","title":"Survival Analysis via Pooled Logistic Regression","text":"begin simple example, illustrate (asymptotic) equivalence pooled logistic regression Cox proportional hazards regression single example. example, use observations Rotterdam tumor recurrence dataset. goal example able compare Cox proportional hazard model coefficients model coefficients pooled logistic regression see close . Pooled Logistic (log ORs) Cox Proportional Hazard Coefficient Estimates (log HRs) Side--Side example shown just illustrative purposes. approximation error example can decreased example coarsening time-windows less (e.g., carrying pooled logistic regression daily risk sets), takes compute-time .","code":"library(nadir) library(dplyr) library(ggplot2) library(survival) library(survivalVignettes) # contains the rotterdam data library(tidyr)  # extract the original data df <- rotterdam |> dplyr::filter(nodes > 0) dim(df) ## [1] 1546   15 # 1546 matches the Craig et al paper population  df$dtime <- df$dtime / 365 # put time in years for computing convenience  repeated_measures_df <- df %>%   # group by all the “id‐level” fields   nest_by(pid, dtime, hormon, age, meno, grade, nodes, pgr, er, death) %>%   # for each subject, make one row per integer t in [start, stop]   reframe(     removal_indicator = death == 0 & floor(dtime)-1 < 0,     t   = seq(0, ifelse(death == 1, floor(dtime), floor(dtime)-1)),     hormon, # hormone treatment     age, # age     meno, # menopause      grade, # grade     nodes, # positive nodes     pgr, # progesterone      er, # estrogen     event = death, # 1 is a true failure      event = as.integer(event == 1 & t == floor(dtime))   ) |>   filter(! removal_indicator) |>    select(-removal_indicator) |>   ungroup()  pooled_logistic_model <- glm(   data = repeated_measures_df,   formula = event ~ hormon + factor(t) + hormon + age + meno + grade + nodes + pgr + er,   family = binomial(link = 'logit'))  tidy_coefficients <- broom::tidy(pooled_logistic_model)  tidy_pooled_logistic_coefs <- tidy_coefficients |> dplyr::filter(   term %in% c('hormon', 'age', 'meno', 'grade', 'nodes', 'pgr', 'er')) |>    dplyr::select(term, estimate, p.value)  # compare to the equivalent Coxph model: tidy_coxph_coefs <- coxph(Surv(dtime, death) ~ hormon + age + meno + grade + nodes + pgr + er, data = df) |>    broom::tidy() |>   dplyr::select(term, estimate, p.value)  tidy_coefs <- dplyr::left_join(tidy_pooled_logistic_coefs, tidy_coxph_coefs, by = c('term' = 'term'), suffix = c('_logistic', '_coxph'))  tidy_coefs |>    select(term, contains('estimate'), contains('p.value')) |>    knitr::kable(caption = 'Pooled Logistic (log ORs) and Cox Proportional Hazard Coefficient Estimates (log HRs) Side-by-Side')"},{"path":"https://ctesta01.github.io/nadir/articles/Survival.html","id":"comparing-and-contrasting-different-approaches-to-time","dir":"Articles","previous_headings":"Review of Pooled Logistic Regression > Examples","what":"Comparing and Contrasting Different Approaches to Time","title":"Survival Analysis via Pooled Logistic Regression","text":"least three ways time can handled: Craig et al. paper describes Introduction, providing basis discrete risk sets. another continuous predictor variable scalar coefficient . another continuous predictor variable smoothing spline . See Section 3.1 Functional Form Time Zivich et al 2025 details. time modeled set distinct indicators time-period, imposes additional constraints baseline hazard similar approach Cox proportional hazards. time treated numeric column estimated scalar model coefficient, akin restriction exponential model. Finally, using spline somewhat -. illustrate point, visualize following three separate model fits:    Since highly adaptive lasso (HAL) (see https://pmc.ncbi.nlm.nih.gov/articles/PMC5662030/, https://github.com/tlverse/hal9001) trending lately, also show examples using HAL. Additionally, finally show utility super_learner() choose/weight across multiple candidate learners.","code":"# construct a person-period level dataset from the survival::bladder data repeated_measures_df <- df_to_survival_stacked(     data = survival::bladder,     time_col = 'stop',     status_col = 'event',     covariate_cols = c('rx', 'size', 'number', 'size', 'enum'),     id_col = 'id')  plot_super_learned_pooled_logistic_regression <- function(     data,     formulas,     learners,     extra_learner_args = NULL,     title ) {      sl_fit_for_km <- super_learner(     data         = data,     formulas     = formulas,     learners     = learners,     y_variable = 'event', # TODO: FIX this so y_variable doesn't need to be explicitly passed     outcome_type = \"binary\",     extra_learner_args = extra_learner_args,     n_folds      = 5,     cv_schema = \\(data, n_folds) { cv_origami_schema(data = data, n_folds = n_folds, cluster_ids = data$id) },     verbose_output = FALSE   )      # grid for kaplan meier curves   grid1_for_km <- repeated_measures_df %>% distinct(t) %>% mutate(rx = 1)   grid2_for_km <- grid1_for_km %>% mutate(rx = 2)      # calculate hazards   h1_for_km <- sl_fit_for_km(grid1_for_km)   # P(event at t | A=0)   h2_for_km <- sl_fit_for_km(grid2_for_km)      # compute survival curves S(t)=Product_{u<=t} [1−h(u)]   surv_df_for_km <- tibble(     t   = grid1_for_km$t,     h1  = h1_for_km,     h2  = h2_for_km   ) %>%     arrange(t) %>%     mutate(       S1 = cumprod(1 - h1),       S2 = cumprod(1 - h2),       HR = h2 / h1     )      # plot    plt <- ggplot(surv_df_for_km, aes(x = t)) +     geom_step(aes(y = S1, color = \"rx=1\"), direction = \"hv\") +     geom_step(aes(y = S2, color = \"rx=2\"), direction = \"hv\") +     labs(       y = \"Estimated survival probability\",       color = \"Treatment arm\",       title = title     ) +     xlim(c(0, max(bladder$stop))) +     ylim(c(0, 1)) +      theme_minimal()      return(plt) }  # discrete time hazard model:  # most similar to Cox proportional hazards: plot_super_learned_pooled_logistic_regression(   data = repeated_measures_df,   formulas = list(.default = event ~ rx + factor(t)),   learners     = list(lnr_logistic),   title = \"Discrete-time survival curves from pooled-logistic SL\" ) ## Warning in stats::optim(par = weights_before_softmax, fn = loss_fn, method = \"Nelder-Mead\"): one-dimensional optimization by Nelder-Mead is unreliable: ## use \"Brent\" or optimize() directly # continuous time hazard with a scalar coefficient:  # replicates an Exponential continuous-time hazard model: plot_super_learned_pooled_logistic_regression(   data = repeated_measures_df,   formulas = list(.default = event ~ rx + t),   learners = list(lnr_logistic),   title = \"Survival curves from pooled-logistic SL using GLM with time as a numeric column and a scalar coefficient\" ) ## Warning in stats::optim(par = weights_before_softmax, fn = loss_fn, method = \"Nelder-Mead\"): one-dimensional optimization by Nelder-Mead is unreliable: ## use \"Brent\" or optimize() directly # continuous time hazard with a smoothing spline:  plot_super_learned_pooled_logistic_regression(   data = repeated_measures_df,   formulas     = list(.default = event ~ rx + s(t)),   learners     = list(lnr_gam),   title = \"Survival curves from pooled-logistic SL using GAM with s(t)\" ) ## Warning in stats::optim(par = weights_before_softmax, fn = loss_fn, method = \"Nelder-Mead\"): one-dimensional optimization by Nelder-Mead is unreliable: ## use \"Brent\" or optimize() directly truncate_lnr <- function(lnr, min, max) {   truncate <- function(x, min, max) {     pmax(pmin(x, max), min)   }      # needs to return a learner, so it returns a function that    # takes in its inputs and returns a prediction function   return(     function(...) {       predictor_fn <- lnr(...)        truncated_predictor_fn <- function(...) {         truncate(predictor_fn(...), min, max)       }       return(truncated_predictor_fn)     }   ) }  # create a truncated version of the HAL learner #  # we this here because HAL with family = 'binomial' runs much, much slower than # HAL with a continuous outcome. #  # since, in principle, super_learner() will pick/weight the best performing # models, agnostic of whether or not their functional form is 'correct', it # should be fine to use a truncated continuous-outcome HAL for demonstrative # purposes here. #  # for a real application, we would recommend using `family = 'binomial'` as an  # extra learner argument for HAL. lnr_truncated_hal <- truncate_lnr(lnr_hal, 0, 1)   # pooling across several learners  plot_super_learned_pooled_logistic_regression(   data = repeated_measures_df,   formulas     = list(.default = event ~ rx + t,                       unrestricted_baseline_hazard = event ~ rx + factor(t),                       gam = event ~ rx + s(t)),   learners     = list(unrestricted_baseline_hazard = lnr_logistic,                        # exponential = lnr_logistic,                       gam = lnr_gam,                       hal1 = lnr_truncated_hal,                       hal2 = lnr_truncated_hal,                       hal3 = lnr_truncated_hal,                       hal4 = lnr_truncated_hal,                       hal5 = lnr_truncated_hal),   extra_learner_args = list(     hal1 = list(),     hal2 = list(num_knots = 2),     hal3 = list(num_knots = 3),     hal4 = list(num_knots = 10, smoothness_order = 1),     hal5 = list(num_knots = 10, smoothness_order = 2)),   title = \"Survival curves from a super learned model\" ) ## Warning in validate_learner_types(learners, outcome_type): Learners 3, 4, 5, 6, 7 with names [hal1, hal2, hal3, hal4, hal5] do not have attr(., 'sl_lnr_type') == 'binary'. ## See the Creating Learners article on the {nadir} website. ##"},{"path":"https://ctesta01.github.io/nadir/articles/Survival.html","id":"simulation-example-showing-convergence-to-the-cox-proportional-hazard-coefficients","dir":"Articles","previous_headings":"Review of Pooled Logistic Regression > Examples","what":"Simulation example showing convergence to the Cox proportional hazard coefficients","title":"Survival Analysis via Pooled Logistic Regression","text":"shows can recover Cox proportional hazard model coefficients time-windows considered get smaller smaller. Caution: code takes 30-40 minutes run. scenario run 30 iterations. Across columns sample size varied, across rows (descending) time-windows used constructing survival stacked data structure made finer. Interpretation: lowest sample size, coarsest time-window simulations (upper left-hand panel), can see Cox proportional hazards model pooled logistic regression survival stacked data wide variance. Scanning across sample sizes simulations coarsest time-window used (top row), can see time-windows coarse, survival stacking approach may consistently biased compared Cox proportional hazards model. look first column, simulations sample size low (50) time-windows used increasingly fine-grained, can see increasing fineness time-windows remove uncertainty estimation, time-windows used decrease, pooled logistic regression approach Cox proportional hazards model begin produce nearly exactly estimates. Finally, higher sample size, fine time-window regime (bottom right), can see Cox proportional hazards model pooled logistic approach survival stacked data estimate true parameters relatively little uncertainty.","code":"library(future.apply) plan(multicore) # note that `multicore` requires running this in a terminal (not RStudio)  # on a Unix/Linux machine (not Windows) -- see the future.apply documentation for reference.  #  # The advantage of `multicore` is that forking processes is quite fast, though this is  # not supported in Windows or an RStudio session.  generate_survival_data <- function(     n, # sample size      beta = c(0.5, -0.5, 0.3, 0, 0.2), # true log odds ratio coefficients     baseline_rate = 1,     censor_rate   = 0.1 # parameter for the Exponential time-to-censoring distribution ) {   if (length(beta) != 5) stop(\"`beta` must have length 5.\")      # simulate covariates   X <- matrix(rnorm(n * 5), nrow = n, ncol = 5)   colnames(X) <- paste0(\"X\", 1:5)      # linear predictor and event times under Exp(baseline_hazard * exp(eta))   eta <- X %*% beta   U <- runif(n)   T_true <- -log(U) / (baseline_rate * exp(eta))      # independent censoring times ~ Exp(censor_rate)   C <- rexp(n, rate = censor_rate)      # observed times & event indicator   time   <- pmin(T_true, C)   status <- as.integer(T_true <= C)      # return data.frame   data.frame(time, status, X) }   run_coxph_vs_survstack <- function(n = 700, period_duration = 1) {      dat <- generate_survival_data(     n             = n,     beta          = c(0.8, -0.2, 0.4, 0, 1.2),     baseline_rate = 0.25,     censor_rate   = 0.05   )      # fit a cox proportional hazards model   coxph_model <-     coxph(Surv(time, status) ~ X1 + X2 + X3 + X4 + X5, data = dat)      # extract the log odds ratios   coxph_model_coefs <- broom::tidy(coxph_model) |>      select(term, estimate)         # convert the original data into a repeated measures data structure   repeated_measures_df <-     nadir::df_to_survival_stacked(       data = dat,       time_col = 'time',       status_col = 'status',       covariate_cols = paste0('X', 1:5),       period_duration = period_duration     )      # perform the pooled logistic regression   pooled_logistic_model <- glm(     data = repeated_measures_df,     formula = event ~ factor(t) + X1 + X2 + X3 + X4 + X5,     family = binomial(link = 'logit'))      # extract the coefficients   tidy_coefficients <- broom::tidy(pooled_logistic_model)      tidy_coefs_on_tidy_coefficients <- tidy_coefficients |> dplyr::filter(     term %in% c(paste0(\"X\", 1:5))) |>      dplyr::select(term, estimate)      tidy_coefs_on_tidy_coefficients |> arrange(term)      list(     coxph = coxph_model_coefs,     survstack = tidy_coefs_on_tidy_coefficients   ) }  n_sims <- 15  sample_size <- c(50, 200, 500) period_duration <- c(2, .25, 0.1) sim_grid <- expand.grid(sample_size = sample_size, period_duration = period_duration)  sim_grid$results <- lapply(1:nrow(sim_grid), \\(x) list())  sim_grid$results <- future.apply::future_lapply(1:nrow(sim_grid), function(i) {   cat(\".\")   results <-     future_lapply(1:n_sims,            \\(x) {              run_coxph_vs_survstack(n = sim_grid[[i, 'sample_size']], period_duration = sim_grid[[i, 'period_duration']])            })      results_coxph <- future_lapply(1:n_sims, \\(i) { results[[i]][['coxph']] |> mutate(sim = i) }) |> bind_rows()   results_survstack <- future_lapply(1:n_sims, \\(i) { results[[i]][['survstack']] |> mutate(sim = i) }) |> bind_rows()   results_coxph <- results_coxph |> mutate(type = 'coxph')   results_survstack <- results_survstack |> mutate(type = 'survstack')   results_df <- bind_rows(results_coxph, results_survstack)      results_df })  betas <- c(0.8, -0.2, 0.4, 0, 1.2) betas_df <- data.frame(   term = paste0('X', 1:5),   true_coef = betas)  sim_results <- unnest(sim_grid, cols = 'results')  sim_results <- left_join(sim_results, betas_df, by = c('term' = 'term'))  ggplot(sim_results,         aes(          y = term,          x = estimate,          color = type,          fill = type)) +    ggridges::geom_density_ridges(alpha = 0.5) +    geom_segment(aes(x = true_coef, xend = true_coef,                     yend = paste0(\"X\", as.integer(stringr::str_extract(term, \"[0-9]\"))+1)),                color = 'black') +    facet_grid(factor(period_duration, levels = rev(sort(unique(sim_results$period_duration)))) ~ sample_size, scales = 'free',             labeller = labeller(               .cols = \\(x) paste0('sample size: ', x),               .rows = \\(x) paste0('time window duration: ', x)               )) +    theme_bw() +    scale_y_discrete(breaks = paste0('X', 1:5)) +    scale_fill_manual(values = c('#5dc6cc', '#FFC377')) +    scale_color_manual(values = c('#3d9595', '#f1b35c')) +    labs(     title = \"Simulation results of Cox Proportional Hazard vs. Pooled Logistic Regression\",     subtitle = \"A variety of sample sizes and time-coarsening window durations are shown; true coefficients in black\")"},{"path":"https://ctesta01.github.io/nadir/articles/guidance_for_developers.html","id":"on-the-topic-of-pkgdown","dir":"Articles","previous_headings":"","what":"On the topic of {pkgdown}","title":"Guidance for Developers","text":"experience, started running issues compiling pkgdown site, (errors saying .Rd files parsed), needed update dependencies pkgdown. ran install.packages(\"pkgdown\", dependencies = TRUE) fixed issues running towards start pkgdown setup.. think debug rendering pkgdown website, also ended using advice : https://stackoverflow.com/questions/66806694/pkgdown-fails-parsing-rd-files--examples--added, namely make sure development version downlit installed. articles pkgdown website take time compile, find useful know can run sub-components pkgdown::build_site() individually. See: https://pkgdown.r-lib.org/reference/build_site.html order get pkgdown website render math properly documentation nadir::lnr_homoskedastic_density() follow advice https://github.com/r-lib/pkgdown/issues/2704 user @louisaslett said needed include _pkgdown.yml manually: now, images README.Rmd just embedded manually generated (Feb 27 2025) based lack support Rmd generated images getting copied site docs pkgdown discussed thread https://github.com/r-lib/pkgdown/issues/133 advice put images man/figures/ images can also rendered CRAN. tried following advice https://github.com/r-lib/pkgdown/issues/995 get articles better ordering, didn’t seem work .","code":"library(devtools) install_github('r-lib/downlit') template:   bootstrap: 5   includes:     in_header: |       <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css\" integrity=\"sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+\" crossorigin=\"anonymous\">       <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js\" integrity=\"sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg\" crossorigin=\"anonymous\"><\/script>       <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js\" integrity=\"sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk\" crossorigin=\"anonymous\" onload=\"renderMathInElement(document.body);\"><\/script> # library(nadir)"},{"path":"https://ctesta01.github.io/nadir/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Christian Testa. Author, maintainer. Nima Hejazi. Thesis advisor, author.","code":""},{"path":"https://ctesta01.github.io/nadir/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Testa C, Hejazi N (2025). nadir: Super learning flexible formulas. https://ctesta01.github.io/nadir/, https://github.com/ctesta01/nadir/.","code":"@Manual{,   title = {nadir: Super learning with flexible formulas},   author = {Christian Testa and Nima Hejazi},   year = {2025},   note = {https://ctesta01.github.io/nadir/, https://github.com/ctesta01/nadir/}, }"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"nadir-","dir":"","previous_headings":"","what":"Super Learning with Flexible Formulas","title":"Super Learning with Flexible Formulas","text":"nadir (noun): nā-dir lowest point. Fitting minimum loss based estimation12 literature, nadir implementation super learner algorithm improved support flexible formula based syntax fond functional programming techniques closures, currying, function factories. nadir implements super learner algorithm3. quote Guide SuperLearner4 (previous implementation): SuperLearner algorithm uses cross-validation estimate performance multiple machine learning models, model different settings. creates optimal weighted average models, aka “ensemble”, using test data performance. approach proven asymptotically accurate best possible prediction algorithm tested.","code":""},{"path":"https://ctesta01.github.io/nadir/index.html","id":"why-nadir-and-why-reimplement-super-learner-again","dir":"","previous_headings":"","what":"Why {nadir} and why reimplement super learner again?","title":"Super Learning with Flexible Formulas","text":"previous implementations ({SuperLearner}, {sl3}, {mlr3superlearner}), support flexible formula-based syntax limited, instead opting specifying learners models XX matrix YY outcome vector. Many popular R packages lme4 mgcv (random effects generalized additive models) use formulas extensively specify models using syntax like (age | strata) specify random effects age strata, s(age, income) specify smoothing term age income simultaneously. present, difficult use kinds features SuperLearner, sl3 {mlr3superlearner}. example, easy imagine super learner algorithm appealing modelers fond random effects based models may want hedge exact nature random effects models, sure random intercepts enough random slopes included, etc., similar modeling decisions frameworks. Therefore, nadir package takes charges : Implement syntax easy specify different formulas many candidate learners. make easy pass new learners Super Learner algorithm.","code":""},{"path":"https://ctesta01.github.io/nadir/index.html","id":"installation-instructions","dir":"","previous_headings":"","what":"Installation Instructions","title":"Super Learning with Flexible Formulas","text":"present, nadir available GitHub. Warning: package currently active development may wrong! use serious applications message removed, likely time future release.","code":"devtools::install_github(\"ctesta01/nadir\")"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"demonstration","dir":"","previous_headings":"","what":"Demonstration","title":"Super Learning with Flexible Formulas","text":"First, let’s start simplest possible use case nadir::super_learner(), user like feed data, specification regression formula(s), specify library learners, get back prediction function suitable plugging downstream analyses, like Targeted Learning pure-prediction applications. demo extremely simple application using nadir::super_learner:","code":"library(nadir) ## Registered S3 method overwritten by 'future': ##   method               from       ##   all.equal.connection parallelly # we'll use a few basic learners learners <- list(      glm = lnr_glm,      rf = lnr_rf,      glmnet = lnr_glmnet   ) # more learners are available, see ?learners  sl_model <- super_learner(   data = mtcars,   formula = mpg ~ cyl + hp + disp,   learners = learners)  # the output from super_learner is a prediction function: # here we are producing predictions based on a weighted combination of the # trained learners.  sl_model(mtcars) |> head() ##         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive  ##          20.42305          20.42305          24.20789          19.87290  ## Hornet Sportabout           Valiant  ##          16.95804          19.62781"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"one-step-up-fancy-formula-features","dir":"","previous_headings":"","what":"One Step Up: Fancy Formula Features","title":"Super Learning with Flexible Formulas","text":"Continuing mtcars example, suppose user really like use random effects similar types fancy formula language features. One easy way nadir::super_learner using following syntax:","code":"learners <- list(      glm = lnr_glm,      rf = lnr_rf,      glmnet = lnr_glmnet,      lmer = lnr_lmer,      gam = lnr_gam   )  formulas <- c(   .default = mpg ~ cyl + hp + disp,   # our first three learners use same formula   lmer = mpg ~ (1 | cyl) + hp + disp, # both lme4::lmer and mgcv::gam have    gam = mpg ~ s(hp) + cyl + disp      # specialized formula syntax   )  # fit a super_learner sl_model <- super_learner(   data = mtcars,   formulas = formulas,   learners = learners)    sl_model(mtcars) |> head() ##         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive  ##          20.46252          20.46252          24.27968          19.85617  ## Hornet Sportabout           Valiant  ##          16.95162          19.63611"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"how-should-we-assess-performance-of-nadirsuper_learner","dir":"","previous_headings":"","what":"How should we assess performance of nadir::super_learner()?","title":"Super Learning with Flexible Formulas","text":"put learners super learner algorithm level playing field, ’s important learners super learner evaluated held-validation/test data algorithms seen . Using verbose = TRUE output nadir::super_learner(), can call compare_learners() see mean-squared-error (MSE) held-data, also called CV-MSE, candidate learners specified.  Now go getting CV-MSE super learned model? use cv_super_learner() function performs another layer cross-validation order assess specified super learner folds held-data. ’d like read internals cv_super_learner() work, please refer article Currying, Closures, Function Factories article","code":"# construct our super learner with verbose = TRUE sl_model <- super_learner(   data = mtcars,   formulas = formulas,   learners = learners,   verbose = TRUE)    compare_learners(sl_model) ## Inferring the loss metric for learner comparison based on the outcome type:  ## outcome_type=continuous -> using mean squared error  ## # A tibble: 1 × 5 ##     glm    rf glmnet  lmer   gam ##   <dbl> <dbl>  <dbl> <dbl> <dbl> ## 1  11.8  7.70   11.9  10.1  12.9 pacman::p_load('dplyr', 'ggplot2', 'tidyr', 'magrittr')  truth <- sl_model$holdout_predictions$mpg  holdout_var <- sl_model$holdout_predictions |>   dplyr::group_by(.sl_fold) |>    dplyr::summarize(across(everything(), ~ mean((. - mpg)^2))) |>    dplyr::summarize(across(everything(), var)) |>    select(-mpg, -.sl_fold) |>    t() |>    as.data.frame() |>    tibble::rownames_to_column('learner') |>    dplyr::rename(var = V1) |>   dplyr::mutate(sd = sqrt(var))   jitters <- sl_model$holdout_predictions |>    dplyr::mutate(dplyr::across(-.sl_fold, ~ (. - mpg)^2)) |>    dplyr::select(-mpg) %>%   tidyr::pivot_longer(cols = 2:ncol(.), names_to = 'learner', values_to = 'squared_error') |>   dplyr::group_by(learner, .sl_fold) |>    dplyr::summarize(mse = mean(squared_error)) |>    ungroup() |>    rename(fold = .sl_fold) ## `summarise()` has grouped output by 'learner'. You can override using the ## `.groups` argument. learner_comparison_df <- sl_model |>    compare_learners() |>    t() |>    as.data.frame() |>   tibble::rownames_to_column(var = 'learner') |>    dplyr::mutate(learner = factor(learner)) |>   dplyr::rename(mse = V1) |>   dplyr::left_join(holdout_var) |>    dplyr::mutate(     upper_ci = mse + sd,     lower_ci = mse - sd) |>    dplyr::mutate(learner = forcats::fct_reorder(learner, mse)) ## Inferring the loss metric for learner comparison based on the outcome type:  ## outcome_type=continuous -> using mean squared error ## Joining with `by = join_by(learner)` jitters$learner <- factor(jitters$learner, levels = levels(learner_comparison_df$learner))  learner_comparison_df |>    ggplot2::ggplot(ggplot2::aes(y = learner, x = mse, fill = learner)) +    ggplot2::geom_col(alpha = 0.5) +    ggplot2::geom_jitter(data = jitters, mapping = ggplot2::aes(x = mse), height = .15, shape = 'o') +    ggplot2::geom_pointrange(mapping = ggplot2::aes(xmax = upper_ci, xmin = lower_ci),                            alpha = 0.5) +    ggplot2::theme_bw() +    ggplot2::ggtitle(\"Comparison of Candidate Learners\") +    ggplot2::labs(caption = \"Error bars show ±1 standard deviation across the CV estimated MSE for each learner\\n Each open circle represents the hold-out MSE of one fold of the data\") +    ggplot2::theme(plot.caption.position = 'plot') cv_results <- cv_super_learner(   data = mtcars,   formulas = formulas,   learners = learners)  cv_results ## $cv_trained_learners ## # A tibble: 5 × 4 ##   split learned_predictor predictions mpg       ##   <int> <list>            <list>      <list>    ## 1     1 <function>        <dbl [7]>   <dbl [7]> ## 2     2 <function>        <dbl [7]>   <dbl [7]> ## 3     3 <function>        <dbl [7]>   <dbl [7]> ## 4     4 <function>        <dbl [6]>   <dbl [6]> ## 5     5 <function>        <dbl [5]>   <dbl [5]> ##  ## $cv_loss ## [1] 8.173704 cv_jitters <- cv_results$cv_trained_learners |>    dplyr::select(split, predictions, mpg) |>    tidyr::unnest(cols = c('predictions', 'mpg')) |>    dplyr::group_by(split) |>    dplyr::summarize(mse = mean((mpg - predictions)^2)) |>   dplyr::bind_cols(learner = 'super_learner')   cv_var <- cv_results$cv_trained_learners |>    dplyr::select(split, predictions, mpg) |>    tidyr::unnest(cols = c(predictions, mpg)) |>    dplyr::mutate(squared_error = (mpg - predictions)^2) |>    dplyr::group_by(split) |>    dplyr::summarize(mse = mean(squared_error)) |>    dplyr::summarize(     var = var(mse),     mse = mean(mse),     sd = sqrt(var),     upper_ci = mse + sd,     lower_ci = mse - sd) |>    dplyr::bind_cols(learner = 'super_learner')  new_jitters <- bind_rows(jitters, cv_jitters)  learner_comparison_df |>    bind_rows(cv_var) |>    dplyr::mutate(learner = forcats::fct_reorder(learner, mse)) |>    ggplot2::ggplot(ggplot2::aes(y = learner, x = mse, fill = learner)) +    ggplot2::geom_col(alpha = 0.5) +    ggplot2::geom_jitter(data = new_jitters, mapping = ggplot2::aes(x = mse), height = .15, shape = 'o') +    ggplot2::geom_pointrange(mapping = ggplot2::aes(xmax = upper_ci, xmin = lower_ci),                            alpha = 0.5) +    ggplot2::theme_bw() +    ggplot2::scale_fill_brewer(palette = 'Set2') +    ggplot2::ggtitle(\"Comparison of Candidate Learners against Super Learner\") +    ggplot2::labs(caption = \"Error bars show ±1 standard deviation across the CV estimated MSE for each learner\\n Each open circle represents the hold-out MSE of one fold of the data\") +    ggplot2::theme(plot.caption.position = 'plot')"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"what-about-model-hyperparameters-or-extra-arguments","dir":"","previous_headings":"","what":"What about model hyperparameters or extra arguments?","title":"Super Learning with Flexible Formulas","text":"Model hyperparameters easy handle nadir. Two easy solutions available users: nadir::super_learner() extra_learner_args parameter can passed list extra arguments learner. Users can always build new learners (allows building hyperparameter specification), using ... syntax, ’s easy build new learners learners already provided nadir. ’s examples showing approach.","code":""},{"path":"https://ctesta01.github.io/nadir/index.html","id":"using-extra_learner_args","dir":"","previous_headings":"What about model hyperparameters or extra arguments?","what":"Using extra_learner_args:","title":"Super Learning with Flexible Formulas","text":"","code":"# when using extra_learner_args, it's totally okay to use the  # same learner multiple times as long as their hyperparameters differ.  sl_model <- nadir::super_learner(   data = mtcars,   formula = mpg ~ .,   learners = c(     glmnet0 = lnr_glmnet,     glmnet1 = lnr_glmnet,     glmnet2 = lnr_glmnet,     rf0 = lnr_rf,     rf1 = lnr_rf,     rf2 = lnr_rf     ),   extra_learner_args = list(     glmnet0 = list(lambda = 0.01),     glmnet1 = list(lambda = 0.1),     glmnet2 = list(lambda = 1),     rf0 = list(ntree = 3),     rf1 = list(ntree = 10),     rf2 = list(ntree = 30)     ),   verbose = TRUE )  compare_learners(sl_model) ## Inferring the loss metric for learner comparison based on the outcome type:  ## outcome_type=continuous -> using mean squared error  ## # A tibble: 1 × 6 ##   glmnet0 glmnet1 glmnet2   rf0   rf1   rf2 ##     <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> ## 1    17.6    12.8    10.2  13.6  8.30  8.12"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"building-new-learners-programmatically","dir":"","previous_headings":"What about model hyperparameters or extra arguments?","what":"Building New Learners Programmatically","title":"Super Learning with Flexible Formulas","text":"make sense build new learners hyperparameters built rather using extra_learner_args parameter? One instance building new learners may make sense user like produce large number hyperparameterized learners programmatically, example grid hyperparameter values. show example 1-d grid hyperparameters glmnet.","code":"# produce a \"grid\" of glmnet learners with lambda set to  # exp(-1 to 1 in steps of .1) hyperparameterized_learners <- lapply(   exp(seq(-1, 1, by = .1)),   function(lambda) {      # create a new learner with given lambda     new_learner <- function(data, formula, ...) {         lnr_glmnet(data, formula, lambda = lambda, ...) }          # declare it to be a continuous outcome learner     attr(new_learner, 'sl_lnr_type') <- 'continuous'          return(new_learner)   } )    # give them names because nadir::super_learner requires that the  # learners argument be named. names(hyperparameterized_learners) <- paste0('glmnet', 1:length(hyperparameterized_learners))  # fit the super_learner with 20 glmnets with different lambdas sl_model_glmnet <- nadir::super_learner(   data = mtcars,   learners = hyperparameterized_learners,   formula = mpg ~ .,   verbose = TRUE)  compare_learners(sl_model_glmnet) ## Inferring the loss metric for learner comparison based on the outcome type:  ## outcome_type=continuous -> using mean squared error  ## # A tibble: 1 × 21 ##   glmnet1 glmnet2 glmnet3 glmnet4 glmnet5 glmnet6 glmnet7 glmnet8 glmnet9 ##     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> ## 1    8.42    8.27    8.12    8.00    7.89    7.80    7.72    7.70    7.74 ## # ℹ 12 more variables: glmnet10 <dbl>, glmnet11 <dbl>, glmnet12 <dbl>, ## #   glmnet13 <dbl>, glmnet14 <dbl>, glmnet15 <dbl>, glmnet16 <dbl>, ## #   glmnet17 <dbl>, glmnet18 <dbl>, glmnet19 <dbl>, glmnet20 <dbl>, ## #   glmnet21 <dbl>"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"what-are-currying-closures-and-function-factories","dir":"","previous_headings":"","what":"What are currying, closures, and function factories?","title":"Super Learning with Flexible Formulas","text":"R functional programming language, allows functions build return functions just like return object. refer functions create return another function function factory. extended reference, see Advanced R book. Function factories useful nadir , essence, candidate learner needs able 1) accept training data, 2) produce prediction function can make predictions heldout validation data. typical learner nadir looks like: Moreover, given code-lightweight write simple learner, makes relatively easy users write new learners meet exact needs. want implement learners, just need follow following pseudocode approach: details, read Currying, Closures, Function Factories article","code":"lnr_lm <- function(data, formula, ...) {   model <- stats::lm(formula = formula, data = data, ...)    predict_from_trained_lm <- function(newdata) {     predict(model, newdata = newdata, type = 'response')   }   return(predict_from_trained_lm) } lnr_custom <- function(data, formula, ...) {   model <- # train your model using data, formula, ...       predict_from_model <- function(newdata) {     return(...) # return predictions from the trained model      # (predictions should be a vector of predictions, one for each row of newdata)   }   return(predict_from_model) }"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"what-all-can-super_learner-do","dir":"","previous_headings":"","what":"What all can super_learner() do?","title":"Super Learning with Flexible Formulas","text":"built-support Binary multiclass outcomes Density estimation Using observation weights Running super_learner() parallel Graceful error handling Survival analysis via pooled logistic regression Working clusters strata teaser, ’s example visualization density learner trained penguins data:  also ≥26 tests (counting!) run every update ensure correctness implementation.  View source code tests part nadir: https://github.com/ctesta01/nadir/tree/main/tests/testthat Check complete documentation package website: https://ctesta01.github.io/nadir","code":""},{"path":"https://ctesta01.github.io/nadir/index.html","id":"coming-down-the-pipe-️","dir":"","previous_headings":"","what":"Coming Down the Pipe ↩︎️🚰🔧✨","title":"Super Learning with Flexible Formulas","text":"(Even ) Automated tests try ensure validity/correctness implementation! built-learners (incl. gradient boosted machines, polspline, etc.) may extend time-series outcome types future!","code":""},{"path":"https://ctesta01.github.io/nadir/reference/add_screener.html","id":null,"dir":"Reference","previous_headings":"","what":"Add a Screener to a Learner — add_screener","title":"Add a Screener to a Learner — add_screener","text":"Add Screener Learner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/add_screener.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add a Screener to a Learner — add_screener","text":"","code":"add_screener(learner, screener, screener_extra_args = NULL)"},{"path":"https://ctesta01.github.io/nadir/reference/add_screener.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add a Screener to a Learner — add_screener","text":"learner learner modified wrapping screening stage top . screener screener added top learner screener_extra_args Extra arguments passed screener","code":""},{"path":"https://ctesta01.github.io/nadir/reference/add_screener.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add a Screener to a Learner — add_screener","text":"modified learner called data formula now runs screening stage fitting learner returning prediction function.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/add_screener.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add a Screener to a Learner — add_screener","text":"","code":"if (FALSE) { # \\dontrun{  # construct a learner where variables with less than .6 correlation are screened out lnr_glm_with_cor_60_thresholding <-   add_screener(     learner = lnr_glm,     screener = screener_cor,     screener_extra_args = list(threshold = .6)   )  # train that on the mtcars dataset — also checking that extra arguments are properly passed to glm lnr_glm_with_cor_60_thresholding(mtcars, formula = mpg ~ ., family = \"gaussian\")(mtcars)  # if we've screened out variables with low correlation to mpg, one such variable is qsec, # so changing qsec shouldn't modify the predictions from our learned algorithm mtcars_but_qsec_is_changed <- mtcars mtcars_but_qsec_is_changed$qsec <- rnorm(n = nrow(mtcars))  identical(   lnr_glm_with_cor_60_thresholding(mtcars, formula = mpg ~ .)(mtcars),   lnr_glm_with_cor_60_thresholding(mtcars, formula = mpg ~ .)(mtcars_but_qsec_is_changed)  )  # earth version lnr_earth_with_cor_60_thresholding <-   add_screener(     learner = lnr_earth,     screener = screener_cor,     screener_extra_args = list(threshold = .6)   ) lnr_earth_with_cor_60_thresholding(mtcars, formula = mpg ~ .)(mtcars)  identical(   lnr_earth_with_cor_60_thresholding(mtcars, formula = mpg ~ .)(mtcars),   lnr_earth_with_cor_60_thresholding(mtcars, formula = mpg ~ .)(mtcars)  )  # note that this 'test' does not pass for a learner like randomForest that has # some randomness in its predictions.  } # }"},{"path":"https://ctesta01.github.io/nadir/reference/binary_learners.html","id":null,"dir":"Reference","previous_headings":"","what":"Binary Learners in {nadir} — binary_learners","title":"Binary Learners in {nadir} — binary_learners","text":"lnr_nnet lnr_rf_binary lnr_logistic lnr_multinomial_nnet lnr_multinomial_vglm","code":""},{"path":"https://ctesta01.github.io/nadir/reference/binary_learners.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binary Learners in {nadir} — binary_learners","text":"important thing know binary learners need produce predictions outcome  == 1 TRUE. Also, binary outcomes, make sure use determine_weights_for_binary_outcomes calls super_learner() calculates estimated probability observed outcome (either 0 1) applies negative log loss function afterwards. can done automatically declaring outcome_type = 'binary' calling super_learner() Suppose one trained data fit learner stored. Suppose going call newdata newdata$class outcome variable predicting. important thing know multiclass learners produce predictions outcome class equal newdata$class given covariates specified newdata. Similar density estimation, want use determine_weights_using_neg_log_loss calls super_learner(). can done automatically declaring outcome_type = 'multiclass' calling super_learner()","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/binary_learners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binary Learners in {nadir} — binary_learners","text":"","code":"if (FALSE) { # \\dontrun{   super_learner(     data = mtcars,     learners = list(logistic1 = lnr_logistic, logistic2 = lnr_logistic, lnr_rf_binary),     formulas = list(     .default = am ~ .,     logistic2 = am ~ mpg * hp + .),     outcome_type = 'binary',     verbose = TRUE     ) } # }  if (FALSE) { # \\dontrun{   super_learner(     data = iris,     learners = list(lnr_multinomial_vglm, lnr_multinomial_vglm, lnr_multinomial_nnet),     formulas = list(     .default = Species ~ .,     multinomial_vglm2 = Species ~ Petal.Length*Petal.Width + .),     outcome_type = 'multiclass',     verbose = TRUE     ) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/check_simple_lhs.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate that a formula has a simple left‐hand side check_simple_lhs( ~ x1 + x2) # errors because no lhs — check_simple_lhs","title":"Validate that a formula has a simple left‐hand side check_simple_lhs( ~ x1 + x2) # errors because no lhs — check_simple_lhs","text":"Validate formula simple left‐hand side check_simple_lhs( ~ x1 + x2)   # errors lhs","code":""},{"path":"https://ctesta01.github.io/nadir/reference/check_simple_lhs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate that a formula has a simple left‐hand side check_simple_lhs( ~ x1 + x2) # errors because no lhs — check_simple_lhs","text":"","code":"check_simple_lhs(formula)"},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare Learners — compare_learners","title":"Compare Learners — compare_learners","text":"Compare learners using specified loss_metric","code":""},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare Learners — compare_learners","text":"","code":"compare_learners(sl_output, y_variable, loss_metric)"},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare Learners — compare_learners","text":"sl_output Output running super_learner() verbose_output = TRUE. y_variable character vector indicating outcome variable. y_variable automatically inferred missing can inferred sl_output. loss_metric loss metric, like mean-squared-error negative-log-loss used comparing learners. loss metric take two (vector) arguments: predictions, true outcomes, produce single statistic summarizing performance learner.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare Learners — compare_learners","text":"data.frame loss-metric held-data learner.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare Learners — compare_learners","text":"","code":"if (FALSE) { # \\dontrun{ sl_model <- super_learner(   data = mtcars,   learners = list(lm = lnr_lm, rf = lnr_rf, mean = lnr_mean),   formula = mpg ~ .,   verbose = TRUE)  compare_learners(sl_model)  sl_model <- super_learner(   data = mtcars,   learners = list(lnr_logistic, lnr_rf_binary, mean = lnr_mean),   formula = am ~ mpg,   outcome_type = 'binary',   verbose = TRUE) compare_learners(sl_model) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"Designed handle cross-validation models like randomForest, ranger, glmnet, etc., model matrix newdata must match eactly model matrix training dataset, function intends answer need \"training datasets need every level every discrete-type column appears data.\"","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"","code":"cv_character_and_factors_schema(   data,   n_folds = 5,   cv_sl_mode = TRUE,   check_validation_datasets_too = TRUE )"},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"data Data use training `super_learner`. n_folds number cross-validation folds use constructing `super_learner`. cv_sl_mode binary (default: TRUE) indicator output training/validation data lists used inside another `super_learner` call. , training data needs every level appear least twice data can put training/validation splits. check_validation_datasets_too Enforce validation datasets produced also every level every character / factor type column present. particularly useful learners like `glmnet` require `newx` exact shape/structure training data, binary indicators every level appears.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"list two lists ($training_data $validation_data)   lists length n_folds. entries   data.frame contains nth training validation fold data. named list two lists, list `n_folds` data frames.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"fundamental idea check unique levels character /factor columns represented every training dataset. beyond , function designed support cv_super_learner, inherently involves two layers cross-validation.  result, stringent conditions specified `cv_sl_mode` enabled.  convenience mode enabled default","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"","code":"if (FALSE) { # \\dontrun{ require(palmerpenguins) training_validation_splits <- cv_character_and_factors_schema(   palmerpenguins::penguins)  # we can see the population breakdown across all the training # splits: sapply(training_validation_splits$training_data, function(df) {   table(df$species)   }) # notably, none of them are empty! this is crucial for certain # types of learning algorithms that must see all levels appear in the # training data, like random forests.  # certain models like glmnet require that the prediction dataset # newx have the _exact_ same shape as the training data, so it # can be important that every level appears in the validation data # as well.  check that by looking into these types of tables: sapply(training_validation_splits$validation_data, function(df) {   table(df$species)   })  # if you don't need this level of stringency, but you just want # to make cv_splits where every level appears in the training_data, # you can do so using the check_validation_datasets_too = FALSE # argument. penguins_small <- palmerpenguins::penguins[c(1:3, 154:156, 277:279), ] penguins_small <- penguins_small[complete.cases(penguins_small),]  training_validation_splits <- cv_character_and_factors_schema(   penguins_small,   cv_sl_mode = FALSE,   n_folds = 5,   check_validation_datasets_too = FALSE)  sapply(training_validation_splits$training_data, function(df) {   table(df$species)   })  # now you can see plenty of non-appearing levels in the validation data: sapply(training_validation_splits$validation_data, function(df) {   table(df$species)   }) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_origami_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validation with Origami — cv_origami_schema","title":"Cross-Validation with Origami — cv_origami_schema","text":"Cross-Validation Origami","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_origami_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validation with Origami — cv_origami_schema","text":"","code":"cv_origami_schema(   data = data,   n_folds = 5,   fold_fun = origami::folds_vfold,   cluster_ids = NULL,   strata_ids = NULL,   ... )"},{"path":"https://ctesta01.github.io/nadir/reference/cv_origami_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validation with Origami — cv_origami_schema","text":"data data.frame (similar) split training validation datasets. n_folds number `training_data` `validation_data` data frames make. fold_fun origami::folds_* function cluster_ids vector cluster ids. Clusters treated unit – , observations within cluster placed either training validation set. See ?origami::make_folds. strata_ids vector strata ids. Strata balanced: insofar possible distribution sample distribution training validation sets. See ?origami::make_folds. ... Extra arguments passed origami::make_folds()","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_origami_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validation with Origami — cv_origami_schema","text":"","code":"if (FALSE) { # \\dontrun{  # to use origami::folds_vfold behind the scenes, just tell nadir::super_learner # you want to use cv_origami_schema.  sl_model <- super_learner(   data = mtcars,   formula = mpg ~ cyl + hp,   learners = list(rf = lnr_rf, lm = lnr_lm, mean = lnr_mean),   cv_schema = cv_origami_schema,   verbose = TRUE  )  # if you want to use a different origami::folds_* function, pass it into cv_origami_schema sl_model <- super_learner(   data = mtcars,   formula = mpg ~ cyl + hp,   learners = list(rf = lnr_rf, lm = lnr_lm, mean = lnr_mean),   cv_schema = \\(data, n_folds) {     cv_origami_schema(data, n_folds, fold_fun = origami::folds_loo)     },   verbose = TRUE  ) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"row data assigned one `1:n_folds` random. `` `1:n_folds`, `training_data[[]]` comprised data `sl_fold != `, .e., capturing roughly `(n-folds-1)/n_folds` proportion data.  validation data list dataframes, comprising roughly `1/n_folds` proportion data.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"","code":"cv_random_schema(data, n_folds = 5)"},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"data data.frame (similar) split training validation datasets. n_folds number `training_data` `validation_data` data frames make.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"list two lists ($training_data $validation_data)   lists length n_folds. entries   data.frame contains nth training validation fold data.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"Since assignment folds random, proportions exact guaranteed variability size `training_data` data frame, likewise `validation_data` data frames.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"","code":"if (FALSE) { # \\dontrun{   data(Boston, package = 'MASS')   training_validation_data <- cv_random_schema(Boston, n_folds = 3)   # take a look at what's in the output:   str(training_validation_data, max.level = 2) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validating a `super_learner` — cv_super_learner","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"Produce cv-rmse `super_learner` specified closure accepts data returns `super_learner` prediction function.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"","code":"cv_super_learner(   data,   learners,   formulas,   y_variable = NULL,   n_folds = 5,   determine_super_learner_weights = determine_super_learner_weights_nnls,   ensemble_or_discrete = \"ensemble\",   cv_schema = cv_random_schema,   outcome_type = \"continuous\",   extra_learner_args = NULL,   verbose_output = FALSE,   loss_metric )"},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"data Data use training `super_learner`. learners list predictor/closure-returning-functions. See Details. formulas Either single regression formula vector regression formulas. y_variable Typically `y_variable` can inferred automatically `formulas`, needed, y_variable can specified explicitly. n_folds number cross-validation folds use constructing `super_learner`. determine_super_learner_weights function/method determine weights candidate `learners`. default use `determine_super_learner_weights_nnls`. ensemble_or_discrete Defaults `'ensemble'`, can set `'discrete'`. Discrete super_learner() chooses one candidate learners weight 1 resulting prediction algorithm, ensemble super_learner() combines predictions 1 candidate learners, respective weights adding 1. cv_schema function takes `data`, `n_folds` returns list containing `training_data` `validation_data`, lists `n_folds` data frames. outcome_type One 'continuous', 'binary', 'multiclass', 'density'. outcome_type used infer correct determine_super_learner_weights function explicitly passed. extra_learner_args list equal length `learners` additional arguments pass specified learners. verbose_output `verbose_output = TRUE` return list containing fit learners predictions held-data well prediction function closure trained `super_learner`. loss_metric loss metric function, like mean-squared-error negative-log-loss used evaluating learners held-data minimized convex optimization. loss metric take two (vector) arguments: predictions, true outcomes, produce single statistic summarizing performance learner. Defaults mean-squared-error nadir:::mse().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"list containing $trained_learners $cv_loss   respectively include 1) trained super learner models fold data, holdout predictions ,   2) cross-validated estimate risk (expected loss) held-data.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"idea `cv_super_learner` splits data training/validation splits, trains `super_learner` training split, evaluates predictions held-validation data, calculating root-mean-squared-error held-data. function print message loss_function argument set explicitly, letting user know mean-squared-error used default. Pass loss_function = nadir:::mse super_learner() like suppress message, use similar approach appropriate loss function depending context.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"","code":"if (FALSE) { # \\dontrun{   cv_super_learner(     data = mtcars,     formula = mpg ~ cyl + hp,     learners = list(lnr_mean, lnr_lm, lnr_rf))    cv_super_learner(     data = mtcars,     formula = am ~ cyl + hp,     learners = list(lnr_mean, lnr_lm, lnr_logistic, lnr_rf_binary),     outcome_type = 'binary') } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner_internal.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Cross-Validation to a Super Learner Closure — cv_super_learner_internal","title":"Apply Cross-Validation to a Super Learner Closure — cv_super_learner_internal","text":"Taking sl_closure, function trains super learner one argument data produces predictor function, cv_super_learner_internal applies cross validation sl_closure data passed.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner_internal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Cross-Validation to a Super Learner Closure — cv_super_learner_internal","text":"","code":"cv_super_learner_internal(   data,   sl_closure,   y_variable,   n_folds = 5,   cv_schema = cv_random_schema,   loss_metric,   outcome_type = \"continuous\" )"},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner_internal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Cross-Validation to a Super Learner Closure — cv_super_learner_internal","text":"data Data use training `super_learner`. sl_closure function takes data produces `super_learner` predictor. y_variable string name outcome column `data` n_folds number cross-validation folds use constructing `super_learner`. cv_schema function takes `data`, `n_folds` returns list containing `training_data` `validation_data`, lists `n_folds` data frames. loss_metric loss metric function, like mean-squared-error negative-log-loss used evaluating learners held-data minimized convex optimization. loss metric take two (vector) arguments: predictions, true outcomes, produce single statistic summarizing performance learner. Defaults mean-squared-error nadir:::mse(). outcome_type One 'continuous', 'binary', 'multiclass', 'density'. outcome_type used infer correct determine_super_learner_weights function explicitly passed.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner_internal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Cross-Validation to a Super Learner Closure — cv_super_learner_internal","text":"list containing $trained_learners $cv_loss   respectively include 1) trained super learner models fold data, holdout predictions ,   2) cross-validated estimate risk (expected loss) held-data.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/density_learners.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Density Estimation in the {nadir} Package — density_learners","title":"Conditional Density Estimation in the {nadir} Package — density_learners","text":"following learners available conditional density estimation: lnr_lm_density lnr_glm_density lnr_homoscedastic_density","code":""},{"path":"https://ctesta01.github.io/nadir/reference/density_learners.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Density Estimation in the {nadir} Package — density_learners","text":"important things know conditional density estimation nadir package. Firstly, conditional density learners must produce prediction functions predict _densities_ new outcome values given new covariates. Secondly, implemented density estimators come two flavors: strong assumption (conditional normality), much weaker assumptions.  strong assumption encoded learners like lnr_lm_density lnr_glm_density says \"model predicted mean given covariates, expect remaining errors normally distributed.\" flexible learners produced lnr_homoskedastic_density similar spirit, except fit stats::density kernel bandwidth smoother error distribution (predicting conditional expected mean). subpoint point worth calling attention lnr_homoskedastic_density learner factory. say, given mean_lnr, lnr_homoskedastic_density produces conditional density learner uses mean_lnr. Work ongoing implementing lnr_heteroskedastic_density learner allows predicting higher lower variance conditional density given covariates. Conditional density learners combined negative log loss function using super_learner() using compare_learners. Refer 2003 Dudoit van der Laan paper starting place appropriate loss functions use different types outcomes. <https://biostats.bepress.com/ucbbiostat/paper130/>","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","title":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","text":"function accepts dataframe structured one column `Y` columns unique names corresponding different model predictions `Y`, use nonnegative least squares determine weights use SuperLearner.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","text":"","code":"determine_super_learner_weights_nnls(data, y_variable, obs_weights = NULL)"},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","text":"data data frame consisting outcome (y_variable) columns corresponding predictions candidate learners. y_variable string name outcome column `data`. obs_weights vector weights observation dictate prediction targeted higher weighted observations.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","text":"vector weights used learners.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_for_binary_outcomes.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine Weights Appropriately for Super Learner given Binary Outcomes — determine_weights_for_binary_outcomes","title":"Determine Weights Appropriately for Super Learner given Binary Outcomes — determine_weights_for_binary_outcomes","text":"Determine Weights Appropriately Super Learner given Binary Outcomes","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_for_binary_outcomes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine Weights Appropriately for Super Learner given Binary Outcomes — determine_weights_for_binary_outcomes","text":"","code":"determine_weights_for_binary_outcomes(data, y_variable)"},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_for_binary_outcomes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine Weights Appropriately for Super Learner given Binary Outcomes — determine_weights_for_binary_outcomes","text":"data data.frame columns corresponding predicted probabilities 1 learner true y_variable held-data y_variable character indicating outcome variable data.frame.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_for_binary_outcomes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine Weights Appropriately for Super Learner given Binary Outcomes — determine_weights_for_binary_outcomes","text":"vector weights used learners.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_lik","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_lik","text":"Determine Weights Density Estimators SuperLearner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_lik","text":"","code":"determine_weights_using_neg_log_lik(data, y_variable)"},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_lik","text":"data data.frame columns corresponding predicted densities learner true y_variable held-data y_variable character indicating outcome variable data.frame.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_loss","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_loss","text":"Determine Weights Density Estimators SuperLearner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_loss","text":"","code":"determine_weights_using_neg_log_loss(data, y_variable)"},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_loss","text":"data data.frame columns corresponding predicted densities learner true y_variable held-data y_variable character indicating outcome variable data.frame.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_loss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_loss","text":"vector weights used learners.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/df_to_survival_stacked.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeat Observations for Survival Stacking — df_to_survival_stacked","title":"Repeat Observations for Survival Stacking — df_to_survival_stacked","text":"Per approach *review survival stacking: method cast survival regression analysis classification problem* <https://www.degruyterbrill.com/document/doi/10.1515/ijb-2022-0055/html> <https://arxiv.org/abs/2107.13480>, provide df_to_survival_stacked helper function converting traditional survival data (one observation = one row) survival stacked data structure, repeated observations data structure multiple rows exist individual timepoint still risk set including event time.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/df_to_survival_stacked.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeat Observations for Survival Stacking — df_to_survival_stacked","text":"","code":"df_to_survival_stacked(   data,   id_col = NULL,   time_col,   status_col,   covariate_cols,   period_duration = 1,   custom_times = NULL )"},{"path":"https://ctesta01.github.io/nadir/reference/df_to_survival_stacked.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repeat Observations for Survival Stacking — df_to_survival_stacked","text":"data data frame survival -type outcomes including event indicator time--event--censoring column id_col (string) name id column unique observation data. one specified, one created (called .id) assuming row unique observation. time_col (string) name time‐‐event column status_col (string) name 0/1 event indicator column covariate_cols (string vector) names predictors period_duration (numeric) length time-period (e.g. 1) custom_times (numeric vector) [optional] vector time-period breakpoints. events occurred time zero, begin 0.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/extract_y_variable.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Y Variable from a list of Regression Formulas and Learners — extract_y_variable","title":"Extract Y Variable from a list of Regression Formulas and Learners — extract_y_variable","text":"Extract Y Variable list Regression Formulas Learners","code":""},{"path":"https://ctesta01.github.io/nadir/reference/extract_y_variable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Y Variable from a list of Regression Formulas and Learners — extract_y_variable","text":"","code":"extract_y_variable(formulas, learner_names, data_colnames, y_variable = NULL)"},{"path":"https://ctesta01.github.io/nadir/reference/extract_y_variable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Y Variable from a list of Regression Formulas and Learners — extract_y_variable","text":"formulas vector formulas used super learning learner_names character vector names learners data_colnames column names dataset super learning y_variable (Optional) y_variable specified user","code":""},{"path":"https://ctesta01.github.io/nadir/reference/hello.html","id":null,"dir":"Reference","previous_headings":"","what":"Hello, World! — hello","title":"Hello, World! — hello","text":"Prints 'Hello, world!'.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/hello.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hello, World! — hello","text":"","code":"hello()"},{"path":"https://ctesta01.github.io/nadir/reference/hello.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hello, World! — hello","text":"","code":"hello() #> Error in hello(): could not find function \"hello\""},{"path":"https://ctesta01.github.io/nadir/reference/learners.html","id":null,"dir":"Reference","previous_headings":"","what":"Learners in the {nadir} Package — learners","title":"Learners in the {nadir} Package — learners","text":"following learners available continuous outcomes:","code":""},{"path":"https://ctesta01.github.io/nadir/reference/learners.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Learners in the {nadir} Package — learners","text":"lnr_mean lnr_earth lnr_gam lnr_glm lnr_glmer lnr_glmnet lnr_lm lnr_lmer lnr_ranger lnr_rf lnr_xgboost See ?density_learners learn using conditional density estimation nadir. lnr_mean generally provided benchmarking purposes compare learners ensure correct specification learners, since prediction algorithm (theory) -perform just using mean outcome predictions. like build new learner, recommend reading source code several learners provided {nadir} get sense specified. learner, {nadir} understands , function takes `data`, `formula`, possibly `...`, returns function predicts input `newdata`. simple example reproduced ease reference:","code":""},{"path":"https://ctesta01.github.io/nadir/reference/learners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Learners in the {nadir} Package — learners","text":"","code":"if (FALSE) { # \\dontrun{  lnr_glm <- function(data, formula, weights = NULL, ...) {   model <- stats::glm(formula = formula, data = data, weights = weights, ...)    return(function(newdata) {     predict(model, newdata = newdata, type = 'response')   })   } } # }"},{"path":"https://ctesta01.github.io/nadir/reference/list_known_learners.html","id":null,"dir":"Reference","previous_headings":"","what":"List Known Learners — list_known_learners","title":"List Known Learners — list_known_learners","text":"List Known Learners","code":""},{"path":"https://ctesta01.github.io/nadir/reference/list_known_learners.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Known Learners — list_known_learners","text":"","code":"list_known_learners(type = \"any\")"},{"path":"https://ctesta01.github.io/nadir/reference/list_known_learners.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Known Learners — list_known_learners","text":"type One '' supported outcome type nadir including least 'continuous', 'binary', 'multiclass', 'density'. See ?super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/list_known_learners.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Known Learners — list_known_learners","text":"character vector functions automatically recognized nadir learners prediction/outcome type given.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/list_known_learners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Known Learners — list_known_learners","text":"","code":"list_known_learners() #>  [1] \"lnr_earth\"                   \"lnr_gam\"                     #>  [3] \"lnr_glm\"                     \"lnr_glm_density\"             #>  [5] \"lnr_glmer\"                   \"lnr_glmnet\"                  #>  [7] \"lnr_heteroskedastic_density\" \"lnr_homoskedastic_density\"   #>  [9] \"lnr_lm\"                      \"lnr_lm_density\"              #> [11] \"lnr_lmer\"                    \"lnr_logistic\"                #> [13] \"lnr_mean\"                    \"lnr_multinomial_nnet\"        #> [15] \"lnr_multinomial_vglm\"        \"lnr_nnet\"                    #> [17] \"lnr_ranger\"                  \"lnr_rf\"                      #> [19] \"lnr_rf_binary\"               \"lnr_xgboost\"                 list_known_learners('continuous') #>  [1] \"lnr_earth\"   \"lnr_gam\"     \"lnr_glm\"     \"lnr_glmer\"   \"lnr_glmnet\"  #>  [6] \"lnr_lm\"      \"lnr_lmer\"    \"lnr_mean\"    \"lnr_ranger\"  \"lnr_rf\"      #> [11] \"lnr_xgboost\" list_known_learners('binary') #>  [1] \"lnr_earth\"     \"lnr_gam\"       \"lnr_glm\"       \"lnr_glmer\"     #>  [5] \"lnr_glmnet\"    \"lnr_lm\"        \"lnr_lmer\"      \"lnr_logistic\"  #>  [9] \"lnr_mean\"      \"lnr_nnet\"      \"lnr_ranger\"    \"lnr_rf_binary\" #> [13] \"lnr_xgboost\"   list_known_learners('density') #> [1] \"lnr_glm_density\"             \"lnr_heteroskedastic_density\" #> [3] \"lnr_homoskedastic_density\"   \"lnr_lm_density\"              list_known_learners('multiclass') #> [1] \"lnr_multinomial_nnet\" \"lnr_multinomial_vglm\""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_earth.html","id":null,"dir":"Reference","previous_headings":"","what":"Earth Learner — lnr_earth","title":"Earth Learner — lnr_earth","text":"wrapper earth::earth() use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_earth.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Earth Learner — lnr_earth","text":"","code":"lnr_earth(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_earth.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Earth Learner — lnr_earth","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_earth.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Earth Learner — lnr_earth","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_gam.html","id":null,"dir":"Reference","previous_headings":"","what":"Generalized Additive Model Learner — lnr_gam","title":"Generalized Additive Model Learner — lnr_gam","text":"wrapper mgcv::gam() use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_gam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generalized Additive Model Learner — lnr_gam","text":"","code":"lnr_gam(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_gam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generalized Additive Model Learner — lnr_gam","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_gam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generalized Additive Model Learner — lnr_gam","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"GLM Learner — lnr_glm","title":"GLM Learner — lnr_glm","text":"wrapper stats::glm() use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"GLM Learner — lnr_glm","text":"","code":"lnr_glm(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"GLM Learner — lnr_glm","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"GLM Learner — lnr_glm","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glm_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Normal Density Estimation Given Mean Predictors — with GLMs — lnr_glm_density","title":"Conditional Normal Density Estimation Given Mean Predictors — with GLMs — lnr_glm_density","text":"step lnr_lm_density uses glm conditional mean model. Note allows specification glm features like family = ... ,.. arguments, main advantage lnr_lm_density. Also note still differs using lnr_homoskedastic_density mean_lnr = lnr_glm lnr_homoscedastic_density uses stats::density kernel bandwidth smoothing error distribution mean predictions..","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glm_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Normal Density Estimation Given Mean Predictors — with GLMs — lnr_glm_density","text":"","code":"lnr_glm_density(data, formula, weights, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glm_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Normal Density Estimation Given Mean Predictors — with GLMs — lnr_glm_density","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glm_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Normal Density Estimation Given Mean Predictors — with GLMs — lnr_glm_density","text":"closure (function) produces density estimates newdata given according fit model.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glmer.html","id":null,"dir":"Reference","previous_headings":"","what":"Generalized Linear Mixed-Effects (lme4::glmer) Learner — lnr_glmer","title":"Generalized Linear Mixed-Effects (lme4::glmer) Learner — lnr_glmer","text":"wrapper lme4::glmer() use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glmer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generalized Linear Mixed-Effects (lme4::glmer) Learner — lnr_glmer","text":"","code":"lnr_glmer(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glmer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generalized Linear Mixed-Effects (lme4::glmer) Learner — lnr_glmer","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glmer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generalized Linear Mixed-Effects (lme4::glmer) Learner — lnr_glmer","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"glmnet Learner — lnr_glmnet","title":"glmnet Learner — lnr_glmnet","text":"wrapper glmnet::glmnet() use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"glmnet Learner — lnr_glmnet","text":"","code":"lnr_glmnet(data, formula, weights = NULL, lambda = 0.2, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glmnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"glmnet Learner — lnr_glmnet","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm lambda multiplier parameter penalty; see ?glmnet::glmnet ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"glmnet Learner — lnr_glmnet","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_glmnet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"glmnet Learner — lnr_glmnet","text":"glmnet predictions default, lambda unspecified, return matrix predictions varied lambda values, hence need explicitly handle lambda argument building glmnet learners.","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_hal.html","id":null,"dir":"Reference","previous_headings":"","what":"Highly Adaptive Lasso — lnr_hal","title":"Highly Adaptive Lasso — lnr_hal","text":"Highly Adaptive Lasso","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_hal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Highly Adaptive Lasso — lnr_hal","text":"","code":"lnr_hal(data, formula, weights = NULL, lambda = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_hal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Highly Adaptive Lasso — lnr_hal","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm lambda multiplier parameter penalty; see ?glmnet::glmnet ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_hal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Highly Adaptive Lasso — lnr_hal","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_heteroskedastic_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Density Estimation with Heteroskedasticity — lnr_heteroskedastic_density","title":"Conditional Density Estimation with Heteroskedasticity — lnr_heteroskedastic_density","text":"Conditional Density Estimation Heteroskedasticity","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_heteroskedastic_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Density Estimation with Heteroskedasticity — lnr_heteroskedastic_density","text":"","code":"lnr_heteroskedastic_density(   data,   formula,   mean_lnr,   var_lnr,   mean_lnr_args = NULL,   var_lnr_args = NULL,   density_args = NULL )"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_heteroskedastic_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Density Estimation with Heteroskedasticity — lnr_heteroskedastic_density","text":"data dataframe train learner / learners . formula regression formula use inside learner. mean_lnr learner (function) passed trained data given formula used predict conditional means provided newdata. var_lnr learner (function) passed trained squared error mean_lnr given data used predict expected variance density distribution outcome centered around predicted conditional mean output. mean_lnr_args Extra arguments passed mean_lnr var_lnr_args Extra arguments passed var_lnr density_args Extra arguments passed kernel density smoother stats::density, especially things like bw specifying smoothing bandwidth. See ?stats::density.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_heteroskedastic_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Density Estimation with Heteroskedasticity — lnr_heteroskedastic_density","text":"closure (function) produces density estimates newdata given according fit model.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"function accepting mean_lnr, trains data formula given. stats::density fit error (difference observed outcome mean_lnr predictions).","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"","code":"lnr_homoskedastic_density(   data,   formula,   mean_lnr,   mean_lnr_args = NULL,   density_args = NULL,   weights = NULL )"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"data dataframe train learner / learners . formula regression formula use inside learner. mean_lnr learner (function) passed trained data given formula used predict conditional means provided newdata. mean_lnr_args Extra arguments passed mean_lnr density_args Extra arguments passed kernel density smoother stats::density, especially things like bw specifying smoothing bandwidth. See ?stats::density. weights Observation weights; see ?lm","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"predictor function takes newdata produces density estimates","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"returns function takes newdata produces density estimates according estimated stats::density fit error newdata observed outcome prediction mean_lnr. say, follows following procedure (assuming \\(Y\\) outcome \\(X\\) matrix predictors): $$\\texttt{obtain } \\hat{\\mathbb E}(Y | X) \\quad \\mathtt{using \\quad mean\\_learner}$$ $$\\texttt{fit } \\hat{f} \\gets \\mathtt{density}(Y - \\hat{\\mathbb E}(Y | X))$$ $$\\mathtt{return \\quad  function(newdata) \\{ } \\hat{f}(\\mathtt{newdata\\$Y} -   \\hat{\\mathbb E}[Y | \\mathtt{newdata\\$X}]) \\} $$","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"","code":"if (FALSE) { # \\dontrun{ # fit a conditional density model with mean model as a randomForest fit_density_lnr <- lnr_homoskedastic_density(   data = mtcars,   formula = mpg ~ hp,   mean_lnr = lnr_rf)  # and what we should get back should be predicted densities at the # observed mpg given the covariates hp fit_density_lnr(mtcars) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm.html","id":null,"dir":"Reference","previous_headings":"","what":"Linear Model Learner — lnr_lm","title":"Linear Model Learner — lnr_lm","text":"wrapper lm() use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Linear Model Learner — lnr_lm","text":"","code":"lnr_lm(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Linear Model Learner — lnr_lm","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Linear Model Learner — lnr_lm","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata). prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Normal Density Estimation Given Mean Predictors — lnr_lm_density","title":"Conditional Normal Density Estimation Given Mean Predictors — lnr_lm_density","text":"simplest possible density estimator entertainable.  fits lm model data, uses variance residuals parameterize model data \\(\\mathcal N(y | \\beta x, \\sigma^2)\\).","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Normal Density Estimation Given Mean Predictors — lnr_lm_density","text":"","code":"lnr_lm_density(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Normal Density Estimation Given Mean Predictors — lnr_lm_density","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Normal Density Estimation Given Mean Predictors — lnr_lm_density","text":"closure (function) produces density estimates newdata given according fit model.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lmer.html","id":null,"dir":"Reference","previous_headings":"","what":"Random/Mixed-Effects (lme4::lmer) Learner — lnr_lmer","title":"Random/Mixed-Effects (lme4::lmer) Learner — lnr_lmer","text":"wrapper lme4::lmer use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lmer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random/Mixed-Effects (lme4::lmer) Learner — lnr_lmer","text":"","code":"lnr_lmer(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lmer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random/Mixed-Effects (lme4::lmer) Learner — lnr_lmer","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lmer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Random/Mixed-Effects (lme4::lmer) Learner — lnr_lmer","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_logistic.html","id":null,"dir":"Reference","previous_headings":"","what":"Standard Logistic Regression for Binary Classification — lnr_logistic","title":"Standard Logistic Regression for Binary Classification — lnr_logistic","text":"wrapper provided convenience around lnr_glm sets family = binomial(link = 'logit').","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_logistic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standard Logistic Regression for Binary Classification — lnr_logistic","text":"","code":"lnr_logistic(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_logistic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standard Logistic Regression for Binary Classification — lnr_logistic","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_logistic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Standard Logistic Regression for Binary Classification — lnr_logistic","text":"prediction function accepts newdata, returns   predictions probability outcome 1/TRUE (numeric   vector values, one row newdata).","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_mean.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean Learner — lnr_mean","title":"Mean Learner — lnr_mean","text":"naive/simple learner simply predicts mean outcome every row input newdata.  primarily useful benchmarking confirming learners performing better lnr_mean. Additionally, may case learners -fitting data, giving weight lnr_mean helps reduce -fitting super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_mean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean Learner — lnr_mean","text":"","code":"lnr_mean(data, formula, weights = NULL)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_mean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean Learner — lnr_mean","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_mean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean Learner — lnr_mean","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_multinomial_nnet.html","id":null,"dir":"Reference","previous_headings":"","what":"nnet::multinom Multinomial Learner — lnr_multinomial_nnet","title":"nnet::multinom Multinomial Learner — lnr_multinomial_nnet","text":"nnet::multinom Multinomial Learner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_multinomial_nnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"nnet::multinom Multinomial Learner — lnr_multinomial_nnet","text":"","code":"lnr_multinomial_nnet(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_multinomial_nnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"nnet::multinom Multinomial Learner — lnr_multinomial_nnet","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_multinomial_nnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"nnet::multinom Multinomial Learner — lnr_multinomial_nnet","text":"","code":"df <- mtcars df$cyl <- as.factor(df$cyl) lnr_multinomial_nnet(df, cyl ~ hp + mpg)(df) #>  [1] 1.0000000 1.0000000 1.0000000 0.9996982 1.0000000 1.0000000 1.0000000 #>  [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 #> [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 #> [22] 1.0000000 1.0000000 1.0000000 0.9998992 1.0000000 1.0000000 1.0000000 #> [29] 1.0000000 0.9999127 1.0000000 0.9997039 lnr_multinomial_nnet(iris, Species ~ .)(iris) #>   [1] 1.0000000 0.9999996 1.0000000 0.9999968 1.0000000 1.0000000 1.0000000 #>   [8] 1.0000000 0.9999871 0.9999992 1.0000000 0.9999997 0.9999992 0.9999998 #>  [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.9999999 #>  [22] 1.0000000 1.0000000 0.9999998 0.9999768 0.9999965 0.9999999 1.0000000 #>  [29] 1.0000000 0.9999968 0.9999956 1.0000000 1.0000000 1.0000000 0.9999994 #>  [36] 1.0000000 1.0000000 1.0000000 0.9999987 1.0000000 1.0000000 0.9997542 #>  [43] 0.9999998 1.0000000 0.9999999 0.9999996 1.0000000 0.9999997 1.0000000 #>  [50] 1.0000000 0.9999877 0.9999501 0.9987828 0.9999567 0.9985711 0.9998954 #>  [57] 0.9986727 0.9999997 0.9999850 0.9999848 1.0000000 0.9999615 0.9999999 #>  [64] 0.9991850 0.9999600 0.9999957 0.9986481 1.0000000 0.9401019 0.9999999 #>  [71] 0.5945365 0.9999988 0.7743208 0.9999586 0.9999984 0.9999924 0.9992755 #>  [78] 0.7236305 0.9990177 0.9999917 0.9999999 1.0000000 0.9999997 0.1323524 #>  [85] 0.9977885 0.9997823 0.9996965 0.9997399 0.9999991 0.9999886 0.9999591 #>  [92] 0.9998366 0.9999995 0.9999998 0.9999845 0.9999997 0.9999968 0.9999976 #>  [99] 0.9997776 0.9999976 1.0000000 0.9996078 0.9999990 0.9997148 0.9999999 #> [106] 1.0000000 0.8908074 0.9999954 0.9999919 1.0000000 0.9901387 0.9997381 #> [113] 0.9999794 0.9999665 0.9999999 0.9999950 0.9976741 0.9999999 1.0000000 #> [120] 0.9203566 0.9999996 0.9995049 1.0000000 0.9480610 0.9999819 0.9995521 #> [127] 0.8239052 0.8019269 0.9999992 0.9710712 0.9999968 0.9999172 0.9999999 #> [134] 0.2060534 0.9664645 1.0000000 0.9999999 0.9964650 0.6689415 0.9998686 #> [141] 0.9999999 0.9999423 0.9996078 1.0000000 1.0000000 0.9999929 0.9990906 #> [148] 0.9989764 0.9999955 0.9775646"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_multinomial_vglm.html","id":null,"dir":"Reference","previous_headings":"","what":"VGAM::vglm Multinomial Learner — lnr_multinomial_vglm","title":"VGAM::vglm Multinomial Learner — lnr_multinomial_vglm","text":"VGAM::vglm Multinomial Learner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_multinomial_vglm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"VGAM::vglm Multinomial Learner — lnr_multinomial_vglm","text":"","code":"lnr_multinomial_vglm(data, formula, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_multinomial_vglm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"VGAM::vglm Multinomial Learner — lnr_multinomial_vglm","text":"data dataframe train learner / learners . formula regression formula use inside learner. ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_multinomial_vglm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"VGAM::vglm Multinomial Learner — lnr_multinomial_vglm","text":"","code":"df <- mtcars df$cyl <- as.factor(df$cyl) lnr_multinomial_vglm(df, cyl ~ hp + mpg)(df) #> Warning: 6 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 8 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 13 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 19 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 20 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 22 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 22 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 22 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 22 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 22 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 22 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 22 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: iterations terminated because half-step sizes are very small #> Warning: some quantities such as z, residuals, SEs may be inaccurate due to convergence at a half-step #> Warning: fitted probabilities numerically 0 or 1 occurred #>  [1] 0.8207630 0.8207630 0.9963843 0.6849419 0.8914861 0.9962009 1.0000000 #>  [8] 0.9999998 0.9943654 0.9988847 0.9715516 0.9998795 0.9986678 0.9999951 #> [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.9094081 #> [22] 0.9988749 0.9994949 1.0000000 0.6835611 1.0000000 0.9999940 0.9999998 #> [29] 1.0000000 0.6377564 1.0000000 0.3649949 lnr_multinomial_vglm(iris, Species ~ .)(iris) #> Warning: 2 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 13 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 22 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 34 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 39 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 41 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 47 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 50 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 54 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 59 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 63 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 78 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: 91 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 96 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: 97 diagonal elements of the working weights variable 'wz' have been replaced by 1.819e-12 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: fitted probabilities numerically 0 or 1 occurred #> Warning: fitted values close to 0 or 1 #> Warning: some quantities such as z, residuals, SEs may be inaccurate due to convergence at a half-step #>   [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 #>   [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 #>  [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 #>  [22] 1.0000000 1.0000000 0.9999999 1.0000000 1.0000000 1.0000000 1.0000000 #>  [29] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 #>  [36] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.9999997 #>  [43] 1.0000000 0.9999999 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 #>  [50] 1.0000000 0.9999883 0.9999514 0.9988014 0.9999578 0.9985915 0.9998981 #>  [57] 0.9986943 1.0000000 0.9999854 0.9999852 1.0000000 0.9999626 0.9999999 #>  [64] 0.9992011 1.0000000 0.9999972 0.9986740 1.0000000 0.9404018 0.9999999 #>  [71] 0.5951619 0.9999997 0.7751662 0.9999598 0.9999986 0.9999929 0.9992876 #>  [78] 0.7239383 0.9990348 0.9999998 0.9999999 1.0000000 0.9999999 0.1323701 #>  [85] 0.9978308 0.9997870 0.9997020 0.9997449 0.9999992 0.9999889 0.9999603 #>  [92] 0.9998404 0.9999996 1.0000000 0.9999850 0.9999997 0.9999969 0.9999977 #>  [99] 0.9999997 0.9999977 1.0000000 0.9996139 0.9999990 0.9997188 0.9999999 #> [106] 1.0000000 0.8908123 0.9999955 0.9999921 1.0000000 0.9902584 0.9997429 #> [113] 0.9999800 0.9999673 0.9999999 0.9999952 0.9976994 0.9999999 1.0000000 #> [120] 0.9204923 0.9999996 0.9995130 1.0000000 0.9484339 0.9999824 0.9995586 #> [127] 0.8245440 0.8022990 0.9999992 0.9712013 0.9999969 0.9999189 0.9999999 #> [134] 0.2048741 0.9664047 1.0000000 0.9999999 0.9964973 0.6691425 0.9998717 #> [141] 1.0000000 0.9999440 0.9996139 1.0000000 1.0000000 0.9999932 0.9991067 #> [148] 0.9989939 0.9999956 0.9776789"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_nnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Use nnet for Binary Classification — lnr_nnet","title":"Use nnet for Binary Classification — lnr_nnet","text":"Use nnet Binary Classification","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_nnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use nnet for Binary Classification — lnr_nnet","text":"","code":"lnr_nnet(data, formula, trace = FALSE, size, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_nnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use nnet for Binary Classification — lnr_nnet","text":"data dataframe train learner / learners . formula regression formula use inside learner. trace Whether nnet print optimization success size Size neural network hidden layer ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_nnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use nnet for Binary Classification — lnr_nnet","text":"","code":"lnr_nnet(mtcars, am ~ ., size = 50)(mtcars) #>                             [,1] #> Mazda RX4           9.984213e-01 #> Mazda RX4 Wag       9.984064e-01 #> Datsun 710          9.984219e-01 #> Hornet 4 Drive      8.248964e-05 #> Hornet Sportabout   8.248962e-05 #> Valiant             8.248962e-05 #> Duster 360          8.248962e-05 #> Merc 240D           8.248685e-05 #> Merc 230            2.249520e-03 #> Merc 280            2.257165e-03 #> Merc 280C           2.249568e-03 #> Merc 450SE          8.248962e-05 #> Merc 450SL          8.248962e-05 #> Merc 450SLC         8.248962e-05 #> Cadillac Fleetwood  8.248962e-05 #> Lincoln Continental 8.248962e-05 #> Chrysler Imperial   8.248962e-05 #> Fiat 128            9.984219e-01 #> Honda Civic         9.984219e-01 #> Toyota Corolla      9.984219e-01 #> Toyota Corona       2.272642e-03 #> Dodge Challenger    8.248962e-05 #> AMC Javelin         8.248962e-05 #> Camaro Z28          8.255486e-05 #> Pontiac Firebird    8.248962e-05 #> Fiat X1-9           9.984219e-01 #> Porsche 914-2       9.984219e-01 #> Lotus Europa        9.984226e-01 #> Ford Pantera L      9.982572e-01 #> Ferrari Dino        9.984221e-01 #> Maserati Bora       9.984219e-01 #> Volvo 142E          9.984216e-01 lnr_nnet(iris, I(Species=='setosa') ~ ., size = 50)(iris) #>     [,1] #> 1      0 #> 2      0 #> 3      0 #> 4      0 #> 5      0 #> 6      0 #> 7      0 #> 8      0 #> 9      0 #> 10     0 #> 11     0 #> 12     0 #> 13     0 #> 14     0 #> 15     0 #> 16     0 #> 17     0 #> 18     0 #> 19     0 #> 20     0 #> 21     0 #> 22     0 #> 23     0 #> 24     0 #> 25     0 #> 26     0 #> 27     0 #> 28     0 #> 29     0 #> 30     0 #> 31     0 #> 32     0 #> 33     0 #> 34     0 #> 35     0 #> 36     0 #> 37     0 #> 38     0 #> 39     0 #> 40     0 #> 41     0 #> 42     0 #> 43     0 #> 44     0 #> 45     0 #> 46     0 #> 47     0 #> 48     0 #> 49     0 #> 50     0 #> 51     0 #> 52     0 #> 53     0 #> 54     0 #> 55     0 #> 56     0 #> 57     0 #> 58     0 #> 59     0 #> 60     0 #> 61     0 #> 62     0 #> 63     0 #> 64     0 #> 65     0 #> 66     0 #> 67     0 #> 68     0 #> 69     0 #> 70     0 #> 71     0 #> 72     0 #> 73     0 #> 74     0 #> 75     0 #> 76     0 #> 77     0 #> 78     0 #> 79     0 #> 80     0 #> 81     0 #> 82     0 #> 83     0 #> 84     0 #> 85     0 #> 86     0 #> 87     0 #> 88     0 #> 89     0 #> 90     0 #> 91     0 #> 92     0 #> 93     0 #> 94     0 #> 95     0 #> 96     0 #> 97     0 #> 98     0 #> 99     0 #> 100    0 #> 101    0 #> 102    0 #> 103    0 #> 104    0 #> 105    0 #> 106    0 #> 107    0 #> 108    0 #> 109    0 #> 110    0 #> 111    0 #> 112    0 #> 113    0 #> 114    0 #> 115    0 #> 116    0 #> 117    0 #> 118    0 #> 119    0 #> 120    0 #> 121    0 #> 122    0 #> 123    0 #> 124    0 #> 125    0 #> 126    0 #> 127    0 #> 128    0 #> 129    0 #> 130    0 #> 131    0 #> 132    0 #> 133    0 #> 134    0 #> 135    0 #> 136    0 #> 137    0 #> 138    0 #> 139    0 #> 140    0 #> 141    0 #> 142    0 #> 143    0 #> 144    0 #> 145    0 #> 146    0 #> 147    0 #> 148    0 #> 149    0 #> 150    0"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_ranger.html","id":null,"dir":"Reference","previous_headings":"","what":"ranger Learner — lnr_ranger","title":"ranger Learner — lnr_ranger","text":"wrapper ranger::ranger() use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_ranger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ranger Learner — lnr_ranger","text":"","code":"lnr_ranger(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_ranger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ranger Learner — lnr_ranger","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_ranger.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ranger Learner — lnr_ranger","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_rf.html","id":null,"dir":"Reference","previous_headings":"","what":"randomForest Learner — lnr_rf","title":"randomForest Learner — lnr_rf","text":"wrapper randomForest::randomForest() use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_rf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"randomForest Learner — lnr_rf","text":"","code":"lnr_rf(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_rf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"randomForest Learner — lnr_rf","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_rf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"randomForest Learner — lnr_rf","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/lnr_rf_binary.html","id":null,"dir":"Reference","previous_headings":"","what":"Use Random Forest for Binary Classification — lnr_rf_binary","title":"Use Random Forest for Binary Classification — lnr_rf_binary","text":"Use Random Forest Binary Classification","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_rf_binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use Random Forest for Binary Classification — lnr_rf_binary","text":"","code":"lnr_rf_binary(data, formula, weights = NULL, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_rf_binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use Random Forest for Binary Classification — lnr_rf_binary","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_rf_binary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use Random Forest for Binary Classification — lnr_rf_binary","text":"prediction function accepts newdata, returns   predictions probability outcome 1/TRUE (numeric   vector values, one row newdata).","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_rf_binary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Use Random Forest for Binary Classification — lnr_rf_binary","text":"","code":"lnr_rf_binary(data = mtcars, am ~ mpg)(mtcars) #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>               0.916               0.916               0.398               0.432  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>               0.002               0.000               0.100               0.134  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>               0.398               0.024               0.000               0.214  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>               0.002               0.092               0.008               0.008  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>               0.254               0.996               0.996               0.996  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>               0.174               0.054               0.092               0.040  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>               0.024               0.910               0.878               0.996  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>               0.644               0.660               0.642               0.432"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_xgboost.html","id":null,"dir":"Reference","previous_headings":"","what":"XGBoost Learner — lnr_xgboost","title":"XGBoost Learner — lnr_xgboost","text":"wrapper xgboost::xgboost() use nadir::super_learner().","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_xgboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"XGBoost Learner — lnr_xgboost","text":"","code":"lnr_xgboost(data, formula, weights = NULL, nrounds = 1000, verbose = 0, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_xgboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"XGBoost Learner — lnr_xgboost","text":"data dataframe train learner / learners . formula regression formula use inside learner. weights Observation weights; see ?lm nrounds max number boosting iterations verbose verbose > 0 xgboost::xgboost() print messages fitting process. See ?xgboost::xgboost ... extra arguments passed internal model model fitting purposes.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_xgboost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"XGBoost Learner — lnr_xgboost","text":"prediction function accepts newdata, returns predictions (numeric vector values, one row newdata).","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/make_learner_names_unique.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Unique Learner Names — make_learner_names_unique","title":"Make Unique Learner Names — make_learner_names_unique","text":"Make Unique Learner Names","code":""},{"path":"https://ctesta01.github.io/nadir/reference/make_learner_names_unique.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Unique Learner Names — make_learner_names_unique","text":"","code":"make_learner_names_unique(learners)"},{"path":"https://ctesta01.github.io/nadir/reference/make_learner_names_unique.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Unique Learner Names — make_learner_names_unique","text":"list learners. See ?learners","code":""},{"path":"https://ctesta01.github.io/nadir/reference/make_learner_names_unique.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Unique Learner Names — make_learner_names_unique","text":"list learners (possibly) improved names.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/make_learner_names_unique.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make Unique Learner Names — make_learner_names_unique","text":"","code":"learners <-   list(     mean = lnr_mean,     rf = lnr_rf,     rf = lnr_rf,     lnr_glm,     lnr_xgboost,     function(data, formula) {},     function(data, formula) {}) learners <- nadir:::make_learner_names_unique(learners) names(learners) #> [1] \"mean\"          \"rf_1\"          \"rf_2\"          \"glm\"           #> [5] \"xgboost\"       \"unnamed_lnr_1\" \"unnamed_lnr_2\"  learners <-   list(     lnr_mean,     lnr_rf,     lnr_rf,     lnr_glm,     lnr_xgboost,     function(data, formula) {},     function(data, formula) {}) learners <- nadir:::make_learner_names_unique(learners) names(learners) #> [1] \"mean\"          \"rf_1\"          \"rf_2\"          \"glm\"           #> [5] \"xgboost\"       \"unnamed_lnr_1\" \"unnamed_lnr_2\""},{"path":"https://ctesta01.github.io/nadir/reference/mean_squared.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean Squared — mean_squared","title":"Mean Squared — mean_squared","text":"Mean Squared","code":""},{"path":"https://ctesta01.github.io/nadir/reference/mean_squared.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean Squared — mean_squared","text":"","code":"mean_squared(x)"},{"path":"https://ctesta01.github.io/nadir/reference/mse.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean Squared Error — mse","title":"Mean Squared Error — mse","text":"Mean Squared Error","code":""},{"path":"https://ctesta01.github.io/nadir/reference/mse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean Squared Error — mse","text":"","code":"mse(x, y)"},{"path":"https://ctesta01.github.io/nadir/reference/nadir-package.html","id":null,"dir":"Reference","previous_headings":"","what":"nadir: Super Learning with Flexible Formulas — nadir-package","title":"nadir: Super Learning with Flexible Formulas — nadir-package","text":"`nadir::super_learner()` offers improvements compared past implementations super learner algorithm including ability easily use random-effects specified formulas (like `y ~ (age | strata) + ...`) construction new learners simple writing passing new function.","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/nadir-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"nadir: Super Learning with Flexible Formulas — nadir-package","text":"Maintainer: Christian Testa ctesta@hsph.harvard.edu (ORCID) Authors: Nima Hejazi (ORCID) [thesis advisor]","code":""},{"path":"https://ctesta01.github.io/nadir/reference/nadir_supported_types.html","id":null,"dir":"Reference","previous_headings":"","what":"Outcome types supported by nadir — nadir_supported_types","title":"Outcome types supported by nadir — nadir_supported_types","text":"following outcome types supported nadir package:","code":""},{"path":"https://ctesta01.github.io/nadir/reference/nadir_supported_types.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome types supported by nadir — nadir_supported_types","text":"","code":"nadir_supported_types"},{"path":"https://ctesta01.github.io/nadir/reference/nadir_supported_types.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Outcome types supported by nadir — nadir_supported_types","text":"object class character length 4.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/nadir_supported_types.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Outcome types supported by nadir — nadir_supported_types","text":"continuous binary multiclass density","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Negative Log Likelihood Loss — negative_log_lik_loss","title":"Negative Log Likelihood Loss — negative_log_lik_loss","text":"Negative Log Likelihood Loss","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Negative Log Likelihood Loss — negative_log_lik_loss","text":"","code":"negative_log_lik_loss(predicted_densities, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Negative Log Likelihood Loss — negative_log_lik_loss","text":"predicted_densities","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Negative Log Likelihood Loss — negative_log_lik_loss","text":"negative_log_lik_loss encodes logic: \\(\\hat p_n\\) good model conditional densities, minimize: $$ -\\sum(\\log(\\hat p_n(X_i)) $$","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Negative Log Loss — negative_log_loss","title":"Negative Log Loss — negative_log_loss","text":"Negative Log Loss","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Negative Log Loss — negative_log_loss","text":"","code":"negative_log_loss(predicted_densities, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Negative Log Loss — negative_log_loss","text":"predicted_densities predicted densities learner predicted newdata. ... nadir::compare_learners() passes estimates, truth loss_metric passed , negative_log_loss accepts ... anything .","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_loss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Negative Log Loss — negative_log_loss","text":"sum negative log loss given vector predicted probabilities/densities   observed outcome.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Negative Log Loss — negative_log_loss","text":"negative_log_loss encodes logic: \\(\\hat p_n\\) good model conditional densities, minimize:    $$ -\\sum(\\log(\\hat p_n(X_i)) $$","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_loss_for_binary.html","id":null,"dir":"Reference","previous_headings":"","what":"Negative Log Loss for Binary — negative_log_loss_for_binary","title":"Negative Log Loss for Binary — negative_log_loss_for_binary","text":"Negative Log Loss Binary","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_loss_for_binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Negative Log Loss for Binary — negative_log_loss_for_binary","text":"","code":"negative_log_loss_for_binary(predicted_probabilities, true_outcomes)"},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_loss_for_binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Negative Log Loss for Binary — negative_log_loss_for_binary","text":"predicted_probabilities predicted probabilities learner predicted newdata. true_outcomes vector true outcomes use calculating negative log loss relevant predicted probabilities.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_extra_learner_arguments.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Extra Arguments — parse_extra_learner_arguments","title":"Parse Extra Arguments — parse_extra_learner_arguments","text":"Parse Extra Arguments","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_extra_learner_arguments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Extra Arguments — parse_extra_learner_arguments","text":"","code":"parse_extra_learner_arguments(extra_learner_args, learner_names)"},{"path":"https://ctesta01.github.io/nadir/reference/parse_extra_learner_arguments.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Extra Arguments — parse_extra_learner_arguments","text":"extra_learner_args list extra learner arguments learner_names names learners","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_extra_learner_arguments.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Extra Arguments — parse_extra_learner_arguments","text":"list extra arguments learner, order learner_names","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_formulas.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Formulas for Super Learner — parse_formulas","title":"Parse Formulas for Super Learner — parse_formulas","text":"Parse Formulas Super Learner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_formulas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Formulas for Super Learner — parse_formulas","text":"","code":"parse_formulas(formulas, learner_names)"},{"path":"https://ctesta01.github.io/nadir/reference/parse_formulas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Formulas for Super Learner — parse_formulas","text":"formulas Formulas passed learner super learner learner_names names learners passed super learner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor.html","id":null,"dir":"Reference","previous_headings":"","what":"Correlation Threshold Based Screening — screener_cor","title":"Correlation Threshold Based Screening — screener_cor","text":"Correlation Threshold Based Screening","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Correlation Threshold Based Screening — screener_cor","text":"","code":"screener_cor(data, formula, threshold = 0.2, cor... = NULL)"},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Correlation Threshold Based Screening — screener_cor","text":"data dataframe intended used super_learner() formula formula specifying regression done threshold correlation coefficient cutoff, variables screened dataset regression formula. cor... optional list extra arguments pass cor. Use method = 'spearman' Spearman rank based correlation coefficient.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Correlation Threshold Based Screening — screener_cor","text":"list $data columns screened , $formula variables screened , $failed_to_correlate_names names variables failed correlate outcome least threshold level.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Correlation Threshold Based Screening — screener_cor","text":"variable used little correlation outcome predicted, might want screen variable predictors. large datasets, quite important, huge number columns computationally intractable frustratingly time-consuming run super_learner() .","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Correlation Threshold Based Screening — screener_cor","text":"","code":"if (FALSE) { # \\dontrun{ screener_cor(   data = mtcars,   formula = mpg ~ .,   threshold = .5)  # We're also showing how to specify that you want the Spearman rank-based # correlation coefficient, to get away from the assumption of linearity.  screener_cor(   data = mtcars,   formula = mpg ~ .,   threshold = .5,   cor... = list(method = 'spearman')   ) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor_top_n.html","id":null,"dir":"Reference","previous_headings":"","what":"Correlation Threshold Based Screening — screener_cor_top_n","title":"Correlation Threshold Based Screening — screener_cor_top_n","text":"Correlation Threshold Based Screening","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor_top_n.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Correlation Threshold Based Screening — screener_cor_top_n","text":"","code":"screener_cor_top_n(data, formula, keep_n_terms, cor... = NULL)"},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor_top_n.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Correlation Threshold Based Screening — screener_cor_top_n","text":"data dataframe intended used super_learner() formula formula specifying regression done keep_n_terms Set integer value >=1, indicates top n terms model frame greatest absolute correlation outcome kept. cor... optional list extra arguments pass cor. Use method = 'spearman' Spearman rank based correlation coefficient.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor_top_n.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Correlation Threshold Based Screening — screener_cor_top_n","text":"list $data columns screened , $formula variables screened , $failed_to_correlate_names names variables failed correlate outcome least threshold level.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor_top_n.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Correlation Threshold Based Screening — screener_cor_top_n","text":"variable used little correlation outcome predicted, might want screen variable predictors. large datasets, quite important, huge number columns computationally intractable frustratingly time-consuming run super_learner() .","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_cor_top_n.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Correlation Threshold Based Screening — screener_cor_top_n","text":"","code":"if (FALSE) { # \\dontrun{ screener_cor_top_n(   data = mtcars,   formula = mpg ~ .,   keep_n_terms = 5)  # We're also showing how to specify that you want the Spearman rank-based # correlation coefficient, to get away from the assumption of linearity.  screener_cor_top_n(   data = mtcars,   formula = mpg ~ .,   keep_n_terms = 5,   cor... = list(method = 'spearman')   ) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/screener_t_test.html","id":null,"dir":"Reference","previous_headings":"","what":"t-test Based Screening: Thresholds on p.values and/or t statistics — screener_t_test","title":"t-test Based Screening: Thresholds on p.values and/or t statistics — screener_t_test","text":"Screens variables formula dataset based p.value /absolute value t statistic univariate linear regression (intercept one term) comparing predictor outcome (dependent) variable.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_t_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"t-test Based Screening: Thresholds on p.values and/or t statistics — screener_t_test","text":"","code":"screener_t_test(   data,   formula,   p_value_threshold = NULL,   t_statistic_threshold = NULL )"},{"path":"https://ctesta01.github.io/nadir/reference/screener_t_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"t-test Based Screening: Thresholds on p.values and/or t statistics — screener_t_test","text":"data dataset variables mentioned formula formula formula terms data, intended used learner nadir. p_value_threshold numeric scalar terms pass t test linear model coefficient p value lower equal p_value_threshold given. t_statistic_threshold numeric scalar terms pass t test statistic greater equal t_statistic_threshold given.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_t_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"t-test Based Screening: Thresholds on p.values and/or t statistics — screener_t_test","text":"list $data columns screened , $formula variables screened , $failed_to_pass_threshold names variables failed associate outcome least threshold level.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screener_t_test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"t-test Based Screening: Thresholds on p.values and/or t statistics — screener_t_test","text":"intended use screener_t_test screeners pragmatic purposes: large number candidate predictors, super_learner slow run, predictor variables fail detectable association dependent variable formula dropped learner.","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/screeners.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapping Learners with a Screener — screeners","title":"Wrapping Learners with a Screener — screeners","text":"Screeners work principle take arguments learner return modified dataset formula variables failed meet threshold screened .","code":""},{"path":"https://ctesta01.github.io/nadir/reference/screeners.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Wrapping Learners with a Screener — screeners","text":"screener can added learner using add_screener(learner, screener) function provided.  returns modified learner implements screening based data formula passed. far, screeners implemented rely able call model.matrix therefore support standard (generalized) linear model syntax like mentioned ?formula.","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/screeners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapping Learners with a Screener — screeners","text":"","code":"if (FALSE) { # \\dontrun{   # examples for setting up a screened regression problem:   #   # users can just run a screener to see what data and formula terms pass the   # given screener conditions:    screened_regression_problem <- screener_cor(data = mtcars, formula = mpg ~ ., threshold = 0.5)   screened_regression_problem    screened_regression_problem2 <- screener_cor(data = mtcars, formula = mpg ~ ., threshold = 0.5, cor... = list(method = 'spearman'))   screened_regression_problem2    screened_regression_problem3 <- screener_t_test(data = mtcars, formula = mpg ~ ., t_statistic_threshold = 10)   screened_regression_problem3    # build a new learner with screening builtin:    lnr_rf_screener_top_5_cor_terms <- add_screener(      learner = lnr_rf,      screener = screener_cor_top_n,      screener_extra_args = list(cor... = list(method = 'spearman'),                                 keep_n_terms = 5)    )    # train learner   trained_learner <- lnr_rf_screener_top_5_cor_terms(data = mtcars, formula = mpg ~ .)   mtcars_modified <- mtcars   mtcars_modified['gear'] <- 1 # gear is one of the least correlated variables with mpg   identical(trained_learner(mtcars), trained_learner(mtcars_modified)) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Softmax — softmax","title":"Softmax — softmax","text":"common transformation used go collection numbers R numbers [0,1] sum 1.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softmax — softmax","text":"","code":"softmax(beta)"},{"path":"https://ctesta01.github.io/nadir/reference/softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softmax — softmax","text":"beta vector numeric values transform","code":""},{"path":"https://ctesta01.github.io/nadir/reference/stochastic_round.html","id":null,"dir":"Reference","previous_headings":"","what":"Round up or down randomly with probability equal to the decimal part of x — stochastic_round","title":"Round up or down randomly with probability equal to the decimal part of x — stochastic_round","text":"Round randomly probability equal decimal part x","code":""},{"path":"https://ctesta01.github.io/nadir/reference/stochastic_round.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Round up or down randomly with probability equal to the decimal part of x — stochastic_round","text":"","code":"stochastic_round(x)"},{"path":"https://ctesta01.github.io/nadir/reference/stochastic_round.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Round up or down randomly with probability equal to the decimal part of x — stochastic_round","text":"x numeric vector","code":""},{"path":"https://ctesta01.github.io/nadir/reference/stochastic_round.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Round up or down randomly with probability equal to the decimal part of x — stochastic_round","text":"vector integer values","code":""},{"path":"https://ctesta01.github.io/nadir/reference/stochastic_round.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Round up or down randomly with probability equal to the decimal part of x — stochastic_round","text":"","code":"for (i in 1:3) {   print(nadir:::stochastic_round(c(1.01, 1.99, 1.5, 0.5, 1.6))) } #> [1] 1 2 2 1 2 #> [1] 1 2 1 0 2 #> [1] 1 2 1 1 1 #> [1] 1 2 2 0 2 #> [1] 1 2 1 1 2 #> [1] 1 2 1 0 1  nadir:::stochastic_round(c(-1.01, 2.99, -5.5, 15.5, 51.6)) #> [1] -1  3 -5 15 51 #> [1] -1  3 -5 15 51"},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","title":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","text":"Super learning functional programming!","code":""},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","text":"","code":"super_learner(   data,   learners,   formulas,   y_variable = NULL,   n_folds = 5,   determine_super_learner_weights,   ensemble_or_discrete = \"ensemble\",   cv_schema,   outcome_type = \"continuous\",   extra_learner_args = NULL,   verbose_output = FALSE,   cluster_ids,   strata_ids,   weights = NULL )"},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","text":"data Data use training `super_learner`. learners list predictor/closure-returning-functions. See Details. formulas Either single regression formula vector regression formulas. y_variable Typically `y_variable` can inferred automatically `formulas`, needed, y_variable can specified explicitly. n_folds number cross-validation folds use constructing `super_learner`. determine_super_learner_weights function/method determine weights candidate `learners`. default use `determine_super_learner_weights_nnls`. ensemble_or_discrete Defaults `'ensemble'`, can set `'discrete'`. Discrete super_learner() chooses one candidate learners weight 1 resulting prediction algorithm, ensemble super_learner() combines predictions 1 candidate learners, respective weights adding 1. cv_schema function takes `data`, `n_folds` returns list containing `training_data` `validation_data`, lists `n_folds` data frames. outcome_type One 'continuous', 'binary', 'multiclass', 'density'. outcome_type used infer correct determine_super_learner_weights function explicitly passed. extra_learner_args list equal length `learners` additional arguments pass specified learners. verbose_output `verbose_output = TRUE` return list containing fit learners predictions held-data well prediction function closure trained `super_learner`. cluster_ids (default: null) specified, clusters either entirely assigned training validation () cross-validation split. strata_ids (default: null) specified, strata balanced across training validation splits strata appear training validation splits. weights specified, (per observation) weights used indicate risk minimization across models (.e., meta-learning step) targeted higher weight observations.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","text":"goal super learner use cross-validation set candidate learners 1) evaluate learners perform held data 2) use evaluation produce weighted average (continuous super learner) pick best learner (discrete super learner) specified candidate learners. Super learner statistically desirable properties written length, including least following references: * <https://biostats.bepress.com/ucbbiostat/paper222/>   * <https://www.stat.berkeley.edu/users/laan/Class/Class_subpages/BASS_sec1_3.1.pdf> `nadir::super_learner` adopts several user-interface design-perspectives useful know understanding works: * specification learners _very flexible_, really   constrained fact candidate learners designed   prediction problem details can wildly vary   learner learner.   * easy specify customized new learner. `nadir::super_learner` core accepts `data`, `formula` (single one passed `formulas` fine), list `learners`. `learners` taken lists functions following specification: * learner must accept `data` `formula` argument,   * learner may accept arguments,   * learner must return prediction function accepts `newdata` produces vector prediction values given `newdata`. essence, learner specified function taking (`data`, `formula`, ...) returning _closure_ (see <http://adv-r..co.nz/Functional-programming.html#closures> introduction closures) function accepting `newdata` returning predictions. Since many candidate learners hyperparameters tuned, like depth trees random forests, `lambda` parameter `glmnet`, extra arguments can passed learner via `extra_learner_args` argument. `extra_learner_args` list lists, one list extra arguments learner. additional arguments needed learners, learners using require additional arguments, can just put `NULL` value `extra_learner_args`. See examples. order seamlessly support using features implemented extensions formula syntax (like random effects formatted like random intercepts slopes use `(age | strata)` syntax `lme4` splines like `s(age | strata)` `mgcv`), allow `formulas` argument either one fixed formula `super_learner` use models, vector formulas, one learner specified. Note examples mean-squared-error (mse) calculated training/test set, useful crude diagnostic see super_learner working. rigorous performance metric evaluate `super_learner` cv-rmse produced cv_super_learner.","code":""},{"path":[]},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","text":"","code":"if (FALSE) { # \\dontrun{  learners <- list(      glm = lnr_glm,      rf = lnr_rf,      glmnet = lnr_glmnet,      lmer = lnr_lmer   )  # mtcars example --- formulas <- c(   .default = mpg ~ cyl + hp, # first three models use same formula   lmer = mpg ~ (1 | cyl) + hp # lme4 uses different language features   )  # fit a super_learner sl_model <- super_learner(   data = mtcars,   formula = formulas,   learners = learners,   verbose = TRUE)  # We recommend taking a look at this object, and comparing it to the sole function # returned when verbose = FALSE.  tip: It's the $sl_predictor function in the # verbose output. sl_model  compare_learners(sl_model)  # iris example --- sl_model <- super_learner(   data = iris,   formula = list(     .default = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,     lmer = Sepal.Length ~ (Sepal.Width | Species) + Petal.Length),   learners = learners,   verbose = TRUE)  # produce super_learner predictions and compare against the individual learners compare_learners(sl_model) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/validate_learner_types.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Learner Types — validate_learner_types","title":"Validate Learner Types — validate_learner_types","text":"Validate Learner Types","code":""},{"path":"https://ctesta01.github.io/nadir/reference/validate_learner_types.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Learner Types — validate_learner_types","text":"","code":"validate_learner_types(learners, outcome_type)"},{"path":"https://ctesta01.github.io/nadir/reference/validate_learner_types.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Learner Types — validate_learner_types","text":"learners list learners. See ?learners outcome_type outcome type nadir::super_learner() supports","code":""}]
