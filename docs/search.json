[{"path":"https://ctesta01.github.io/nadir/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Christian Testa. Maintainer.","code":""},{"path":"https://ctesta01.github.io/nadir/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Testa C (2025). nadir: implementation Super Learner algorithm fond closures flexible syntax. R package version 0.0.001, https://ctesta01.github.io/nadir/.","code":"@Manual{,   title = {nadir: An implementation of the Super Learner algorithm fond of closures and flexible syntax},   author = {Christian Testa},   year = {2025},   note = {R package version 0.0.001},   url = {https://ctesta01.github.io/nadir/}, }"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"why-nadir-and-why-reimplement-super-learner-again","dir":"","previous_headings":"","what":"Why {nadir} and why reimplement Super Learner again?","title":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","text":"previous implementations ({SuperLearner}, {sl3}, {mlr3superlearner}), support flexible formula-based syntax limited, instead opting specifying learners models XX matrix YY outcome vector. Many popular R packages lme4 mgcv (random effects generalized additive models) use formulas extensively specify models using syntax like (age | strata) specify random effects age strata, s(age, income) specify smoothing term age income simultaneously. present, difficult use kinds features SuperLearner, sl3 {ml3superlearner}. example, easy imagine Super Learner algorithm appealing modelers fond random effects based models may want hedge exact nature random effects models, sure random intercepts enough random slopes included, etc., similar modeling decisions frameworks. Therefore, nadir package takes charges : Implement syntax easy specify different formulas many candidate learners. make easy pass new learners Super Learner algorithm.","code":""},{"path":"https://ctesta01.github.io/nadir/index.html","id":"installation-instructions","dir":"","previous_headings":"","what":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","title":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","text":"present, nadir available GitHub. Warning: package currently active development may wrong! use serious applications message removed, likely time future release.","code":"devtools::install_github(\"ctesta01/nadir\")"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"demonstration","dir":"","previous_headings":"","what":"Demonstration","title":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","text":"First, let’s start simplest possible use case nadir::super_learner(), user like feed data, specification regression formula(s), specify library learners, get back prediction function suitable plugging downstream analyses, like Targeted Learning pure-prediction applications. demo extremely simple application using nadir::super_learner:","code":"library(nadir)  # we'll use a few basic learners learners <- list(      glm = lnr_glm,      rf = lnr_randomForest,      glmnet = lnr_glmnet   ) # more learners are available, see ?learners  sl_model <- super_learner(   data = mtcars,   regression_formula = mpg ~ cyl + hp,   learners = learners)  # the output from super_learner is a prediction function: # here we are producing predictions based on a weighted combination of the # trained learners.  sl_model(mtcars) |> head() ##         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive  ##          20.40028          20.40028          24.93155          20.40028  ## Hornet Sportabout           Valiant  ##          16.86985          19.92227"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"one-step-up-fancy-formula-features","dir":"","previous_headings":"","what":"One Step Up: Fancy Formula Features","title":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","text":"Continuing mtcars example, suppose user really like use random effects similar types fancy formula language features. One easy way nadir::super_learner using following syntax:","code":"learners <- list(      glm = lnr_glm,      rf = lnr_randomForest,      glmnet = lnr_glmnet,      lmer = lnr_lmer,      gam = lnr_gam   )  regression_formulas <- c(   .default = mpg ~ cyl + hp,   # our first three learners use same formula   lmer = mpg ~ (1 | cyl) + hp, # both lme4::lmer and mgcv::gam have    gam = mpg ~ s(hp) + cyl      # specialized formula syntax   )  # fit a super_learner sl_model <- super_learner(   data = mtcars,   regression_formulas = regression_formulas,   learners = learners)    sl_model(mtcars) |> head() ##         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive  ##          20.33819          20.33819          24.94327          20.33819  ## Hornet Sportabout           Valiant  ##          16.87255          19.99762"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"how-should-we-assess-performance-of-nadirsuper_learner","dir":"","previous_headings":"","what":"How should we assess performance of nadir::super_learner()?","title":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","text":"put learners super learner algorithm level playing field, ’s important learners super learner evaluated held-validation/test data algorithms seen . Using verbose = TRUE output nadir::super_learner(), can call compare_learners() see mean-squared-error (MSE) held-data, also called CV-MSE, candidate learners specified. Now go getting CV-MSE super learned model? curry super learner function takes data (additional specification built ) returns prediction function (.e., closure). Technical aside: “” curry function? Well, perform cross-validation super_learner(), behind scenes, ’re going want split data training/validation sets apply super_learner() training sets, producing prediction-closure , can predict onto held-validation data. perspective, basically want function takes one input (training data) spits relevant prediction function (closure). Don’t let complicated language scare ; ’s fairly straightforward. Essentially just need wrap super learner specification inside sl_closure <- function(data) { ... }, make sure specify data = data inside inner super_learner() call, ’re done. return value function closure super_learner() returns already closure eats newdata returns predictions.","code":"# construct our super learner with verbose = TRUE sl_model <- super_learner(   data = mtcars,   regression_formulas = regression_formulas,   learners = learners,   verbose = TRUE)    compare_learners(sl_model) ## The default in nadir::compare_learners is to use CV-MSE for comparing learners.  ## Other metrics can be set using the metric argument to compare_learners.  ## # A tibble: 1 × 5 ##     glm    rf glmnet  lmer   gam ##   <dbl> <dbl>  <dbl> <dbl> <dbl> ## 1  12.0  11.0   11.9  12.2  13.2 sl_closure_mtcars <- function(data) {   nadir::super_learner(   data = data,   regression_formulas = regression_formulas,   learners = learners   ) }  cv_super_learner(data = mtcars, sl_closure_mtcars,                   y_variable = 'mpg',                  n_folds = 5)$cv_mse ## boundary (singular) fit: see help('isSingular')  ## [1] 10.00147 # iris example --- sl_model_iris <- super_learner(   data = iris,   regression_formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,   learners = learners[1:3],   verbose = TRUE)    compare_learners(sl_model_iris) ## The default in nadir::compare_learners is to use CV-MSE for comparing learners.  ## Other metrics can be set using the metric argument to compare_learners.  ## # A tibble: 1 × 3 ##     glm    rf glmnet ##   <dbl> <dbl>  <dbl> ## 1 0.101 0.139  0.208 sl_closure_iris <- function(data) {   nadir::super_learner(   data = data,   regression_formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,   learners = learners[1:3]) }  cv_super_learner(data = iris, sl_closure_iris, y_variable = 'Sepal.Length')$cv_mse ## [1] 0.1009702"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"what-about-model-hyperparameters-or-extra-arguments","dir":"","previous_headings":"","what":"What about model hyperparameters or extra arguments?","title":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","text":"Model hyperparameters easy handle nadir. Two easy solutions available users: nadir::super_learner() extra_learner_args parameter can passed list extra arguments learner. Users can always build new learners (allows building hyperparameter specification), using ... syntax, ’s easy build new learners learners already provided nadir. ’s examples showing approach.","code":""},{"path":"https://ctesta01.github.io/nadir/index.html","id":"using-extra_learner_args","dir":"","previous_headings":"What about model hyperparameters or extra arguments?","what":"Using extra_learner_args:","title":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","text":"","code":"# when using extra_learner_args, it's totally okay to use the  # same learner multiple times as long as their hyperparameters differ.  sl_model <- nadir::super_learner(   data = mtcars,   regression_formula = mpg ~ .,   learners = c(     glmnet0 = lnr_glmnet,     glmnet1 = lnr_glmnet,     glmnet2 = lnr_glmnet,     rf0 = lnr_randomForest,     rf1 = lnr_randomForest,     rf2 = lnr_randomForest     ),   extra_learner_args = list(     glmnet0 = list(lambda = 0.01),     glmnet1 = list(lambda = 0.1),     glmnet2 = list(lambda = 1),     rf0 = list(ntree = 30),     rf1 = list(ntree = 30),     rf2 = list(ntree = 30)     ),   verbose = TRUE )  compare_learners(sl_model) ## The default in nadir::compare_learners is to use CV-MSE for comparing learners.  ## Other metrics can be set using the metric argument to compare_learners.  ## # A tibble: 1 × 6 ##   glmnet0 glmnet1 glmnet2   rf0   rf1   rf2 ##     <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> ## 1    10.3    6.85    8.09  5.81  6.45  5.98"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"building-new-learners-programmatically","dir":"","previous_headings":"What about model hyperparameters or extra arguments?","what":"Building New Learners Programmatically","title":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","text":"make sense build new learners hyperparameters built rather using extra_learner_args parameter? One instance building new learners may make sense user like produce large number hyperparameterized learners programmatically, example grid hyperparameter values. show example 1-d grid hyperparameters glmnet.","code":"# produce a \"grid\" of glmnet learners with lambda set to  # exp(-1 to 1 in steps of .1) hyperparameterized_learners <- lapply(   exp(seq(-1, 1, by = .1)),    function(lambda) {      return(       function(data, regression_formula, ...) {         lnr_glmnet(data, regression_formula, lambda = lambda, ...)         })   })    # give them names because nadir::super_learner requires that the  # learners argument be named. names(hyperparameterized_learners) <- paste0('glmnet', 1:length(hyperparameterized_learners))  # fit the super_learner with 20 glmnets with different lambdas sl_model_glmnet <- nadir::super_learner(   data = mtcars,   learners = hyperparameterized_learners,   regression_formula = mpg ~ .,   verbose = TRUE)  compare_learners(sl_model_glmnet) ## The default in nadir::compare_learners is to use CV-MSE for comparing learners.  ## Other metrics can be set using the metric argument to compare_learners.  ## # A tibble: 1 × 21 ##   glmnet1 glmnet2 glmnet3 glmnet4 glmnet5 glmnet6 glmnet7 glmnet8 glmnet9 ##     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> ## 1    9.31    9.26    9.26    9.28    9.35    9.46    9.63    9.77    9.93 ## # ℹ 12 more variables: glmnet10 <dbl>, glmnet11 <dbl>, glmnet12 <dbl>, ## #   glmnet13 <dbl>, glmnet14 <dbl>, glmnet15 <dbl>, glmnet16 <dbl>, ## #   glmnet17 <dbl>, glmnet18 <dbl>, glmnet19 <dbl>, glmnet20 <dbl>, ## #   glmnet21 <dbl>"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"coming-down-the-pipe","dir":"","previous_headings":"","what":"Coming Down the Pipe","title":"An implementation of the Super Learner algorithm fond of closures and flexible syntax","text":"Automated tests try ensure validity/correctness implementation! future future.apply origami Hopefully pkgdown website vignettes soon.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare Learners — compare_learners","title":"Compare Learners — compare_learners","text":"Compare learners using specified loss_metric","code":""},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare Learners — compare_learners","text":"","code":"compare_learners(sl_output, y_variable, loss_metric)"},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare Learners — compare_learners","text":"sl_output Output nadir::super_learner() verbose = TRUE","code":""},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare Learners — compare_learners","text":"","code":"if (FALSE) { # \\dontrun{ sl_model <- super_learner(   data = mtcars,   learners = list(lm = lnr_lm, rf = lnr_randomForest, mean = lnr_mean),   regression_formula = mpg ~ .,   verbose = TRUE)  compare_learners(sl_model) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"Designed handle cross-validation models like randomForest, ranger, glmnet, etc., model matrix newdata must match eactly model matrix training dataset, function intends answer need \"training datasets need every level every discrete-type column appears data.\"","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"","code":"cv_character_and_factors_schema(   data,   n_folds = 5,   cv_sl_mode = TRUE,   check_validation_datasets_too = TRUE )"},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"data Data use training `super_learner`. n_folds number cross-validation folds use constructing `super_learner`. cv_sl_mode binary (default: TRUE) indicator output training/validation data lists used inside another `super_learner` call. , training data needs every level appear least twice data can put training/validation splits. check_validation_datasets_too Enforce validation datasets produced also every level every character / factor type column present. particularly useful learners like `glmnet` require `newx` exact shape/structure training data, binary indicators every level appears.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"fundamental idea check unique levels character /factor columns represented every training dataset. beyond , function designed support cv_super_learner, inherently involves two layers cross-validation.  result, stringent conditions specified `cv_sl_mode` enabled.  convenience mode enabled default","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns — cv_character_and_factors_schema","text":"","code":"if (FALSE) { # \\dontrun{ require(palmerpenguins) training_validation_splits <- cv_character_and_factors_schema(   palmerpenguins::penguins)  # we can see the population breakdown across all the training # splits: sapply(training_validation_splits$training_data, function(df) {   table(df$species)   }) # notably, none of them are empty! this is crucial for certain # types of learning algorithms that must see all levels appear in the # training data, like random forests.  # certain models like glmnet require that the prediction dataset # newx have the _exact_ same shape as the training data, so it # can be important that every level appears in the validation data # as well.  check that by looking into these types of tables: sapply(training_validation_splits$validation_data, function(df) {   table(df$species)   })  # if you don't need this level of stringency, but you just want # to make cv_splits where every level appears in the training_data, # you can do so using the check_validation_datasets_too = FALSE # argument. penguins_small <- palmerpenguins::penguins[c(1:3, 154:156, 277:279), ] penguins_small <- penguins_small[complete.cases(penguins_small),]  training_validation_splits <- cv_character_and_factors_schema(   penguins_small,   cv_sl_mode = FALSE,   n_folds = 5,   check_validation_datasets_too = FALSE)  sapply(training_validation_splits$training_data, function(df) {   table(df$species)   })  # now you can see plenty of non-appearing levels in the validation data: sapply(training_validation_splits$validation_data, function(df) {   table(df$species)   }) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"row data assigned one `1:n_folds` random. `` `1:n_folds`, `training_data[[]]` comprised data `sl_fold != `, .e., capturing roughly `(n-folds-1)/n_folds` proportion data.  validation data list dataframes, comprising roughly `1/n_folds` proportion data.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"","code":"cv_random_schema(data, n_folds = 5)"},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"data data.frame (similar) split training validation datasets. n_folds number `training_data` `validation_data` data frames make.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"named list two lists, list `n_folds` data frames.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"Since assignment folds random, proportions exact guaranteed variability size `training_data` data frame, likewise `validation_data` data frames.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists — cv_random_schema","text":"","code":"if (FALSE) { # \\dontrun{   data(Boston, package = 'MASS')   training_validation_data <- cv_random_schema(Boston, n_folds = 3)   # take a look at what's in the output:   str(training_validation_data, max.level = 2) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validating a `super_learner` — cv_super_learner","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"Produce cv-rmse `super_learner` specified closure accepts data returns `super_learner` prediction function.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"","code":"cv_super_learner(   data,   sl_closure,   y_variable,   n_folds = 5,   cv_schema = cv_random_schema,   loss_metric )"},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"data Data use training `super_learner`. sl_closure function takes data produces `super_learner` predictor. y_variable string name outcome column `data` n_folds number cross-validation folds use constructing `super_learner`. cv_schema function takes `data`, `n_folds` returns list containing `training_data` `validation_data`, lists `n_folds` data frames.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-Validating a `super_learner` — cv_super_learner","text":"idea `cv_super_learner` splits data training/validation splits, trains `super_learner` training split, evaluates predictions held-validation data, calculating root-mean-squared-error held-data.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","title":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","text":"function accepts dataframe structured one column `Y` columns unique names corresponding different model predictions `Y`, use nonnegative least squares determine weights use SuperLearner.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","text":"","code":"determine_super_learner_weights_nnls(data, yvar)"},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","text":"data data frame consisting outcome (y_variable) columns corresponding predictions candidate learners. yvar string name outcome column `data`.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine SuperLearner Weights with Nonnegative Least Squares — determine_super_learner_weights_nnls","text":"vector weights used learners.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_lik","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_lik","text":"Determine Weights Density Estimators SuperLearner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_lik","text":"","code":"determine_weights_using_neg_log_lik(data, y_variable)"},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine Weights for Density Estimators for SuperLearner — determine_weights_using_neg_log_lik","text":"data data.frame columns corresponding predicted densities learner true y_variable held-data y_variable character indicating outcome variable data.frame.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/extract_y_variable.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Y Variable from a list of Regression Formulas and Learners — extract_y_variable","title":"Extract Y Variable from a list of Regression Formulas and Learners — extract_y_variable","text":"Extract Y Variable list Regression Formulas Learners","code":""},{"path":"https://ctesta01.github.io/nadir/reference/extract_y_variable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Y Variable from a list of Regression Formulas and Learners — extract_y_variable","text":"","code":"extract_y_variable(   regression_formulas,   learner_names,   data_colnames,   y_variable )"},{"path":"https://ctesta01.github.io/nadir/reference/extract_y_variable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Y Variable from a list of Regression Formulas and Learners — extract_y_variable","text":"regression_formulas vector formulas used super learning learner_names character vector names learners data_colnames column names dataset super learning y_variable (Optional) y_variable specified user","code":""},{"path":"https://ctesta01.github.io/nadir/reference/hello.html","id":null,"dir":"Reference","previous_headings":"","what":"Hello, World! — hello","title":"Hello, World! — hello","text":"Prints 'Hello, world!'.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/hello.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hello, World! — hello","text":"","code":"hello()"},{"path":"https://ctesta01.github.io/nadir/reference/hello.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hello, World! — hello","text":"","code":"hello() #> Error in hello(): could not find function \"hello\""},{"path":"https://ctesta01.github.io/nadir/reference/learners.html","id":null,"dir":"Reference","previous_headings":"","what":"Learners in the {nadir} Package — learners","title":"Learners in the {nadir} Package — learners","text":"following learners available:","code":""},{"path":"https://ctesta01.github.io/nadir/reference/learners.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Learners in the {nadir} Package — learners","text":"lnr_mean lnr_gam lnr_glm lnr_glmer lnr_glmnet lnr_lm lnr_lmer lnr_ranger lnr_rf lnr_xgboost lnr_mean generally provided benchmarking purposes compare learners ensure correct specification learners, since prediction algorithm (theory) -perform just using mean outcome predictions. like build new learner, recommend reading source code several learners provided {nadir} get sense specified. learner, {nadir} understands , function takes `data`, `regression_formula`, possibly `...`, returns function predicts input `newdata`. simple example reproduced ease reference:","code":""},{"path":"https://ctesta01.github.io/nadir/reference/learners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Learners in the {nadir} Package — learners","text":"","code":"if (FALSE) { # \\dontrun{  lnr_glm <- function(data, regression_formula, ...) {   model <- stats::glm(formula = regression_formula, data = data, ...)    return(function(newdata) {     predict(model, newdata = newdata, type = 'response')   })   } } # }"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_heteroskedastic_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Density Estimation with Heteroskedasticity — lnr_heteroskedastic_density","title":"Conditional Density Estimation with Heteroskedasticity — lnr_heteroskedastic_density","text":"Conditional Density Estimation Heteroskedasticity","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_heteroskedastic_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Density Estimation with Heteroskedasticity — lnr_heteroskedastic_density","text":"","code":"lnr_heteroskedastic_density(   data,   regression_formula,   mean_lnr,   var_lnr,   mean_lnr_extra_arguments = NULL,   var_lnr_extra_arguments = NULL )"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"function accepting mean_lnr, trains data formula given. stats::density fit error (difference observed outcome mean_lnr predictions).","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"","code":"lnr_homoskedastic_density(   data,   regression_formula,   mean_lnr,   extra_mean_lnr_args = NULL,   extra_density_args = NULL )"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"mean_lnr suitable learner (see ?learners) can take data regression_formula given.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"predictor function takes newdata produces density estimates","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"returns function takes newdata produces density estimates according estimated stats::density fit error newdata observed outcome prediction mean_lnr. say, follows following procedure: $$\\text{obtain } \\hat{\\mathbb E}(Y | X) \\quad \\mathtt{using \\;\\; mean\\_learner}$$ $$\\text{fit } \\hat{f} \\gets \\mathtt{density}(Y - \\hat{\\mathbb E}(Y | X))$$ $$\\mathtt{return \\quad  function(newdata) \\{ } \\hat{f}(\\mathtt{newdata\\$Y} -   \\hat{\\mathbb E}[Y | \\mathtt{newdata\\$X}]) \\} $$","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Density Estimation with Homoskedasticity Assumption — lnr_homoskedastic_density","text":"","code":"if (FALSE) { # \\dontrun{ # fit a conditional density model with mean model as a randomForest fit_density_lnr <- lnr_homoskedastic_density(   data = mtcars,   regression_formula = mpg ~ hp,   mean_lnr = lnr_rf)  # and what we should get back should be predicted densities at the # observed mpg given the covariates hp fit_density_lnr(mtcars) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Normal Density Estimation Given Mean Predictors — lnr_lm_density","title":"Conditional Normal Density Estimation Given Mean Predictors — lnr_lm_density","text":"simplest possible density estimator entertainable.  fits lm model data, uses variance residuals parameterize model data \\(\\mathcal N(y | \\beta x, \\sigma^2)\\).","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Normal Density Estimation Given Mean Predictors — lnr_lm_density","text":"","code":"lnr_lm_density(data, regression_formula, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Normal Density Estimation Given Mean Predictors — lnr_lm_density","text":"closure (function) produces density estimates newdata given according fit model.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Negative Log Likelihood Loss — negative_log_lik_loss","title":"Negative Log Likelihood Loss — negative_log_lik_loss","text":"Negative Log Likelihood Loss","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Negative Log Likelihood Loss — negative_log_lik_loss","text":"","code":"negative_log_lik_loss(predicted_densities, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Negative Log Likelihood Loss — negative_log_lik_loss","text":"predicted_densities","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Negative Log Likelihood Loss — negative_log_lik_loss","text":"negative_log_lik_loss encodes logic: \\(\\hat p_n\\) good model conditional densities, minimize: $$ -\\sum(\\log(\\hat p_n(X_i)) $$","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_extra_learner_arguments.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Extra Arguments — parse_extra_learner_arguments","title":"Parse Extra Arguments — parse_extra_learner_arguments","text":"Parse Extra Arguments","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_extra_learner_arguments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Extra Arguments — parse_extra_learner_arguments","text":"","code":"parse_extra_learner_arguments(extra_learner_args, learner_names)"},{"path":"https://ctesta01.github.io/nadir/reference/parse_formulas.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Formulas for Super Learner — parse_formulas","title":"Parse Formulas for Super Learner — parse_formulas","text":"Parse Formulas Super Learner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_formulas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Formulas for Super Learner — parse_formulas","text":"","code":"parse_formulas(regression_formulas, learner_names)"},{"path":"https://ctesta01.github.io/nadir/reference/parse_formulas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Formulas for Super Learner — parse_formulas","text":"regression_formulas Formulas passed learner super learner learner_names names learners passed super learner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Softmax — softmax","title":"Softmax — softmax","text":"common transformation used go collection numbers R numbers [0,1] sum 1.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softmax — softmax","text":"","code":"softmax(beta)"},{"path":"https://ctesta01.github.io/nadir/reference/softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softmax — softmax","text":"beta vector numeric values transform","code":""},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","title":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","text":"Super learning functional programming!","code":""},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","text":"","code":"super_learner(   data,   learners,   regression_formulas,   y_variable,   n_folds = 5,   determine_super_learner_weights = determine_super_learner_weights_nnls,   continuous_or_discrete = \"continuous\",   cv_schema = cv_random_schema,   extra_learner_args = NULL,   verbose_output = FALSE )"},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","text":"data Data use training `super_learner`. learners list predictor/closure-returning-functions. See Details. regression_formulas Either single regression formula vector regression formulas. y_variable Typically `y_variable` can inferred automatically `regression_formulas`, needed, y_variable can specified explicitly. n_folds number cross-validation folds use constructing `super_learner`. determine_super_learner_weights function/method determine weights candidate `learners`. default use `determine_super_learner_weights_nnls`. continuous_or_discrete Defaults `'continuous'`, can set `'discrete'`. cv_schema function takes `data`, `n_folds` returns list containing `training_data` `validation_data`, lists `n_folds` data frames. extra_learner_args list equal length `learners` additional arguments pass specified learners. verbose_output `verbose_output = TRUE` return list containing fit learners predictions held-data well prediction function closure trained `super_learner`.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Super Learner: Cross-Validation Based Ensemble Learning — super_learner","text":"goal super learner use cross-validation set candidate learners 1) evaluate learners perform held data 2) use evaluation produce weighted average (continuous super learner) pick best learner (discrete super learner) specified candidate learners. Super learner statistically desirable properties written length, including least following references: * <https://biostats.bepress.com/ucbbiostat/paper222/>   * <https://www.stat.berkeley.edu/users/laan/Class/Class_subpages/BASS_sec1_3.1.pdf> `nadir::super_learner` adopts several user-interface design-perspectives useful know understanding works: * specification learners _very flexible_, really   constrained fact candidate learners designed   prediction problem details can wildly vary   learner learner.   * easy specify customized new learner. `nadir::super_learner` core accepts `data`, `regression_formula` (single one passed `regression_formulas` fine), list `learners`. `learners` taken lists functions following specification: * learner must accept `data` `regression_formula` argument,   * learner may accept arguments,   * learner must return prediction function accepts `newdata` produces vector prediction values given `newdata`. essence, learner specified function taking (`data`, `regression_formula`, ...) returning _closure_ (see <http://adv-r..co.nz/Functional-programming.html#closures> introduction closures) function accepting `newdata` returning predictions. Since many candidate learners hyperparameters tuned, like depth trees random forests, `lambda` parameter `glmnet`, extra arguments can passed learner via `extra_learner_args` argument. `extra_learner_args` list lists, one list extra arguments learner. additional arguments needed learners, learners using require additional arguments, can just put `NULL` value `extra_learner_args`. See examples. order seamlessly support using features implemented extensions formula syntax (like random effects formatted like random intercepts slopes use `(age | strata)` syntax `lme4` splines like `s(age | strata)` `mgcv`), allow `regression_formulas` argument either one fixed formula `super_learner` use models, vector formulas, one learner specified. Note examples mean-squared-error (mse) calculated training/test set, useful crude diagnostic see super_learner working. rigorous performance metric evaluate `super_learner` cv-rmse produced cv_super_learner.","code":""},{"path":[]}]
