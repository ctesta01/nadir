[{"path":"https://ctesta01.github.io/nadir/articles/Density-Estimation.html","id":"lets-validate-that-conditional-density-works-the-way-it-should","dir":"Articles","previous_headings":"","what":"Let‚Äôs validate that conditional density works the way it should","title":"Density Estimation","text":"","code":"lm_density_predict <- lnr_lm_homoskedastic_density(Boston, reg_formula)  f_lm <- function(ys) {   x <- Boston[1,]   sapply(ys, function(y) {     x[['crim']] <- y     lm_density_predict(x)   }) }  integrate(f_lm, min(Boston$crim) - sd(Boston$crim), max(Boston$crim) + sd(Boston$crim), subdivisions = 10000) #> 0.9604284 with absolute error < 1.9e-05   earth_density_predict <- lnr_earth_homoskedastic_density(Boston, reg_formula, density_args = list(bw = 30))  f_earth <- function(ys) {   x <- Boston[1,]   sapply(ys, function(y) {     x[['crim']] <- y     earth_density_predict(x)   }) } earth_density_predict(Boston[1,]) #> [1] 0.01314322  integrate(f_earth, min(Boston$crim) - 10*sd(Boston$crim), max(Boston$crim) + 10*sd(Boston$crim)) #> 0.9985732 with absolute error < 9.1e-05  y_seq <- seq(min(Boston$crim) - 10*sd(Boston$crim), max(Boston$crim) + 10*sd(Boston$crim), length.out = 10000) # f_earth(y_seq)  delta_y <- y_seq[2]-y_seq[1]  sum(f_earth(y_seq)*delta_y) #> [1] 0.9985829"},{"path":"https://ctesta01.github.io/nadir/articles/Density-Estimation.html","id":"heteroskedastic-learners","dir":"Articles","previous_headings":"","what":"Heteroskedastic Learners","title":"Density Estimation","text":"indicates now, lnr_heteroskedastic_density() method working correctly.","code":"lnr_earth_mean_glm_var_heteroskedastic_density <- function(data, formula, ...) {    lnr_heteroskedastic_density(data, formula, mean_lnr = lnr_earth,                              var_lnr = lnr_glm,                              var_lnr_args = list(family = gaussian(link = 'log')),                             ...) }  earth_mean_glm_var_heteroskedastic_predict <- lnr_earth_mean_glm_var_heteroskedastic_density(Boston, reg_formula)  earth_mean_glm_var_heteroskedastic_predict(Boston[1,]) #>            1  #> 1.432048e-56  #> attr(,\"non-estim\") #> 1  #> 1  f_earth_glm <- function(ys) {   x <- Boston[1,]   sapply(ys, function(y) {     x[['crim']] <- y     earth_mean_glm_var_heteroskedastic_predict(x)   }) }  integrate(f_earth_glm, min(Boston$crim) - 10*sd(Boston$crim), max(Boston$crim) + 10*sd(Boston$crim), subdivisions = 1000) #> 3.737656e-54 with absolute error < 4.1e-68  sum(f_earth_glm(y_seq)*delta_y) #> [1] 3.73803e-54 lnr_earth_glm_heteroskedastic_density <- function(data, formula, ...) {    lnr_heteroskedastic_density(data, formula, mean_lnr = lnr_earth,                              var_lnr = lnr_mean,                              density_args = list(bw = 3),                             ...) }  lnr_earth_glm_heteroskedastic_predict <- lnr_earth_glm_heteroskedastic_density(Boston, reg_formula)  lnr_earth_glm_heteroskedastic_predict(Boston[1,]) #> [1] 0.02128947  f_earth_mean <- function(ys) {   x <- Boston[1,]   sapply(ys, function(y) {     x[['crim']] <- y     lnr_earth_glm_heteroskedastic_predict(x)   }) }  # integrate(f_earth, min(Boston$crim) - sd(Boston$crim), max(Boston$crim) + sd(Boston$crim), subdivisions = )  sum(f_earth_mean(y_seq)*delta_y) #> [1] 0.9912578"},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"i-want-to-use-super_learner-for-binary-outcomes-","dir":"Articles","previous_headings":"","what":"I want to use super_learner() for binary outcomes.","title":"FAQs","text":"‚Äôll use Boston dataset create binary outcome regression problem. course, ‚Äôll train super_learner() binary outcomes. handle binary outcomes, need adjust method determining weights. don‚Äôt want use default mse() loss function, instead rely using negative log likelihood loss held-data. appropriately context binary data, use determine_weights_for_binary_outcomes() function provided nadir.","code":"data('Boston', package = 'MASS')  # create a binary outcome to predict Boston$high_crime <- as.integer(Boston$crim > mean(Boston$crim)) data <- Boston |> dplyr::select(-crim)  # train a super learner on a binary outcome trained_binary_super_learner <- super_learner(   data = data,   formula = high_crime ~ .,   learners = list(     logistic = lnr_glm, # for a logistic model, use glm + extra family arguments below     rf = lnr_rf,  # random forest     lm = lnr_lm), # linear probability model   extra_learner_args = list(     logistic = list(family = 'binomial')   ),   determine_super_learner_weights = nadir::determine_weights_for_binary_outcomes,   verbose = TRUE )  # let's take a look at the learned weights trained_binary_super_learner$learner_weights #>     logistic           rf           lm  #> 1.000000e+00 2.691182e-16 4.748858e-20  # what are the predictions? you can think of them as \\hat{P}(Y = 1 | X). # i.e., predictions of P(Y = 1) given X where Y and X are the left & right hand # side of your regression formula(s) head(trained_binary_super_learner$sl_predict(data)) #>            1            2            3            4            5            6  #> 2.220369e-16 2.220386e-16 2.220375e-16 2.220392e-16 2.220395e-16 2.220400e-16  rm(data) # cleanup"},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"i-want-to-use-super_learner-for-count-or-nonnegative-outcomes-","dir":"Articles","previous_headings":"","what":"I want to use super_learner() for count or nonnegative outcomes.","title":"FAQs","text":"principle, can use super_learner() whatever type outcomes want long things hold: learners pass super_learner() predict type outcome. loss function used inside determine_super_learner_weights() function argument consistent loss function used type data. loss function ‚Äú‚Äù used depends context, using mean-squared-error loss continuous outcomes negative log likelihood loss binary outcomes conditional density models written far. Refer source R/determine_weights.R. using nadir::super_learner() applications context Targeted Learning, may useful understand better arguments ofUnified Cross-Validation Methodology Selection Among Estimators General Cross-Validated Adaptive Epsilon-Net Estimator: Finite Sample Oracle Inequalities Examples Mark van der Laan Sandrine Dudoit, 2003 first understand appropriate loss function chosen depending outcome type. understanding people used Poisson distribution motivated loss functions (https://discuss.pytorch.org/t/poisson-loss-function/44301/6, https://pytorch.org/docs/stable/generated/torch.nn.PoissonNLLLoss.html) need think right thing general count outcome data.","code":""},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"what-are-the-limitations-of-nadirsuper_learner","dir":"Articles","previous_headings":"","what":"What are the limitations of nadir::super_learner()?","title":"FAQs","text":"key limitations design. say, want peek beta coefficients fit statistics learner, supported nadir::super_learner() design. reasoning explicit goal nadir keep learner objects lightweight building super_learner() can fast. explicit subpoint call attention , means far, work put supporting survival type outcomes. far, everything nadir assumes completeness (missingness) data.","code":""},{"path":"https://ctesta01.github.io/nadir/articles/FAQs.html","id":"what-if-the-learner-that-i-want-to-write-really-isnt-formula-based","dir":"Articles","previous_headings":"","what":"What if the learner that I want to write really isn‚Äôt formula based?","title":"FAQs","text":"solution case --less ditch formula piece learner entirely, just treating unused argument, custom needs can always build learners encode details structure data. can see immediately prior code snippet, niche application like avoid using formulas argument nadir::super_learner() , can taking advantage know ‚Äôre going structure data argument.","code":"data <- matrix(data = rnorm(n = 200), nrow = 20) colnames(data) <- paste0(\"X\", 1:10) data <- cbind(data, data %*% rnorm(10)) colnames(data)[ncol(data)] <- 'Y'  lnr_nonformula1 <- function(data, formula, ...) {      # notice by way of knowing things about our data structure, we never reference   # the formula;  so if you truly don't want to use it, you don't have to.      # as an example, here we do OLS assuming inputs are numeric matrices ‚Äî    # this might even be computationally more performant given how much extra   # stuff is inside an lm or glm fit.   X <- as.matrix(data[,grepl(pattern = \"^X\", colnames(data))])   Y <- as.matrix(data[,'Y'])   model_betas <- solve(t(X) %*% X) %*% t(X) %*% Y      learned_predictor <- function(newdata) {     if ('Y' %in% colnames(newdata)) {       index_of_y <- which(colnames(newdata) == 'Y')[[1]]     } else {       index_of_y <- NULL     }     if (is.data.frame(newdata)) {       newdata <- as.matrix(newdata)     }     as.vector(t(model_betas) %*% t(newdata[,-index_of_y, drop=FALSE]))   }   return(learned_predictor) }  # this is essentially a re-implementation of lnr_mean with no reference to the formula  lnr_nonformula2 <- function(data, formula, ...) {      Y <- data[,'Y']   Y_mean <- mean(Y)      learned_predictor <- function(newdata) {     rep(Y_mean, nrow(newdata))   }   return(learned_predictor) }  learned_super_learner <- super_learner(   data = data,   learners = list(     nonformula1 = lnr_nonformula1,     nonformula2 = lnr_nonformula2),   formulas = . ~ ., # it doesn't matter what we put here, because neither    # learner uses their formula inputs.    y_variable = 'Y',   verbose = TRUE   )  # observe that the OLS model gets all the weight because it's the correct model: round(learned_super_learner$learner_weights, 10)  #> nonformula1 nonformula2  #>           1           0   rm(data) # cleanup"},{"path":"https://ctesta01.github.io/nadir/articles/Guidance-for-Developers.html","id":"on-the-topic-of-pkgdown","dir":"Articles","previous_headings":"","what":"On the topic of {pkgdown}","title":"Guidance for Developers","text":"experience, started running issues compiling pkgdown site, (errors saying .Rd files parsed), needed update dependencies pkgdown. ran install.packages(\"pkgdown\", dependencies = TRUE) fixed issues running towards start pkgdown setup.. think debug rendering pkgdown website, also ended using advice : https://stackoverflow.com/questions/66806694/pkgdown-fails-parsing-rd-files--examples--added, namely make sure development version downlit installed. articles pkgdown website take time compile, find useful know can run sub-components pkgdown::build_site() individually. See: https://pkgdown.r-lib.org/reference/build_site.html order get pkgdown website render math properly documentation nadir::lnr_homoskedastic_density() follow advice https://github.com/r-lib/pkgdown/issues/2704 user @louisaslett said needed include _pkgdown.yml manually: now, images README.Rmd just embedded manually generated (Feb 27 2025) based lack support Rmd generated images getting copied site docs pkgdown discussed thread https://github.com/r-lib/pkgdown/issues/133 advice put images man/figures/ images can also rendered CRAN. tried following advice https://github.com/r-lib/pkgdown/issues/995 get articles better ordering, didn‚Äôt seem work .","code":"library(devtools) install_github('r-lib/downlit') template:   bootstrap: 5   includes:     in_header: |       <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css\" integrity=\"sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+\" crossorigin=\"anonymous\">       <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js\" integrity=\"sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg\" crossorigin=\"anonymous\"><\/script>       <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js\" integrity=\"sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk\" crossorigin=\"anonymous\" onload=\"renderMathInElement(document.body);\"><\/script> # library(nadir)"},{"path":"https://ctesta01.github.io/nadir/articles/guidance_for_developers.html","id":"on-the-topic-of-pkgdown","dir":"Articles","previous_headings":"","what":"On the topic of {pkgdown}","title":"Guidance for Developers","text":"experience, started running issues compiling pkgdown site, (errors saying .Rd files parsed), needed update dependencies pkgdown. ran install.packages(\"pkgdown\", dependencies = TRUE) fixed issues running towards start pkgdown setup.. think debug rendering pkgdown website, also ended using advice : https://stackoverflow.com/questions/66806694/pkgdown-fails-parsing-rd-files--examples--added, namely make sure development version downlit installed. articles pkgdown website take time compile, find useful know can run sub-components pkgdown::build_site() individually. See: https://pkgdown.r-lib.org/reference/build_site.html order get pkgdown website render math properly documentation nadir::lnr_homoskedastic_density() follow advice https://github.com/r-lib/pkgdown/issues/2704 user @louisaslett said needed include _pkgdown.yml manually: now, images README.Rmd just embedded manually generated (Feb 27 2025) based lack support Rmd generated images getting copied site docs pkgdown discussed thread https://github.com/r-lib/pkgdown/issues/133 advice put images man/figures/ images can also rendered CRAN. tried following advice https://github.com/r-lib/pkgdown/issues/995 get articles better ordering, didn‚Äôt seem work .","code":"library(devtools) install_github('r-lib/downlit') template:   bootstrap: 5   includes:     in_header: |       <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css\" integrity=\"sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+\" crossorigin=\"anonymous\">       <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js\" integrity=\"sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg\" crossorigin=\"anonymous\"><\/script>       <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js\" integrity=\"sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk\" crossorigin=\"anonymous\" onload=\"renderMathInElement(document.body);\"><\/script> # library(nadir)"},{"path":"https://ctesta01.github.io/nadir/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Christian Testa. Author, maintainer.","code":""},{"path":"https://ctesta01.github.io/nadir/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Testa C (2025). nadir: Super learning flexible formulas. https://ctesta01.github.io/nadir/, https://github.com/ctesta01/nadir/.","code":"@Manual{,   title = {nadir: Super learning with flexible formulas},   author = {Christian Testa},   year = {2025},   note = {https://ctesta01.github.io/nadir/, https://github.com/ctesta01/nadir/}, }"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"nadir-","dir":"","previous_headings":"","what":"Super learning with flexible formulas","title":"Super learning with flexible formulas","text":"nadir (noun): nƒÅ-dir lowest point. Fitting minimum loss based estimation12 literature, nadir implementation Super Learner algorithm improved support flexible formula based syntax fond functional programming techniques closures, currying, function factories. nadir implements Super Learner3 algorithm. quote Guide SuperLearner4: SuperLearner algorithm uses cross-validation estimate performance multiple machine learning models, model different settings. creates optimal weighted average models, aka ‚Äúensemble‚Äù, using test data performance. approach proven asymptotically accurate best possible prediction algorithm tested.","code":""},{"path":"https://ctesta01.github.io/nadir/index.html","id":"why-nadir-and-why-reimplement-super-learner-again","dir":"","previous_headings":"","what":"Why {nadir} and why reimplement Super Learner again?","title":"Super learning with flexible formulas","text":"previous implementations ({SuperLearner}, {sl3}, {mlr3superlearner}), support flexible formula-based syntax limited, instead opting specifying learners models XX matrix YY outcome vector. Many popular R packages lme4 mgcv (random effects generalized additive models) use formulas extensively specify models using syntax like (age | strata) specify random effects age strata, s(age, income) specify smoothing term age income simultaneously. present, difficult use kinds features SuperLearner, sl3 {ml3superlearner}. example, easy imagine Super Learner algorithm appealing modelers fond random effects based models may want hedge exact nature random effects models, sure random intercepts enough random slopes included, etc., similar modeling decisions frameworks. Therefore, nadir package takes charges : Implement syntax easy specify different formulas many candidate learners. make easy pass new learners Super Learner algorithm.","code":""},{"path":"https://ctesta01.github.io/nadir/index.html","id":"installation-instructions","dir":"","previous_headings":"","what":"Installation Instructions","title":"Super learning with flexible formulas","text":"present, nadir available GitHub. Warning: package currently active development may wrong! use serious applications message removed, likely time future release.","code":"devtools::install_github(\"ctesta01/nadir\")"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"demonstration","dir":"","previous_headings":"","what":"Demonstration","title":"Super learning with flexible formulas","text":"First, let‚Äôs start simplest possible use case nadir::super_learner(), user like feed data, specification regression formula(s), specify library learners, get back prediction function suitable plugging downstream analyses, like Targeted Learning pure-prediction applications. demo extremely simple application using nadir::super_learner:","code":"library(nadir)  # we'll use a few basic learners learners <- list(      glm = lnr_glm,      rf = lnr_rf,      glmnet = lnr_glmnet   ) # more learners are available, see ?learners  sl_model <- super_learner(   data = mtcars,   formula = mpg ~ cyl + hp,   learners = learners)  # the output from super_learner is a prediction function: # here we are producing predictions based on a weighted combination of the # trained learners.  sl_model(mtcars) |> head() ##         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive  ##          20.36228          20.36228          24.89787          20.36228  ## Hornet Sportabout           Valiant  ##          16.95507          19.93187"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"one-step-up-fancy-formula-features","dir":"","previous_headings":"","what":"One Step Up: Fancy Formula Features","title":"Super learning with flexible formulas","text":"Continuing mtcars example, suppose user really like use random effects similar types fancy formula language features. One easy way nadir::super_learner using following syntax:","code":"learners <- list(      glm = lnr_glm,      rf = lnr_rf,      glmnet = lnr_glmnet,      lmer = lnr_lmer,      gam = lnr_gam   )  formulas <- c(   .default = mpg ~ cyl + hp,   # our first three learners use same formula   lmer = mpg ~ (1 | cyl) + hp, # both lme4::lmer and mgcv::gam have    gam = mpg ~ s(hp) + cyl      # specialized formula syntax   )  # fit a super_learner sl_model <- super_learner(   data = mtcars,   formulas = formulas,   learners = learners)    sl_model(mtcars) |> head() ##         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive  ##          20.44673          20.44673          24.86184          20.44673  ## Hornet Sportabout           Valiant  ##          16.87359          20.08569"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"how-should-we-assess-performance-of-nadirsuper_learner","dir":"","previous_headings":"","what":"How should we assess performance of nadir::super_learner()?","title":"Super learning with flexible formulas","text":"put learners super learner algorithm level playing field, ‚Äôs important learners super learner evaluated held-validation/test data algorithms seen . Using verbose = TRUE output nadir::super_learner(), can call compare_learners() see mean-squared-error (MSE) held-data, also called CV-MSE, candidate learners specified.  Now go getting CV-MSE super learned model? curry super learner function takes data (additional specification built ) returns prediction function (.e., closure). Technical aside: ‚Äú‚Äù curry function? Well, perform cross-validation super_learner(), behind scenes, ‚Äôre going want split data training/validation sets apply super_learner() training sets, producing prediction-closure , can predict onto held-validation data. perspective, basically want function takes one input (training data) spits relevant prediction function (closure). Don‚Äôt let complicated language scare ; ‚Äôs fairly straightforward. Essentially just need wrap super learner specification inside sl_closure <- function(data) { ... }, make sure specify data = data inside inner super_learner() call, ‚Äôre done. return value function closure super_learner() returns already closure eats newdata returns predictions.","code":"# construct our super learner with verbose = TRUE sl_model <- super_learner(   data = mtcars,   formulas = formulas,   learners = learners,   verbose = TRUE)    compare_learners(sl_model) ## The default in nadir::compare_learners is to use CV-MSE for comparing learners.  ## Other metrics can be set using the loss_metric argument to compare_learners.  ## # A tibble: 1 √ó 5 ##     glm    rf glmnet  lmer   gam ##   <dbl> <dbl>  <dbl> <dbl> <dbl> ## 1  11.0  9.27   10.8  12.4  21.0 pacman::p_load('dplyr', 'ggplot2', 'tidyr', 'magrittr')  truth <- sl_model$holdout_predictions$mpg  holdout_var <- sl_model$holdout_predictions |>   dplyr::group_by(.sl_fold) |>    dplyr::summarize(across(everything(), ~ mean((. - mpg)^2))) |>    dplyr::summarize(across(everything(), var)) |>    select(-mpg, -.sl_fold) |>    t() |>    as.data.frame() |>    tibble::rownames_to_column('learner') |>    dplyr::rename(var = V1) |>   dplyr::mutate(sd = sqrt(var))   jitters <- sl_model$holdout_predictions |>    dplyr::mutate(dplyr::across(-.sl_fold, ~ (. - mpg)^2)) |>    dplyr::select(-mpg) %>%   tidyr::pivot_longer(cols = 2:ncol(.), names_to = 'learner', values_to = 'squared_error') |>   dplyr::group_by(learner, .sl_fold) |>    dplyr::summarize(mse = mean(squared_error)) |>    ungroup() |>    rename(fold = .sl_fold)  learner_comparison_df <- sl_model |>    compare_learners() |>    t() |>    as.data.frame() |>   tibble::rownames_to_column(var = 'learner') |>    dplyr::mutate(learner = factor(learner)) |>   dplyr::rename(mse = V1) |>   dplyr::left_join(holdout_var) |>    dplyr::mutate(     upper_ci = mse + sd,     lower_ci = mse - sd) |>    dplyr::mutate(learner = forcats::fct_reorder(learner, mse))  jitters$learner <- factor(jitters$learner, levels = levels(learner_comparison_df$learner))  learner_comparison_df |>    ggplot2::ggplot(ggplot2::aes(y = learner, x = mse, fill = learner)) +    ggplot2::geom_col(alpha = 0.5) +    ggplot2::geom_jitter(data = jitters, mapping = ggplot2::aes(x = mse), height = .15, shape = 'o') +    ggplot2::geom_pointrange(mapping = ggplot2::aes(xmax = upper_ci, xmin = lower_ci),                            alpha = 0.5) +    ggplot2::theme_bw() +    ggplot2::ggtitle(\"Comparison of Candidate Learners\") +    ggplot2::labs(caption = \"Error bars show ¬±1 standard deviation across the CV estimated MSE for each learner\\n Each open circle represents the CV-MSE on one held-out fold of the data\") +    ggplot2::theme(plot.caption.position = 'plot') sl_closure_mtcars <- function(data) {   nadir::super_learner(   data = data,   formulas = formulas,   learners = learners   ) }  cv_results <- cv_super_learner(data = mtcars, sl_closure_mtcars,                   y_variable = 'mpg',                  n_folds = 5)  cv_jitters <- cv_results$cv_trained_learners |>    dplyr::select(split, predictions, mpg) |>    tidyr::unnest(cols = c('predictions', 'mpg')) |>    dplyr::group_by(split) |>    dplyr::summarize(mse = mean((mpg - predictions)^2)) |>   dplyr::bind_cols(learner = 'super_learner')   cv_var <- cv_results$cv_trained_learners |>    dplyr::select(split, predictions, mpg) |>    tidyr::unnest(cols = c(predictions, mpg)) |>    dplyr::mutate(squared_error = (mpg - predictions)^2) |>    dplyr::group_by(split) |>    dplyr::summarize(mse = mean(squared_error)) |>    dplyr::summarize(     var = var(mse),     mse = mean(mse),     sd = sqrt(var),     upper_ci = mse + sd,     lower_ci = mse - sd) |>    dplyr::bind_cols(learner = 'super_learner')  new_jitters <- bind_rows(jitters, cv_jitters)  learner_comparison_df |>    bind_rows(cv_var) |>    dplyr::mutate(learner = forcats::fct_reorder(learner, mse)) |>    ggplot2::ggplot(ggplot2::aes(y = learner, x = mse, fill = learner)) +    ggplot2::geom_col(alpha = 0.5) +    ggplot2::geom_jitter(data = new_jitters, mapping = ggplot2::aes(x = mse), height = .15, shape = 'o') +    ggplot2::geom_pointrange(mapping = ggplot2::aes(xmax = upper_ci, xmin = lower_ci),                            alpha = 0.5) +    ggplot2::theme_bw() +    ggplot2::scale_fill_brewer(palette = 'Set2') +    ggplot2::ggtitle(\"Comparison of Candidate Learners against Super Learner\") +    ggplot2::labs(caption = \"Error bars show ¬±1 standard deviation across the CV estimated MSE for each learner\\n Each open circle represents the CV-MSE on one held-out fold of the data\") +    ggplot2::theme(plot.caption.position = 'plot') # iris example --- sl_model_iris <- super_learner(   data = iris,   formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,   learners = learners[1:3],   verbose = TRUE)    compare_learners(sl_model_iris) ## The default in nadir::compare_learners is to use CV-MSE for comparing learners.  ## Other metrics can be set using the loss_metric argument to compare_learners.  ## # A tibble: 1 √ó 3 ##     glm    rf glmnet ##   <dbl> <dbl>  <dbl> ## 1 0.101 0.131  0.208 sl_closure_iris <- function(data) {   nadir::super_learner(   data = data,   formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,   learners = learners[1:3]) }  cv_super_learner(data = iris, sl_closure_iris, y_variable = 'Sepal.Length')$cv_mse ## The default is to report CV-MSE if no other loss_metric is specified.  ## NULL"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"what-about-model-hyperparameters-or-extra-arguments","dir":"","previous_headings":"","what":"What about model hyperparameters or extra arguments?","title":"Super learning with flexible formulas","text":"Model hyperparameters easy handle nadir. Two easy solutions available users: nadir::super_learner() extra_learner_args parameter can passed list extra arguments learner. Users can always build new learners (allows building hyperparameter specification), using ... syntax, ‚Äôs easy build new learners learners already provided nadir. ‚Äôs examples showing approach.","code":""},{"path":"https://ctesta01.github.io/nadir/index.html","id":"using-extra_learner_args","dir":"","previous_headings":"What about model hyperparameters or extra arguments?","what":"Using extra_learner_args:","title":"Super learning with flexible formulas","text":"","code":"# when using extra_learner_args, it's totally okay to use the  # same learner multiple times as long as their hyperparameters differ.  sl_model <- nadir::super_learner(   data = mtcars,   formula = mpg ~ .,   learners = c(     glmnet0 = lnr_glmnet,     glmnet1 = lnr_glmnet,     glmnet2 = lnr_glmnet,     rf0 = lnr_rf,     rf1 = lnr_rf,     rf2 = lnr_rf     ),   extra_learner_args = list(     glmnet0 = list(lambda = 0.01),     glmnet1 = list(lambda = 0.1),     glmnet2 = list(lambda = 1),     rf0 = list(ntree = 30),     rf1 = list(ntree = 30),     rf2 = list(ntree = 30)     ),   verbose = TRUE )  compare_learners(sl_model) ## The default in nadir::compare_learners is to use CV-MSE for comparing learners.  ## Other metrics can be set using the loss_metric argument to compare_learners.  ## # A tibble: 1 √ó 6 ##   glmnet0 glmnet1 glmnet2   rf0   rf1   rf2 ##     <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> ## 1    18.3    11.8    9.66  7.14  6.24  6.37"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"building-new-learners-programmatically","dir":"","previous_headings":"What about model hyperparameters or extra arguments?","what":"Building New Learners Programmatically","title":"Super learning with flexible formulas","text":"make sense build new learners hyperparameters built rather using extra_learner_args parameter? One instance building new learners may make sense user like produce large number hyperparameterized learners programmatically, example grid hyperparameter values. show example 1-d grid hyperparameters glmnet.","code":"# produce a \"grid\" of glmnet learners with lambda set to  # exp(-1 to 1 in steps of .1) hyperparameterized_learners <- lapply(   exp(seq(-1, 1, by = .1)),    function(lambda) {      return(       function(data, formula, ...) {         lnr_glmnet(data, formula, lambda = lambda, ...)         })   })    # give them names because nadir::super_learner requires that the  # learners argument be named. names(hyperparameterized_learners) <- paste0('glmnet', 1:length(hyperparameterized_learners))  # fit the super_learner with 20 glmnets with different lambdas sl_model_glmnet <- nadir::super_learner(   data = mtcars,   learners = hyperparameterized_learners,   formula = mpg ~ .,   verbose = TRUE)  compare_learners(sl_model_glmnet) ## The default in nadir::compare_learners is to use CV-MSE for comparing learners.  ## Other metrics can be set using the loss_metric argument to compare_learners.  ## # A tibble: 1 √ó 21 ##   glmnet1 glmnet2 glmnet3 glmnet4 glmnet5 glmnet6 glmnet7 glmnet8 glmnet9 ##     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> ## 1    8.86    8.79    8.61    8.40    8.27    8.17    8.09    8.04    8.00 ## # ‚Ñπ 12 more variables: glmnet10 <dbl>, glmnet11 <dbl>, glmnet12 <dbl>, ## #   glmnet13 <dbl>, glmnet14 <dbl>, glmnet15 <dbl>, glmnet16 <dbl>, ## #   glmnet17 <dbl>, glmnet18 <dbl>, glmnet19 <dbl>, glmnet20 <dbl>, ## #   glmnet21 <dbl>"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"what-are-currying-closures-and-function-factories","dir":"","previous_headings":"","what":"What are currying, closures, and function factories?","title":"Super learning with flexible formulas","text":"R functional programming language, allows functions build return functions just like return object. refer functions create return another function function factory. extended reference, see Advanced R book. Function factories useful nadir , essence, candidate learner needs able 1) accept training data, 2) produce prediction function can make predictions heldout validation data. typical learner nadir looks like: Moreover, given code-lightweight write simple learner, makes relatively easy users write new learners meet exact needs. want implement learners, just need follow following pseudocode approach: details, read Currying, Closures, Function Factories article","code":"lnr_lm <- function(data, formula, ...) {   lnr_lm <- function(data, formula, ...) {   model <- stats::lm(formula = formula, data = data, ...)    predict_from_trained_lm <- function(newdata) {     predict(model, newdata = newdata, type = 'response')   }   return(predict_from_trained_lm) } lnr_custom <- function(data, formula, ...) {   model <- # train your model using data, formula, ...       predict_from_model <- function(newdata) {     return(...) # return predictions from the trained model      # (predictions should be a vector of predictions, one for each row of newdata)   }   return(predict_from_model) }"},{"path":"https://ctesta01.github.io/nadir/index.html","id":"coming-down-the-pipe-Ô∏è","dir":"","previous_headings":"","what":"Coming Down the Pipe ‚Ü©Ô∏éÔ∏èüö∞üîß‚ú®","title":"Super learning with flexible formulas","text":"() Automated tests try ensure validity/correctness implementation! Reworking internals use future future.apply origami Performance benchmarking (hopefully leading carefully considered improvements speed) vignettes/articles soon. Better explicit support binary outcomes density estimation.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare Learners ‚Äî compare_learners","title":"Compare Learners ‚Äî compare_learners","text":"Compare learners using specified loss_metric","code":""},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare Learners ‚Äî compare_learners","text":"","code":"compare_learners(sl_output, y_variable, loss_metric)"},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare Learners ‚Äî compare_learners","text":"sl_output Output nadir::super_learner() verbose = TRUE","code":""},{"path":"https://ctesta01.github.io/nadir/reference/compare_learners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare Learners ‚Äî compare_learners","text":"","code":"if (FALSE) { # \\dontrun{ sl_model <- super_learner(   data = mtcars,   learners = list(lm = lnr_lm, rf = lnr_randomForest, mean = lnr_mean),   formula = mpg ~ .,   verbose = TRUE)  compare_learners(sl_model) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross Validation Training/Validation Splits with Characters/Factor Columns ‚Äî cv_character_and_factors_schema","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns ‚Äî cv_character_and_factors_schema","text":"Designed handle cross-validation models like randomForest, ranger, glmnet, etc., model matrix newdata must match eactly model matrix training dataset, function intends answer need \"training datasets need every level every discrete-type column appears data.\"","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns ‚Äî cv_character_and_factors_schema","text":"","code":"cv_character_and_factors_schema(   data,   n_folds = 5,   cv_sl_mode = TRUE,   check_validation_datasets_too = TRUE )"},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns ‚Äî cv_character_and_factors_schema","text":"data Data use training `super_learner`. n_folds number cross-validation folds use constructing `super_learner`. cv_sl_mode binary (default: TRUE) indicator output training/validation data lists used inside another `super_learner` call. , training data needs every level appear least twice data can put training/validation splits. check_validation_datasets_too Enforce validation datasets produced also every level every character / factor type column present. particularly useful learners like `glmnet` require `newx` exact shape/structure training data, binary indicators every level appears.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns ‚Äî cv_character_and_factors_schema","text":"fundamental idea check unique levels character /factor columns represented every training dataset. beyond , function designed support cv_super_learner, inherently involves two layers cross-validation.  result, stringent conditions specified `cv_sl_mode` enabled.  convenience mode enabled default","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_character_and_factors_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross Validation Training/Validation Splits with Characters/Factor Columns ‚Äî cv_character_and_factors_schema","text":"","code":"if (FALSE) { # \\dontrun{ require(palmerpenguins) training_validation_splits <- cv_character_and_factors_schema(   palmerpenguins::penguins)  # we can see the population breakdown across all the training # splits: sapply(training_validation_splits$training_data, function(df) {   table(df$species)   }) # notably, none of them are empty! this is crucial for certain # types of learning algorithms that must see all levels appear in the # training data, like random forests.  # certain models like glmnet require that the prediction dataset # newx have the _exact_ same shape as the training data, so it # can be important that every level appears in the validation data # as well.  check that by looking into these types of tables: sapply(training_validation_splits$validation_data, function(df) {   table(df$species)   })  # if you don't need this level of stringency, but you just want # to make cv_splits where every level appears in the training_data, # you can do so using the check_validation_datasets_too = FALSE # argument. penguins_small <- palmerpenguins::penguins[c(1:3, 154:156, 277:279), ] penguins_small <- penguins_small[complete.cases(penguins_small),]  training_validation_splits <- cv_character_and_factors_schema(   penguins_small,   cv_sl_mode = FALSE,   n_folds = 5,   check_validation_datasets_too = FALSE)  sapply(training_validation_splits$training_data, function(df) {   table(df$species)   })  # now you can see plenty of non-appearing levels in the validation data: sapply(training_validation_splits$validation_data, function(df) {   table(df$species)   }) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists ‚Äî cv_random_schema","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists ‚Äî cv_random_schema","text":"row data assigned one `1:n_folds` random. `` `1:n_folds`, `training_data[[]]` comprised data `sl_fold != `, .e., capturing roughly `(n-folds-1)/n_folds` proportion data.  validation data list dataframes, comprising roughly `1/n_folds` proportion data.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists ‚Äî cv_random_schema","text":"","code":"cv_random_schema(data, n_folds = 5)"},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists ‚Äî cv_random_schema","text":"data data.frame (similar) split training validation datasets. n_folds number `training_data` `validation_data` data frames make.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists ‚Äî cv_random_schema","text":"named list two lists, list `n_folds` data frames.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists ‚Äî cv_random_schema","text":"Since assignment folds random, proportions exact guaranteed variability size `training_data` data frame, likewise `validation_data` data frames.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_random_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign Data to One of n_folds Randomly and Produce Training/Validation Data Lists ‚Äî cv_random_schema","text":"","code":"if (FALSE) { # \\dontrun{   data(Boston, package = 'MASS')   training_validation_data <- cv_random_schema(Boston, n_folds = 3)   # take a look at what's in the output:   str(training_validation_data, max.level = 2) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validating a `super_learner` ‚Äî cv_super_learner","title":"Cross-Validating a `super_learner` ‚Äî cv_super_learner","text":"Produce cv-rmse `super_learner` specified closure accepts data returns `super_learner` prediction function.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validating a `super_learner` ‚Äî cv_super_learner","text":"","code":"cv_super_learner(   data,   sl_closure,   y_variable,   n_folds = 5,   cv_schema = cv_random_schema,   loss_metric )"},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validating a `super_learner` ‚Äî cv_super_learner","text":"data Data use training `super_learner`. sl_closure function takes data produces `super_learner` predictor. y_variable string name outcome column `data` n_folds number cross-validation folds use constructing `super_learner`. cv_schema function takes `data`, `n_folds` returns list containing `training_data` `validation_data`, lists `n_folds` data frames.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/cv_super_learner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-Validating a `super_learner` ‚Äî cv_super_learner","text":"idea `cv_super_learner` splits data training/validation splits, trains `super_learner` training split, evaluates predictions held-validation data, calculating root-mean-squared-error held-data.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine SuperLearner Weights with Nonnegative Least Squares ‚Äî determine_super_learner_weights_nnls","title":"Determine SuperLearner Weights with Nonnegative Least Squares ‚Äî determine_super_learner_weights_nnls","text":"function accepts dataframe structured one column `Y` columns unique names corresponding different model predictions `Y`, use nonnegative least squares determine weights use SuperLearner.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine SuperLearner Weights with Nonnegative Least Squares ‚Äî determine_super_learner_weights_nnls","text":"","code":"determine_super_learner_weights_nnls(data, yvar)"},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine SuperLearner Weights with Nonnegative Least Squares ‚Äî determine_super_learner_weights_nnls","text":"data data frame consisting outcome (y_variable) columns corresponding predictions candidate learners. yvar string name outcome column `data`.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_super_learner_weights_nnls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine SuperLearner Weights with Nonnegative Least Squares ‚Äî determine_super_learner_weights_nnls","text":"vector weights used learners.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_for_binary_outcomes.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine Weights Appropriately for Super Learner given Binary Outcomes ‚Äî determine_weights_for_binary_outcomes","title":"Determine Weights Appropriately for Super Learner given Binary Outcomes ‚Äî determine_weights_for_binary_outcomes","text":"Determine Weights Appropriately Super Learner given Binary Outcomes","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_for_binary_outcomes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine Weights Appropriately for Super Learner given Binary Outcomes ‚Äî determine_weights_for_binary_outcomes","text":"","code":"determine_weights_for_binary_outcomes(data, y_variable)"},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine Weights for Density Estimators for SuperLearner ‚Äî determine_weights_using_neg_log_lik","title":"Determine Weights for Density Estimators for SuperLearner ‚Äî determine_weights_using_neg_log_lik","text":"Determine Weights Density Estimators SuperLearner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine Weights for Density Estimators for SuperLearner ‚Äî determine_weights_using_neg_log_lik","text":"","code":"determine_weights_using_neg_log_lik(data, y_variable)"},{"path":"https://ctesta01.github.io/nadir/reference/determine_weights_using_neg_log_lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine Weights for Density Estimators for SuperLearner ‚Äî determine_weights_using_neg_log_lik","text":"data data.frame columns corresponding predicted densities learner true y_variable held-data y_variable character indicating outcome variable data.frame.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/extract_y_variable.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Y Variable from a list of Regression Formulas and Learners ‚Äî extract_y_variable","title":"Extract Y Variable from a list of Regression Formulas and Learners ‚Äî extract_y_variable","text":"Extract Y Variable list Regression Formulas Learners","code":""},{"path":"https://ctesta01.github.io/nadir/reference/extract_y_variable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Y Variable from a list of Regression Formulas and Learners ‚Äî extract_y_variable","text":"","code":"extract_y_variable(formulas, learner_names, data_colnames, y_variable)"},{"path":"https://ctesta01.github.io/nadir/reference/extract_y_variable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Y Variable from a list of Regression Formulas and Learners ‚Äî extract_y_variable","text":"formulas vector formulas used super learning learner_names character vector names learners data_colnames column names dataset super learning y_variable (Optional) y_variable specified user","code":""},{"path":"https://ctesta01.github.io/nadir/reference/hello.html","id":null,"dir":"Reference","previous_headings":"","what":"Hello, World! ‚Äî hello","title":"Hello, World! ‚Äî hello","text":"Prints 'Hello, world!'.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/hello.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hello, World! ‚Äî hello","text":"","code":"hello()"},{"path":"https://ctesta01.github.io/nadir/reference/hello.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hello, World! ‚Äî hello","text":"","code":"hello() #> Error in hello(): could not find function \"hello\""},{"path":"https://ctesta01.github.io/nadir/reference/learners.html","id":null,"dir":"Reference","previous_headings":"","what":"Learners in the {nadir} Package ‚Äî learners","title":"Learners in the {nadir} Package ‚Äî learners","text":"following learners available:","code":""},{"path":"https://ctesta01.github.io/nadir/reference/learners.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Learners in the {nadir} Package ‚Äî learners","text":"lnr_mean lnr_gam lnr_glm lnr_glmer lnr_glmnet lnr_lm lnr_lmer lnr_ranger lnr_rf lnr_xgboost lnr_mean generally provided benchmarking purposes compare learners ensure correct specification learners, since prediction algorithm (theory) -perform just using mean outcome predictions. like build new learner, recommend reading source code several learners provided {nadir} get sense specified. learner, {nadir} understands , function takes `data`, `formula`, possibly `...`, returns function predicts input `newdata`. simple example reproduced ease reference:","code":""},{"path":"https://ctesta01.github.io/nadir/reference/learners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Learners in the {nadir} Package ‚Äî learners","text":"","code":"if (FALSE) { # \\dontrun{  lnr_glm <- function(data, formula, ...) {   model <- stats::glm(formula = formula, data = data, ...)    return(function(newdata) {     predict(model, newdata = newdata, type = 'response')   })   } } # }"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_heteroskedastic_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Density Estimation with Heteroskedasticity ‚Äî lnr_heteroskedastic_density","title":"Conditional Density Estimation with Heteroskedasticity ‚Äî lnr_heteroskedastic_density","text":"TODO: following code bug / statistical issue. =======================================================","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_heteroskedastic_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Density Estimation with Heteroskedasticity ‚Äî lnr_heteroskedastic_density","text":"","code":"lnr_heteroskedastic_density(   data,   formula,   mean_lnr,   var_lnr,   mean_lnr_args = NULL,   var_lnr_args = NULL,   density_args = NULL )"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_heteroskedastic_density.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Density Estimation with Heteroskedasticity ‚Äî lnr_heteroskedastic_density","text":"think bugs performing basic test fix conditioning set (X) integrate, integrating conditional probability density X fixed yield 1. numerical tests, variance scaled , integrating conditional densities seems yield integration values exceeding 1 (sometimes lot). pretty sure poses problem optimizing negative log likelihood loss. Said numerical tests displayed `Density-Estimation` article.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Density Estimation with Homoskedasticity Assumption ‚Äî lnr_homoskedastic_density","title":"Conditional Density Estimation with Homoskedasticity Assumption ‚Äî lnr_homoskedastic_density","text":"function accepting mean_lnr, trains data formula given. stats::density fit error (difference observed outcome mean_lnr predictions).","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Density Estimation with Homoskedasticity Assumption ‚Äî lnr_homoskedastic_density","text":"","code":"lnr_homoskedastic_density(   data,   formula,   mean_lnr,   mean_lnr_args = NULL,   density_args = NULL )"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Density Estimation with Homoskedasticity Assumption ‚Äî lnr_homoskedastic_density","text":"mean_lnr suitable learner (see ?learners) can take data formula given.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Density Estimation with Homoskedasticity Assumption ‚Äî lnr_homoskedastic_density","text":"predictor function takes newdata produces density estimates","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Density Estimation with Homoskedasticity Assumption ‚Äî lnr_homoskedastic_density","text":"returns function takes newdata produces density estimates according estimated stats::density fit error newdata observed outcome prediction mean_lnr. say, follows following procedure (assuming \\(Y\\) outcome \\(X\\) matrix predictors): $$\\texttt{obtain } \\hat{\\mathbb E}(Y | X) \\quad \\mathtt{using \\quad mean\\_learner}$$ $$\\texttt{fit } \\hat{f} \\gets \\mathtt{density}(Y - \\hat{\\mathbb E}(Y | X))$$ $$\\mathtt{return \\quad  function(newdata) \\{ } \\hat{f}(\\mathtt{newdata\\$Y} -   \\hat{\\mathbb E}[Y | \\mathtt{newdata\\$X}]) \\} $$","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_homoskedastic_density.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Density Estimation with Homoskedasticity Assumption ‚Äî lnr_homoskedastic_density","text":"","code":"if (FALSE) { # \\dontrun{ # fit a conditional density model with mean model as a randomForest fit_density_lnr <- lnr_homoskedastic_density(   data = mtcars,   formula = mpg ~ hp,   mean_lnr = lnr_rf)  # and what we should get back should be predicted densities at the # observed mpg given the covariates hp fit_density_lnr(mtcars) } # }"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Normal Density Estimation Given Mean Predictors ‚Äî lnr_lm_density","title":"Conditional Normal Density Estimation Given Mean Predictors ‚Äî lnr_lm_density","text":"simplest possible density estimator entertainable.  fits lm model data, uses variance residuals parameterize model data \\(\\mathcal N(y | \\beta x, \\sigma^2)\\).","code":""},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Normal Density Estimation Given Mean Predictors ‚Äî lnr_lm_density","text":"","code":"lnr_lm_density(data, formula, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/lnr_lm_density.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional Normal Density Estimation Given Mean Predictors ‚Äî lnr_lm_density","text":"closure (function) produces density estimates newdata given according fit model.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Negative Log Likelihood Loss ‚Äî negative_log_lik_loss","title":"Negative Log Likelihood Loss ‚Äî negative_log_lik_loss","text":"Negative Log Likelihood Loss","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Negative Log Likelihood Loss ‚Äî negative_log_lik_loss","text":"","code":"negative_log_lik_loss(predicted_densities, ...)"},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Negative Log Likelihood Loss ‚Äî negative_log_lik_loss","text":"predicted_densities","code":""},{"path":"https://ctesta01.github.io/nadir/reference/negative_log_lik_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Negative Log Likelihood Loss ‚Äî negative_log_lik_loss","text":"negative_log_lik_loss encodes logic: \\(\\hat p_n\\) good model conditional densities, minimize: $$ -\\sum(\\log(\\hat p_n(X_i)) $$","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_extra_learner_arguments.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Extra Arguments ‚Äî parse_extra_learner_arguments","title":"Parse Extra Arguments ‚Äî parse_extra_learner_arguments","text":"Parse Extra Arguments","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_extra_learner_arguments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Extra Arguments ‚Äî parse_extra_learner_arguments","text":"","code":"parse_extra_learner_arguments(extra_learner_args, learner_names)"},{"path":"https://ctesta01.github.io/nadir/reference/parse_formulas.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Formulas for Super Learner ‚Äî parse_formulas","title":"Parse Formulas for Super Learner ‚Äî parse_formulas","text":"Parse Formulas Super Learner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/parse_formulas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Formulas for Super Learner ‚Äî parse_formulas","text":"","code":"parse_formulas(formulas, learner_names)"},{"path":"https://ctesta01.github.io/nadir/reference/parse_formulas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Formulas for Super Learner ‚Äî parse_formulas","text":"formulas Formulas passed learner super learner learner_names names learners passed super learner","code":""},{"path":"https://ctesta01.github.io/nadir/reference/softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Softmax ‚Äî softmax","title":"Softmax ‚Äî softmax","text":"common transformation used go collection numbers R numbers [0,1] sum 1.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softmax ‚Äî softmax","text":"","code":"softmax(beta)"},{"path":"https://ctesta01.github.io/nadir/reference/softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softmax ‚Äî softmax","text":"beta vector numeric values transform","code":""},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":null,"dir":"Reference","previous_headings":"","what":"Super Learner: Cross-Validation Based Ensemble Learning ‚Äî super_learner","title":"Super Learner: Cross-Validation Based Ensemble Learning ‚Äî super_learner","text":"Super learning functional programming!","code":""},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Super Learner: Cross-Validation Based Ensemble Learning ‚Äî super_learner","text":"","code":"super_learner(   data,   learners,   formulas,   y_variable,   n_folds = 5,   determine_super_learner_weights = determine_super_learner_weights_nnls,   continuous_or_discrete = \"continuous\",   cv_schema = cv_random_schema,   extra_learner_args = NULL,   verbose_output = FALSE )"},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Super Learner: Cross-Validation Based Ensemble Learning ‚Äî super_learner","text":"data Data use training `super_learner`. learners list predictor/closure-returning-functions. See Details. formulas Either single regression formula vector regression formulas. y_variable Typically `y_variable` can inferred automatically `formulas`, needed, y_variable can specified explicitly. n_folds number cross-validation folds use constructing `super_learner`. determine_super_learner_weights function/method determine weights candidate `learners`. default use `determine_super_learner_weights_nnls`. continuous_or_discrete Defaults `'continuous'`, can set `'discrete'`. cv_schema function takes `data`, `n_folds` returns list containing `training_data` `validation_data`, lists `n_folds` data frames. extra_learner_args list equal length `learners` additional arguments pass specified learners. verbose_output `verbose_output = TRUE` return list containing fit learners predictions held-data well prediction function closure trained `super_learner`.","code":""},{"path":"https://ctesta01.github.io/nadir/reference/super_learner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Super Learner: Cross-Validation Based Ensemble Learning ‚Äî super_learner","text":"goal super learner use cross-validation set candidate learners 1) evaluate learners perform held data 2) use evaluation produce weighted average (continuous super learner) pick best learner (discrete super learner) specified candidate learners. Super learner statistically desirable properties written length, including least following references: * <https://biostats.bepress.com/ucbbiostat/paper222/>   * <https://www.stat.berkeley.edu/users/laan/Class/Class_subpages/BASS_sec1_3.1.pdf> `nadir::super_learner` adopts several user-interface design-perspectives useful know understanding works: * specification learners _very flexible_, really   constrained fact candidate learners designed   prediction problem details can wildly vary   learner learner.   * easy specify customized new learner. `nadir::super_learner` core accepts `data`, `formula` (single one passed `formulas` fine), list `learners`. `learners` taken lists functions following specification: * learner must accept `data` `formula` argument,   * learner may accept arguments,   * learner must return prediction function accepts `newdata` produces vector prediction values given `newdata`. essence, learner specified function taking (`data`, `formula`, ...) returning _closure_ (see <http://adv-r..co.nz/Functional-programming.html#closures> introduction closures) function accepting `newdata` returning predictions. Since many candidate learners hyperparameters tuned, like depth trees random forests, `lambda` parameter `glmnet`, extra arguments can passed learner via `extra_learner_args` argument. `extra_learner_args` list lists, one list extra arguments learner. additional arguments needed learners, learners using require additional arguments, can just put `NULL` value `extra_learner_args`. See examples. order seamlessly support using features implemented extensions formula syntax (like random effects formatted like random intercepts slopes use `(age | strata)` syntax `lme4` splines like `s(age | strata)` `mgcv`), allow `formulas` argument either one fixed formula `super_learner` use models, vector formulas, one learner specified. Note examples mean-squared-error (mse) calculated training/test set, useful crude diagnostic see super_learner working. rigorous performance metric evaluate `super_learner` cv-rmse produced cv_super_learner.","code":""},{"path":[]}]
